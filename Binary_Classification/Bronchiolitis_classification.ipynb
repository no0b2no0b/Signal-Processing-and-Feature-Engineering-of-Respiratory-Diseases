{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3tVdLGeo3oR",
        "outputId": "c196bc7b-88a3-4c99-a327-73da425e59b8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.7.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.6)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install resampy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkY0fA18o5EH",
        "outputId": "0dfdc796-3f89-4abc-9950-59f519f719f2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: resampy in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from resampy) (1.23.5)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.10/dist-packages (from resampy) (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (67.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JvrSQWdeqAo",
        "outputId": "9b609044-ca2c-4ea5-c7e2-fb4db9dc1f4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load various imports\n",
        "from datetime import datetime\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "stVxcRZIfXIW"
      },
      "outputs": [],
      "source": [
        "mypath = \"/content/gdrive/MyDrive/Major_Project/ICBHI_final_database/audio_and_txt_files/\"\n",
        "filenames = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f.endswith('.wav'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "iKOVKAJyfUIB"
      },
      "outputs": [],
      "source": [
        "p_id_in_file = [] # patient IDs corresponding to each file\n",
        "for name in filenames:\n",
        "    p_id_in_file.append(int(name[:3]))\n",
        "\n",
        "p_id_in_file = np.array(p_id_in_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Mah7LSBGf8Dz"
      },
      "outputs": [],
      "source": [
        "max_pad_len = 862 # to make the length of all MFCC equal\n",
        "\n",
        "def extract_features(file_name):\n",
        "    \"\"\"\n",
        "    This function takes in the path for an audio file as a string, loads it, and returns the MFCC\n",
        "    of the audio\"\"\"\n",
        "\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, duration=20)\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        pad_width = max_pad_len - mfccs.shape[1]\n",
        "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file_name)\n",
        "        return None\n",
        "\n",
        "    return mfccs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepaths = [join(mypath, f) for f in filenames] # full paths of files"
      ],
      "metadata": {
        "id": "_llFxDtW3cTM"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "dSvm1AQWgBWL"
      },
      "outputs": [],
      "source": [
        "p_diag = pd.read_csv(\"/content/gdrive/MyDrive/Major_Project/ICBHI_final_database/patient_diagnosis.csv\") # patient diagnosis file\n",
        "labels = []\n",
        "for x in p_id_in_file:\n",
        "  labels.append(p_diag[p_diag['patient_Id']==x]['Disease'].values[0])\n",
        "label=[]\n",
        "for i in  labels:\n",
        "    if i != 'Bronchiolitis':\n",
        "        i=\"NO\"\n",
        "    label.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HByUdNW6gRB-",
        "outputId": "7e10ea10-ca2d-47c2-8b3a-8c519de9bac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n"
          ]
        }
      ],
      "source": [
        "count=0\n",
        "for i in labels:\n",
        "  if(i=='Bronchiolitis'):\n",
        "    count+=1\n",
        "print(count)\n",
        "labels=np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqIif0KngVAm",
        "outputId": "5dd45032-df27-4d31-c061-860fe39a4353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished feature extraction from  920  files\n"
          ]
        }
      ],
      "source": [
        "features = []\n",
        "\n",
        "# Iterate through each sound file and extract the features\n",
        "for file_name in filepaths:\n",
        "    data = extract_features(file_name)\n",
        "    features.append(data)\n",
        "\n",
        "print('Finished feature extraction from ', len(features), ' files')\n",
        "features = np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id0iS3OOhcgj",
        "outputId": "3ccd4e80-ae35-4451-ad96-ce5364f05508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Bronchiolitis' 'NO']\n",
            " ['13' '907']]\n"
          ]
        }
      ],
      "source": [
        "features= np.array(features) # convert to numpy array\n",
        "\n",
        "# delete the very rare diseases\n",
        "features1 = np.delete(features, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)\n",
        "\n",
        "labels1 = np.delete(labels, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)\n",
        "\n",
        "# print class counts\n",
        "unique_elements, counts_elements = np.unique(labels, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mrewHzpThfVX",
        "outputId": "3a9d1c75-702e-49d9-9cf6-56d3e18bbd9d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAHwCAYAAABkJOM0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAj2UlEQVR4nO3deZhtV1kn4N9nwiSBBMgVSQgkCHQYFMSATI1ARAFRhAcCNkhE2jTdDDLI4NDi1Co2MimDEVqDIFMIGgYRZZY5gYSQABIZAxluQghECJDw9R97V3K4VN1h3XtSVcn7Pk89dfbaa6/z7VO77v3VOuucU90dAABg1/3AehcAAACblTANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJiGK4iqenFV/e/1rmOzq6pTq+ru613H7qiqd1bVf9/FYy69fqrq7lV1xhLqulpVnVZVN9jTY+9py3oMriiq6nFV9cz1rgM2AmEaNoGq+lxVfbOqvl5VX62q91XVo6vq0t/h7n50d//heta5s6rqBlX10qo6cz6nT1bV71fVNZd8v79XVS/fXp/uvlV3v3Nw/PtX1UlV9bWqOreq3l5VhwwVuyTzY/Cdqrpw4eupl9P1c1SSd3f3mXMtf1tVXVV3WKjvplW1Wx+AMJ9jV9VP7sIxXVU33Z373Siq6leq6t/W2PfOqrpo/rmfW1XHzb+Pv7VwPVxUVZcsbJ86H7v4GP11kodV1Q9dXucFG5UwDZvHz3f3tZLcOMmfJnlakpeub0m7rqqum+T9Sa6R5E7zOd0ryX5JfmQdS9stc8h4WZInJ9k3ySFJXpDkkvWsaw2v7u59Fr7+7HK630cn+btt2r6S5I/21B1UVSV5xDzuI/bUuBtVVe09cNhju3ufJDdNsk+SZ3X3H69cD5l+Tu9fuD5ute0A3X1Rkn/KleAxhh0RpmGT6e4Luvv4JA9JcmRV3Tq5dJbvj+bb+1fVG+dZ7K9U1XtWZrGr6oCqel1Vba2qz1bV41fGrqo7VNX75+POrKq/rKqrzvuqqp5TVefMM6+nLNz31arqWVX1hao6e14ycI01TuFJSb6e5OHd/bn5nL7Y3b/e3R+bx7tzVX24qi6Yv995ocbPVdVPL2xfOttcVQfPs2dHzrWcW1W/Pe+7d5LfSvKQebbt5NWKWxx/Hvs1VfWyeQb91Ko6bI3zum2Sz3b323ry9e5+XXd/YeExem5VfXn+em5VXW3e930ziYuzgPPP9gVV9aa5jg9W1Y8s9L1XTbP7F1TVXyapNWpc0+L1s8q+HV0zJ8zXxNlV9ew1xrhRkpsk+eA2u45J8mNV9VPbue/j5+v49Kr6tR2cyn9NcoMkj0/y0JXrdx7rplX1rvlxOreqXj23v3vucvJ8bTxk4Zgnz9f8mVX1yIX2v62qF1bVP83HvLeqfnj+uZ4//zx+fKH/06vqP+af32lV9YC1TmAH18rdq+qMqnpaVZ2V5G928Hisqbu/muQfMl27I96Z5OdG7x+uKIRp2KS6+0NJzsgUHrb15HnfliTXzxQiu6ZA/YYkJyc5MMnhSZ5QVT87H3dJkicm2T/Jneb9/2ve9zNJ7pbk5plmXo9Ict6870/n9ttmmu06MMnvrlH6Tyc5rru/u9rOmmau35Tk+Umul+TZSd5UVddb88H4fndN8l/m+n+3qm7R3W9J8se5bFb2Njs51i8keVWmmfPjk/zlGv0+kuTQ+Q+Oe1TVPtvs/+0kd8z0GN0myR2S/M7On1IemuT3k1wnyelJ/k8y/eGU5Lh5rP2T/EeSu+zCuNu1E9fM85I8r7uvnemZhdesMdSPJvlMd1+8Tfs3Mv1c/s8ax70q07V8QJIHJfnjqrrndko+cq53pY6fX9j3h0nemukxvGGSv0iS7r7bvP8287Xx6nn7hzNd6wcmeVSSF1TVdRbGOyKXPe7fyvSMy0fm7WMzXbsr/iPT7+q+mX6OL6+1147v6Fr54STXzfQs1VFrPhI7MP9OPTDT9TTiE3N9cKUmTMPm9uVM/6lu6zuZZudu3N3f6e73dHcnuX2SLd39B9397e7+TKa1jw9Nku4+sbs/0N0Xz7PGf5XkpxbGvFaSQ5NUd3+iu8+sqsr0H/oTu/sr3f31TOHooWvUfL0kZ27nnH4uyae7++/mOl6Z5JP53lC0I7/f3d/s7pMzhcDd+Q//37r7zd19SaYlCquONT+Wd88UvF6T5Nx59nIlVD8syR909zndvTVToPrlXajj9d39oTmMviKXzSbeN8mp3X1sd38nyXOTnLWDsY6o6dmHla8DttN3u9dMpuviplW1f3df2N0fWGOc/TI9I7Gav0pyo6q6z2JjVR2U6Q+Dp3X3Rd19UpKXZI2lBVX1g0kenOTv58fi2G36fidTAD1gHm/VdcXb9P+D+XfozUkuzPRH2orXz78zFyV5fZKLuvtl87Xy6iSXzkx392u7+8vd/d05rH86U0hezY6ule8meUZ3f6u7v7mDc1jN86vqgiTnZgr+jxsYI5l+nvsOHgtXGMI0bG4HZlobuq3/m2m26a1V9ZmqevrcfuMkBywGqUyz1tdPkqq6eU3LQ86qqq9lCsX7J0l3vz3TrOwLkpxTVUdX1bUzzX7/YJITF8Z8y9y+mvMyBf21HJDk89u0fX4+1521GCa/kWld6Khtx7p6rbFOdf5D5Iju3pJpFvJumWYZk+8/r8/PbaN1rJzTAUm+uFBDL26v4TXdvd/C15e303e710ymGdubJ/lkTUty7rfGOOdn+mPs+3T3tzLNGm/7AsgDkqz8gbZie9fCA5JcnOTN8/YrktynqlauxadmWgLzoZqW7PzqGuOsOG+bmfRtr6WzF25/c5XtS/tW1SNqenHqymN468y/W6vY0bWydQ7wox7f3fsm+bFcNks/4lpJLtiNOuAKQZiGTaqqbp8pVHzf7Nq8XvfJ3X2TTMsUnlRVh2cKWZ/dJkhdq7vvOx/6okyzwDebn7b/rSysv+3u53f3TyS5ZaYA9ZRMs1vfTHKrhTH3nV/ItJp/TfKAWngnkm18OVOAW3SjJF+ab/9npvC+4ofXGGc1u/UuEbuiuz+cafnFreembc/rRnNbss05VdWunNOZSQ5aOLYWt/eA7V4z3f3p7v6lJD+U5JlJjq3V35XlY0kOWesPkUxrf/fLtOxgxZeTXLeqFkP44rWwrSMzBdgvzOuJX5vkKkn+21zrWd39a919QJL/keSFdTm8g0dV3TjTbP5jk1yvu/dL8vGsvbZ9e9dKsoeu4+4+JdOLP18wXze76haZnvmBKzVhGjaZqrr2PPv3qiQvn/9D3LbP/eYXW1WmmaNLMj01/KEkX59fvHSNqtqrqm49B/Nkmmn6WpILq+rQJP9zYczbV9VPVtVVMoW/i5J8d177/NdJnlPz22RV1YELa2q39ewk105yzBwyVvo/u6p+LNOs4s2r6r9V1d7zi8FumeSN8/EnZXph2VVqejHgg3bh4Ts7ycHbCfLDququVfVrC4/BoZn+kFlZ9vDKJL9TVVvmdc6/m2TlbfpOTnKrqrptVV09ye/twl2/aT72gXNQfXx27Q+MHdnuNVNVD6+qLfN18NX5mO9bD9/dZ2R6tmTVpQ3zDPAzMr1LzUrbF5O8L8mfVNXV5+vjUbnscbtUVa2s575fpiUwt820JOeZmZd6VNWDq2plFvb8TKF0pdazM71AchmuOd/X1rmOR+ayP7JWs71rZWfV/Jhd+rVGv2MyPcvwC7s4fjItAfungePgCkWYhs3jDVX19Uwzhb+dKZQ+co2+N8s0A3xhphdFvbC73zGv5VwJG5/NNKv8kly27vE3Ms3ifT1TQH71wpjXntvOz/S083mZlpMkUwA6PckH5uUh/5rvXVt6qe7+SpI7Z1qP+sH5nN6WKfSf3t3nzTU+eb6Ppya5X3efOw/xvzO90O38TGtJ/36tB2wVr52/n1dVH9mF43bGVzMFklOq6sJMS11en2Tlbef+KMkJmWZoT8n0QrU/SpLu/vckf5Dpcft0Vnm2YS3z4/LgTC8CPS/Tz/69u302l42/o2vm3klOnc/5eUkeup11vH+V7a8Tf2W+fz39LyU5ONPM7OszrRX+11WO/eUkJ3X3W+cZ6LO6+6xML2T9sZreeeb2ma65CzO9mPTX5zXgyfQHzDHzMowjtlPjLuvu05L8eabfxbMzvRhzez+jNa+VXXDnTM8YXfq12rMC3f3tTD+3XfrApzmc3zdTGIcrtZqW1wHActX09m4fTXJ4zx/cwuZUVY9LclB3P3W9a4H1JkwDAMAgyzwAAGCQMA0AAIOEaQAAGCRMAwDAoLXePH9T2H///fvggw9e7zIAALiCO/HEE8+dP+H2e2zqMH3wwQfnhBNOWO8yAAC4gquqz6/WbpkHAAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg/Ze7wIAuHJ6zr/8+3qXAGwyT7zXzde7hO9jZhoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAxaapiuqidW1alV9fGqemVVXb2qDqmqD1bV6VX16qq66tz3avP26fP+g5dZGwAA7K6lhemqOjDJ45Mc1t23TrJXkocmeWaS53T3TZOcn+RR8yGPSnL+3P6cuR8AAGxYy17msXeSa1TV3kl+MMmZSe6Z5Nh5/zFJfnG+ff95O/P+w6uqllwfAAAMW1qY7u4vJXlWki9kCtEXJDkxyVe7++K52xlJDpxvH5jki/OxF8/9r7ftuFV1VFWdUFUnbN26dVnlAwDADi1zmcd1Ms02H5LkgCTXTHLv3R23u4/u7sO6+7AtW7bs7nAAADBsmcs8fjrJZ7t7a3d/J8lxSe6SZL952UeS3DDJl+bbX0pyUJLM+/dNct4S6wMAgN2yzDD9hSR3rKofnNc+H57ktCTvSPKguc+RSf5xvn38vJ15/9u7u5dYHwAA7JZlrpn+YKYXEn4kySnzfR2d5GlJnlRVp2daE/3S+ZCXJrne3P6kJE9fVm0AALAn7L3jLuO6+xlJnrFN82eS3GGVvhclefAy6wEAgD3JJyACAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGLTVMV9V+VXVsVX2yqj5RVXeqqutW1b9U1afn79eZ+1ZVPb+qTq+qj1XV7ZZZGwAA7K5lz0w/L8lbuvvQJLdJ8okkT0/ytu6+WZK3zdtJcp8kN5u/jkryoiXXBgAAu2VpYbqq9k1ytyQvTZLu/nZ3fzXJ/ZMcM3c7Jskvzrfvn+RlPflAkv2q6gbLqg8AAHbXMmemD0myNcnfVNVHq+olVXXNJNfv7jPnPmcluf58+8AkX1w4/oy5DQAANqRlhum9k9wuyYu6+8eT/GcuW9KRJOnuTtK7MmhVHVVVJ1TVCVu3bt1jxQIAwK5aZpg+I8kZ3f3BefvYTOH67JXlG/P3c+b9X0py0MLxN5zbvkd3H93dh3X3YVu2bFla8QAAsCNLC9PdfVaSL1bVf5mbDk9yWpLjkxw5tx2Z5B/n28cnecT8rh53THLBwnIQAADYcPZe8viPS/KKqrpqks8keWSmAP+aqnpUks8nOWLu++Yk901yepJvzH0BAGDDWmqY7u6Tkhy2yq7DV+nbSR6zzHoAAGBP8gmIAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGLRTYbqq7rIzbQAAcGWyszPTf7GTbQAAcKWx9/Z2VtWdktw5yZaqetLCrmsn2WuZhQEAwEa33TCd5KpJ9pn7XWuh/WtJHrSsogAAYDPYbpju7ncleVdV/W13f/5yqgkAADaFHc1Mr7haVR2d5ODFY7r7nssoCgAANoOdDdOvTfLiJC9JcsnyygEAgM1jZ8P0xd39oqVWAgAAm8zOvjXeG6rqf1XVDarquitfS60MAAA2uJ2dmT5y/v6UhbZOcpM9Ww4AAGweOxWmu/uQZRcCAACbzU6F6ap6xGrt3f2yPVsOAABsHju7zOP2C7evnuTwJB9JIkwDAHCltbPLPB63uF1V+yV51TIKAgCAzWJn381jW/+ZxDpqAACu1HZ2zfQbMr17R5LsleQWSV6zrKIAAGAz2Nk1089auH1xks939xlLqAcAADaNnVrm0d3vSvLJJNdKcp0k315mUQAAsBnsVJiuqiOSfCjJg5MckeSDVfWgZRYGAAAb3c4u8/jtJLfv7nOSpKq2JPnXJMcuqzAAANjodvbdPH5gJUjPztuFYwEA4AppZ2em31JV/5zklfP2Q5K8eTklAQDA5rDdMF1VN01y/e5+SlU9MMld513vT/KKZRcHAAAb2Y5mpp+b5DeTpLuPS3JcklTVj877fn6JtQEAwIa2o3XP1+/uU7ZtnNsOXkpFAACwSewoTO+3nX3X2IN1AADAprOjMH1CVf3ato1V9d+TnLickgAAYHPY0ZrpJyR5fVU9LJeF58OSXDXJA5ZYFwAAbHjbDdPdfXaSO1fVPZLcem5+U3e/femVAQDABrdT7zPd3e9I8o4l1wIAAJuKTzEEAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAoKWH6araq6o+WlVvnLcPqaoPVtXpVfXqqrrq3H61efv0ef/By64NAAB2x+UxM/3rST6xsP3MJM/p7psmOT/Jo+b2RyU5f25/ztwPAAA2rKWG6aq6YZKfS/KSebuS3DPJsXOXY5L84nz7/vN25v2Hz/0BAGBDWvbM9HOTPDXJd+ft6yX5andfPG+fkeTA+faBSb6YJPP+C+b+AACwIS0tTFfV/ZKc090n7uFxj6qqE6rqhK1bt+7JoQEAYJcsc2b6Lkl+oao+l+RVmZZ3PC/JflW199znhkm+NN/+UpKDkmTev2+S87YdtLuP7u7DuvuwLVu2LLF8AADYvqWF6e7+ze6+YXcfnOShSd7e3Q9L8o4kD5q7HZnkH+fbx8/bmfe/vbt7WfUBAMDuWo/3mX5akidV1emZ1kS/dG5/aZLrze1PSvL0dagNAAB22t477rL7uvudSd453/5Mkjus0ueiJA++POoBAIA9wScgAgDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwaGlhuqoOqqp3VNVpVXVqVf363H7dqvqXqvr0/P06c3tV1fOr6vSq+lhV3W5ZtQEAwJ6wzJnpi5M8ubtvmeSOSR5TVbdM8vQkb+vumyV527ydJPdJcrP566gkL1pibQAAsNuWFqa7+8zu/sh8++tJPpHkwCT3T3LM3O2YJL84375/kpf15ANJ9quqGyyrPgAA2F2Xy5rpqjo4yY8n+WCS63f3mfOus5Jcf759YJIvLhx2xty27VhHVdUJVXXC1q1bl1c0AADswNLDdFXtk+R1SZ7Q3V9b3NfdnaR3ZbzuPrq7D+vuw7Zs2bIHKwUAgF2z1DBdVVfJFKRf0d3Hzc1nryzfmL+fM7d/KclBC4ffcG4DAIANaZnv5lFJXprkE9397IVdxyc5cr59ZJJ/XGh/xPyuHndMcsHCchAAANhw9l7i2HdJ8stJTqmqk+a230ryp0leU1WPSvL5JEfM+96c5L5JTk/yjSSPXGJtAACw25YWprv735LUGrsPX6V/J3nMsuoBAIA9zScgAgDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABgkTAMAwCBhGgAABgnTAAAwSJgGAIBBwjQAAAwSpgEAYJAwDQAAg4RpAAAYJEwDAMAgYRoAAAYJ0wAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEHCNAAADBKmAQBgkDANAACDhGkAABi093oXsFk951/+fb1LADaZJ97r5utdAgB7mJlpAAAYtKHCdFXdu6o+VVWnV9XT17seAADYng0TpqtqryQvSHKfJLdM8ktVdcv1rQoAANa2YcJ0kjskOb27P9Pd307yqiT3X+eaAABgTRspTB+Y5IsL22fMbQAAsCFtunfzqKqjkhw1b15YVZ9az3pgFfsnOXe9i2DjedJ6FwCbh39HWdU6/zt649UaN1KY/lKSgxa2bzi3fY/uPjrJ0ZdXUbCrquqE7j5svesA2Kz8O8pmspGWeXw4yc2q6pCqumqShyY5fp1rAgCANW2YmenuvriqHpvkn5PsleT/dfep61wWAACsacOE6STp7jcnefN61wG7yTIkgN3j31E2jeru9a4BAAA2pY20ZhoAADYVYZorrKq6pKpOqqqTq+ojVXXnJd/f71XVb6yx7307OPbgqvr4fPuwqnr+fPvui3VX1aOr6hF7sm6A9VBVXVV/vrD9G1X1ewvbR1XVJ+evD1XVXdelUNiBDbVmGvawb3b3bZOkqn42yZ8k+anFDlW1d3dfvOxCunung3x3n5DkhHnz7kkuTPK+ed+L93hxAOvjW0keWFV/0t3f857SVXW/JP8jyV27+9yqul2Sf6iqO3T3WetRLKzFzDRXFtdOcn5y6Wzve6rq+CSnVdXVq+pvquqUqvpoVd1j7vcrVXVcVb2lqj5dVX+2MlhV3Xue7T65qt62cD+3rKp3VtVnqurxC/0vnL9XVf3fqvr4fH8P2bbQub43VtXBSR6d5InzDPt/XZz9rqrHV9VpVfWxqnrVnn/IAJbq4kwvNHziKvueluQpKyG7uz+S5Jgkj7n8yoOdY2aaK7JrVNVJSa6e5AZJ7rmw73ZJbt3dn62qJyfp7v7Rqjo0yVur6uZzv9sm+fFMMyifqqq/SHJRkr9Ocrf5+OsujHtoknskudbc/0Xd/Z2F/Q+cx7xNpk/4+nBVvXu14rv7c1X14iQXdvezkqSqDl/o8vQkh3T3t6pqv115YAA2iBck+djiZMXsVklO3KbthCRHXi5VwS4wM80V2Te7+7bdfWiSeyd5WVXVvO9D3f3Z+fZdk7w8Sbr7k0k+n2QlTL+tuy/o7ouSnJbpo0TvmOTdK8d391cW7vNN3f2teTblnCTX36amuyZ5ZXdf0t1nJ3lXktsPnt/Hkryiqh6eaYYHYFPp7q8leVmSx++oL2xUwjRXCt39/kwzwVvmpv/cyUO/tXD7kuz42Zxd7b87fi7TrM7tMs1we6YJ2Iyem+RRSa650HZakp/Ypt9PJPFhbmw4wjRXCvPyjb2SnLfK7vckedjc7+ZJbpTkU9sZ7gNJ7lZVh8zHXHc7fVe7r4dU1V5VtSXJ3ZJ8aDv9v55pycj3qKofSHJQd78j09rCfZPsswt1AGwI87N7r8kUqFf8WZJnVtX1kqSqbpvkV5K88PKuD3bETBZXZCtrppOkkhzZ3ZdcttLjUi9M8qKqOiXTcolfmdchrzpod2+tqqOSHDeH2nOS3Gsna3p9kjslOTlJJ3lqd581v9hwNW9IcmxV3T/J4xba90ry8qradz6353f3V3eyBoCN5s+TPHZlo7uPr6oDk7yvqjrTxMLDu/vM9SoQ1uITEAEAYJBlHgAAMEiYBgCAQcI0AAAMEqYBAGCQMA0AAIOEaYBNoKouqaqTqurUqjq5qp48vzVjquqwqnr+etcIcGXkrfEANoGqurC795lv/1CSv0/y3u5+xvpWBnDlZmYaYJPp7nOSHJXksTW5e1W9MUmq6qfmGeyTquqjVXWtuf0pVfXhqvpYVf3+ylhV9Q9VdeI8433U3LZXVf1tVX28qk6pqifO7T9SVW+Z+79n/mRRgCs1n4AIsAl192eqaq8kP7TNrt9I8pjufm9V7ZPkoqr6mSQ3S3KHTJ+YeXxV3a27353kV7v7K1V1jSQfrqrXJTk4yYHdfeskqar95rGPTvLo7v50Vf1kpk8PvedyzxRgYxOmAa5Y3pvk2VX1iiTHdfcZc5j+mSQfnfvskylcvzvJ46vqAXP7QXP7p5LcpKr+Ismbkrx1DuZ3TvLaqlq5r6tdHicEsJEJ0wCbUFXdJMklSc5JcouV9u7+06p6U5L7JnlvVf1sptnoP+nuv9pmjLsn+ekkd+rub1TVO5NcvbvPr6rbJPnZJI9OckSSJyT5anffdrlnBrC5WDMNsMlU1ZYkL07yl73Nq8ir6ke6+5TufmaSDyc5NMk/J/nVeXY5VXXg/CLGfZOcPwfpQ5Pccd6/f5If6O7XJfmdJLfr7q8l+WxVPXjuU3PgBrhSMzMNsDlco6pOSnKVJBcn+bskz16l3xOq6h5Jvpvk1CT/1N3fqqpbJHn/vETjwiQPT/KWJI+uqk9kWtrxgXmMA5P8zcpb7yX5zfn7w5K8qKp+Z67jVUlO3qNnCbDJeGs8AAAYZJkHAAAMEqYBAGCQMA0AAIOEaQAAGCRMAwDAIGEaAAAGCdMAADBImAYAgEH/H33OfIltcQUfAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot class counts\n",
        "y_pos = np.arange(len(unique_elements))\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.bar(unique_elements, counts_elements, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, unique_elements)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Disease')\n",
        "plt.title('Disease Count in Sound Files (No Asthma or LRTI)')\n",
        "plt.show()\n",
        "\n",
        "# One-hot encode labels\n",
        "le = LabelEncoder()\n",
        "i_labels = le.fit_transform(labels)\n",
        "oh_labels = to_categorical(i_labels)\n",
        "\n",
        "# add channel dimension for CNN\n",
        "features1 = np.reshape(features, (*features.shape,1))\n",
        "\n",
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(features1, oh_labels, stratify=oh_labels,\n",
        "                                                    test_size=0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uT9JAnmjhiYV"
      },
      "outputs": [],
      "source": [
        "num_rows = 40\n",
        "num_columns = 862\n",
        "num_channels = 1\n",
        "\n",
        "num_labels = oh_labels.shape[1]\n",
        "filter_size = 2\n",
        "\n",
        "# Construct model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=filter_size,\n",
        "                 input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "euimbqFghoOb",
        "outputId": "b2147364-cad3-438d-a745-af42641b9859"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 39, 861, 16)       80        \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 19, 430, 16)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 19, 430, 16)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 18, 429, 32)       2080      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 9, 214, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 9, 214, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 213, 64)        8256      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 106, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 106, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 3, 105, 128)       32896     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 52, 128)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1, 52, 128)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 128)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43,570\n",
            "Trainable params: 43,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "6/6 [==============================] - 2s 149ms/step - loss: 20.0324 - accuracy: 0.0163\n",
            "Pre-training accuracy: 1.6304%\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# Display model architecture summary\n",
        "model.summary()\n",
        "\n",
        "# Calculate pre-training accuracy\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CXz0nH4hqXf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcc11c86-b8b0-405e-b8e5-856f11c19090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 13.1756 - accuracy: 0.6114\n",
            "Epoch 1: val_accuracy improved from -inf to 0.98370, saving model to mymodel2_01.h5\n",
            "6/6 [==============================] - 16s 3s/step - loss: 13.1756 - accuracy: 0.6114 - val_loss: 0.4012 - val_accuracy: 0.9837\n",
            "Epoch 2/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.2355 - accuracy: 0.9864\n",
            "Epoch 2: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 1.2355 - accuracy: 0.9864 - val_loss: 0.6150 - val_accuracy: 0.9837\n",
            "Epoch 3/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.4262 - accuracy: 0.9864\n",
            "Epoch 3: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 1.4262 - accuracy: 0.9864 - val_loss: 0.6115 - val_accuracy: 0.9837\n",
            "Epoch 4/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.3231 - accuracy: 0.9864\n",
            "Epoch 4: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 1.3231 - accuracy: 0.9864 - val_loss: 0.5318 - val_accuracy: 0.9837\n",
            "Epoch 5/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.1304 - accuracy: 0.9864\n",
            "Epoch 5: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 1.1304 - accuracy: 0.9864 - val_loss: 0.4261 - val_accuracy: 0.9837\n",
            "Epoch 6/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.9005 - accuracy: 0.9864\n",
            "Epoch 6: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.9005 - accuracy: 0.9864 - val_loss: 0.3377 - val_accuracy: 0.9837\n",
            "Epoch 7/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.7734 - accuracy: 0.9864\n",
            "Epoch 7: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.7734 - accuracy: 0.9864 - val_loss: 0.2428 - val_accuracy: 0.9837\n",
            "Epoch 8/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5633 - accuracy: 0.9864\n",
            "Epoch 8: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.5633 - accuracy: 0.9864 - val_loss: 0.1821 - val_accuracy: 0.9837\n",
            "Epoch 9/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4708 - accuracy: 0.9864\n",
            "Epoch 9: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.4708 - accuracy: 0.9864 - val_loss: 0.1317 - val_accuracy: 0.9837\n",
            "Epoch 10/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.9864\n",
            "Epoch 10: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.3517 - accuracy: 0.9864 - val_loss: 0.1170 - val_accuracy: 0.9837\n",
            "Epoch 11/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2669 - accuracy: 0.9864\n",
            "Epoch 11: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.2669 - accuracy: 0.9864 - val_loss: 0.1985 - val_accuracy: 0.9837\n",
            "Epoch 12/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1940 - accuracy: 0.9864\n",
            "Epoch 12: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.1940 - accuracy: 0.9864 - val_loss: 0.5628 - val_accuracy: 0.8533\n",
            "Epoch 13/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9864\n",
            "Epoch 13: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.1337 - accuracy: 0.9864 - val_loss: 1.3454 - val_accuracy: 0.0163\n",
            "Epoch 14/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1125 - accuracy: 0.9864\n",
            "Epoch 14: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 2s/step - loss: 0.1125 - accuracy: 0.9864 - val_loss: 1.5726 - val_accuracy: 0.0163\n",
            "Epoch 15/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9864\n",
            "Epoch 15: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.1013 - accuracy: 0.9864 - val_loss: 1.2379 - val_accuracy: 0.0489\n",
            "Epoch 16/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0978 - accuracy: 0.9864\n",
            "Epoch 16: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 2s/step - loss: 0.0978 - accuracy: 0.9864 - val_loss: 1.1748 - val_accuracy: 0.1033\n",
            "Epoch 17/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0879 - accuracy: 0.9864\n",
            "Epoch 17: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 2s/step - loss: 0.0879 - accuracy: 0.9864 - val_loss: 1.3155 - val_accuracy: 0.0870\n",
            "Epoch 18/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9864\n",
            "Epoch 18: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0806 - accuracy: 0.9864 - val_loss: 1.3855 - val_accuracy: 0.0870\n",
            "Epoch 19/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9864\n",
            "Epoch 19: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 2s/step - loss: 0.0817 - accuracy: 0.9864 - val_loss: 1.2398 - val_accuracy: 0.1739\n",
            "Epoch 20/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0792 - accuracy: 0.9864\n",
            "Epoch 20: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0792 - accuracy: 0.9864 - val_loss: 1.2543 - val_accuracy: 0.2011\n",
            "Epoch 21/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9864\n",
            "Epoch 21: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0778 - accuracy: 0.9864 - val_loss: 1.1677 - val_accuracy: 0.2772\n",
            "Epoch 22/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9864\n",
            "Epoch 22: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 20s 4s/step - loss: 0.0756 - accuracy: 0.9864 - val_loss: 1.1990 - val_accuracy: 0.2826\n",
            "Epoch 23/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9864\n",
            "Epoch 23: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0740 - accuracy: 0.9864 - val_loss: 1.2041 - val_accuracy: 0.2989\n",
            "Epoch 24/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9864\n",
            "Epoch 24: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0756 - accuracy: 0.9864 - val_loss: 1.0738 - val_accuracy: 0.3967\n",
            "Epoch 25/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9864\n",
            "Epoch 25: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0800 - accuracy: 0.9864 - val_loss: 1.1827 - val_accuracy: 0.3587\n",
            "Epoch 26/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9864\n",
            "Epoch 26: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0817 - accuracy: 0.9864 - val_loss: 1.1504 - val_accuracy: 0.3750\n",
            "Epoch 27/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9864\n",
            "Epoch 27: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0830 - accuracy: 0.9864 - val_loss: 0.8442 - val_accuracy: 0.5217\n",
            "Epoch 28/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9864\n",
            "Epoch 28: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0788 - accuracy: 0.9864 - val_loss: 1.0349 - val_accuracy: 0.4239\n",
            "Epoch 29/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9864\n",
            "Epoch 29: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0789 - accuracy: 0.9864 - val_loss: 1.2426 - val_accuracy: 0.3261\n",
            "Epoch 30/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9864\n",
            "Epoch 30: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0680 - accuracy: 0.9864 - val_loss: 0.8597 - val_accuracy: 0.5217\n",
            "Epoch 31/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0756 - accuracy: 0.9864\n",
            "Epoch 31: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0756 - accuracy: 0.9864 - val_loss: 0.8183 - val_accuracy: 0.5489\n",
            "Epoch 32/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9864\n",
            "Epoch 32: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0748 - accuracy: 0.9864 - val_loss: 1.1133 - val_accuracy: 0.3859\n",
            "Epoch 33/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9864\n",
            "Epoch 33: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0758 - accuracy: 0.9864 - val_loss: 1.0660 - val_accuracy: 0.3967\n",
            "Epoch 34/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9864\n",
            "Epoch 34: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0668 - accuracy: 0.9864 - val_loss: 0.9329 - val_accuracy: 0.4783\n",
            "Epoch 35/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9864\n",
            "Epoch 35: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0699 - accuracy: 0.9864 - val_loss: 0.9863 - val_accuracy: 0.4511\n",
            "Epoch 36/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9864\n",
            "Epoch 36: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0744 - accuracy: 0.9864 - val_loss: 0.9882 - val_accuracy: 0.4511\n",
            "Epoch 37/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9864\n",
            "Epoch 37: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0649 - accuracy: 0.9864 - val_loss: 0.8814 - val_accuracy: 0.5000\n",
            "Epoch 38/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9864\n",
            "Epoch 38: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0746 - accuracy: 0.9864 - val_loss: 1.0641 - val_accuracy: 0.4130\n",
            "Epoch 39/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9864\n",
            "Epoch 39: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0696 - accuracy: 0.9864 - val_loss: 1.0104 - val_accuracy: 0.4457\n",
            "Epoch 40/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9864\n",
            "Epoch 40: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0697 - accuracy: 0.9864 - val_loss: 0.7851 - val_accuracy: 0.5543\n",
            "Epoch 41/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9864\n",
            "Epoch 41: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0749 - accuracy: 0.9864 - val_loss: 0.9627 - val_accuracy: 0.4620\n",
            "Epoch 42/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9864\n",
            "Epoch 42: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0690 - accuracy: 0.9864 - val_loss: 0.9899 - val_accuracy: 0.4511\n",
            "Epoch 43/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9864\n",
            "Epoch 43: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0688 - accuracy: 0.9864 - val_loss: 0.8163 - val_accuracy: 0.5435\n",
            "Epoch 44/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9864\n",
            "Epoch 44: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0720 - accuracy: 0.9864 - val_loss: 0.9787 - val_accuracy: 0.4511\n",
            "Epoch 45/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9864\n",
            "Epoch 45: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0749 - accuracy: 0.9864 - val_loss: 0.9780 - val_accuracy: 0.4511\n",
            "Epoch 46/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9864\n",
            "Epoch 46: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0669 - accuracy: 0.9864 - val_loss: 0.8078 - val_accuracy: 0.5435\n",
            "Epoch 47/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9864\n",
            "Epoch 47: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0707 - accuracy: 0.9864 - val_loss: 0.9497 - val_accuracy: 0.4674\n",
            "Epoch 48/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0690 - accuracy: 0.9864\n",
            "Epoch 48: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0690 - accuracy: 0.9864 - val_loss: 0.9430 - val_accuracy: 0.4674\n",
            "Epoch 49/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9864\n",
            "Epoch 49: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0637 - accuracy: 0.9864 - val_loss: 0.8249 - val_accuracy: 0.5326\n",
            "Epoch 50/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9864\n",
            "Epoch 50: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0687 - accuracy: 0.9864 - val_loss: 0.9411 - val_accuracy: 0.4674\n",
            "Epoch 51/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9864\n",
            "Epoch 51: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0702 - accuracy: 0.9864 - val_loss: 0.8055 - val_accuracy: 0.5435\n",
            "Epoch 52/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9864\n",
            "Epoch 52: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0698 - accuracy: 0.9864 - val_loss: 0.9365 - val_accuracy: 0.4783\n",
            "Epoch 53/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9864\n",
            "Epoch 53: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0653 - accuracy: 0.9864 - val_loss: 0.8706 - val_accuracy: 0.4891\n",
            "Epoch 54/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9864\n",
            "Epoch 54: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0649 - accuracy: 0.9864 - val_loss: 0.8641 - val_accuracy: 0.5000\n",
            "Epoch 55/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9864\n",
            "Epoch 55: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0634 - accuracy: 0.9864 - val_loss: 0.8137 - val_accuracy: 0.5272\n",
            "Epoch 56/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9864\n",
            "Epoch 56: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0658 - accuracy: 0.9864 - val_loss: 0.8999 - val_accuracy: 0.4837\n",
            "Epoch 57/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9864\n",
            "Epoch 57: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0644 - accuracy: 0.9864 - val_loss: 0.7834 - val_accuracy: 0.5435\n",
            "Epoch 58/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9864\n",
            "Epoch 58: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0640 - accuracy: 0.9864 - val_loss: 0.8085 - val_accuracy: 0.5217\n",
            "Epoch 59/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9864\n",
            "Epoch 59: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0644 - accuracy: 0.9864 - val_loss: 0.8778 - val_accuracy: 0.4891\n",
            "Epoch 60/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9864\n",
            "Epoch 60: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0629 - accuracy: 0.9864 - val_loss: 0.7083 - val_accuracy: 0.5761\n",
            "Epoch 61/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9864\n",
            "Epoch 61: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 17s 3s/step - loss: 0.0602 - accuracy: 0.9864 - val_loss: 0.8440 - val_accuracy: 0.5000\n",
            "Epoch 62/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9864\n",
            "Epoch 62: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0599 - accuracy: 0.9864 - val_loss: 0.7588 - val_accuracy: 0.5435\n",
            "Epoch 63/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9864\n",
            "Epoch 63: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0653 - accuracy: 0.9864 - val_loss: 0.7768 - val_accuracy: 0.5380\n",
            "Epoch 64/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9864\n",
            "Epoch 64: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0670 - accuracy: 0.9864 - val_loss: 0.5893 - val_accuracy: 0.6793\n",
            "Epoch 65/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9864\n",
            "Epoch 65: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0636 - accuracy: 0.9864 - val_loss: 0.8614 - val_accuracy: 0.4837\n",
            "Epoch 66/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9864\n",
            "Epoch 66: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0752 - accuracy: 0.9864 - val_loss: 0.5814 - val_accuracy: 0.6902\n",
            "Epoch 67/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9864\n",
            "Epoch 67: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0645 - accuracy: 0.9864 - val_loss: 0.7867 - val_accuracy: 0.5217\n",
            "Epoch 68/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9864\n",
            "Epoch 68: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0571 - accuracy: 0.9864 - val_loss: 0.5938 - val_accuracy: 0.6902\n",
            "Epoch 69/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9864\n",
            "Epoch 69: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0647 - accuracy: 0.9864 - val_loss: 0.6711 - val_accuracy: 0.5978\n",
            "Epoch 70/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9864\n",
            "Epoch 70: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0604 - accuracy: 0.9864 - val_loss: 0.4404 - val_accuracy: 0.7989\n",
            "Epoch 71/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9864\n",
            "Epoch 71: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0754 - accuracy: 0.9864 - val_loss: 0.4280 - val_accuracy: 0.8152\n",
            "Epoch 72/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9864\n",
            "Epoch 72: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0705 - accuracy: 0.9864 - val_loss: 0.6442 - val_accuracy: 0.6576\n",
            "Epoch 73/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9864\n",
            "Epoch 73: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0656 - accuracy: 0.9864 - val_loss: 0.2803 - val_accuracy: 0.8804\n",
            "Epoch 74/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9864\n",
            "Epoch 74: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0601 - accuracy: 0.9864 - val_loss: 0.7503 - val_accuracy: 0.5543\n",
            "Epoch 75/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9864\n",
            "Epoch 75: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0609 - accuracy: 0.9864 - val_loss: 0.3942 - val_accuracy: 0.8261\n",
            "Epoch 76/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9864\n",
            "Epoch 76: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0544 - accuracy: 0.9864 - val_loss: 0.4590 - val_accuracy: 0.7880\n",
            "Epoch 77/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0529 - accuracy: 0.9864\n",
            "Epoch 77: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0529 - accuracy: 0.9864 - val_loss: 0.4882 - val_accuracy: 0.7772\n",
            "Epoch 78/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9864\n",
            "Epoch 78: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0517 - accuracy: 0.9864 - val_loss: 0.3522 - val_accuracy: 0.8587\n",
            "Epoch 79/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9864\n",
            "Epoch 79: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0520 - accuracy: 0.9864 - val_loss: 0.3998 - val_accuracy: 0.7989\n",
            "Epoch 80/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9864\n",
            "Epoch 80: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0530 - accuracy: 0.9864 - val_loss: 0.2469 - val_accuracy: 0.8750\n",
            "Epoch 81/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9864\n",
            "Epoch 81: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0574 - accuracy: 0.9864 - val_loss: 0.4276 - val_accuracy: 0.7989\n",
            "Epoch 82/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9864\n",
            "Epoch 82: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0470 - accuracy: 0.9864 - val_loss: 0.2794 - val_accuracy: 0.8750\n",
            "Epoch 83/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9864\n",
            "Epoch 83: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0489 - accuracy: 0.9864 - val_loss: 0.4024 - val_accuracy: 0.8098\n",
            "Epoch 84/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9864\n",
            "Epoch 84: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0492 - accuracy: 0.9864 - val_loss: 0.3100 - val_accuracy: 0.8750\n",
            "Epoch 85/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9864\n",
            "Epoch 85: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0467 - accuracy: 0.9864 - val_loss: 0.3569 - val_accuracy: 0.8315\n",
            "Epoch 86/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9851\n",
            "Epoch 86: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0504 - accuracy: 0.9851 - val_loss: 0.3165 - val_accuracy: 0.8750\n",
            "Epoch 87/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9864\n",
            "Epoch 87: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0501 - accuracy: 0.9864 - val_loss: 0.2524 - val_accuracy: 0.8696\n",
            "Epoch 88/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9864\n",
            "Epoch 88: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0460 - accuracy: 0.9864 - val_loss: 0.4130 - val_accuracy: 0.8098\n",
            "Epoch 89/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0420 - accuracy: 0.9864\n",
            "Epoch 89: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0420 - accuracy: 0.9864 - val_loss: 0.2419 - val_accuracy: 0.8750\n",
            "Epoch 90/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9878\n",
            "Epoch 90: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0502 - accuracy: 0.9878 - val_loss: 0.3754 - val_accuracy: 0.8207\n",
            "Epoch 91/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9851\n",
            "Epoch 91: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0456 - accuracy: 0.9851 - val_loss: 0.2763 - val_accuracy: 0.8696\n",
            "Epoch 92/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9864\n",
            "Epoch 92: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0485 - accuracy: 0.9864 - val_loss: 0.3259 - val_accuracy: 0.8478\n",
            "Epoch 93/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9851\n",
            "Epoch 93: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0476 - accuracy: 0.9851 - val_loss: 0.2662 - val_accuracy: 0.8750\n",
            "Epoch 94/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9851\n",
            "Epoch 94: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0425 - accuracy: 0.9851 - val_loss: 0.3669 - val_accuracy: 0.8261\n",
            "Epoch 95/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9851\n",
            "Epoch 95: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0450 - accuracy: 0.9851 - val_loss: 0.3068 - val_accuracy: 0.8641\n",
            "Epoch 96/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9851\n",
            "Epoch 96: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0466 - accuracy: 0.9851 - val_loss: 0.3240 - val_accuracy: 0.8478\n",
            "Epoch 97/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9864\n",
            "Epoch 97: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0414 - accuracy: 0.9864 - val_loss: 0.2979 - val_accuracy: 0.8533\n",
            "Epoch 98/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9851\n",
            "Epoch 98: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0428 - accuracy: 0.9851 - val_loss: 0.2660 - val_accuracy: 0.8859\n",
            "Epoch 99/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9864\n",
            "Epoch 99: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0462 - accuracy: 0.9864 - val_loss: 0.3055 - val_accuracy: 0.8641\n",
            "Epoch 100/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9864\n",
            "Epoch 100: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0390 - accuracy: 0.9864 - val_loss: 0.3215 - val_accuracy: 0.8587\n",
            "Epoch 101/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9878\n",
            "Epoch 101: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0402 - accuracy: 0.9878 - val_loss: 0.2687 - val_accuracy: 0.8804\n",
            "Epoch 102/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9851\n",
            "Epoch 102: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0478 - accuracy: 0.9851 - val_loss: 0.3587 - val_accuracy: 0.8315\n",
            "Epoch 103/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9864\n",
            "Epoch 103: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0494 - accuracy: 0.9864 - val_loss: 0.2196 - val_accuracy: 0.8967\n",
            "Epoch 104/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9851\n",
            "Epoch 104: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0395 - accuracy: 0.9851 - val_loss: 0.3715 - val_accuracy: 0.8370\n",
            "Epoch 105/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9851\n",
            "Epoch 105: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0438 - accuracy: 0.9851 - val_loss: 0.2476 - val_accuracy: 0.8967\n",
            "Epoch 106/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9891\n",
            "Epoch 106: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0367 - accuracy: 0.9891 - val_loss: 0.3898 - val_accuracy: 0.8261\n",
            "Epoch 107/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9851\n",
            "Epoch 107: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0378 - accuracy: 0.9851 - val_loss: 0.2525 - val_accuracy: 0.8967\n",
            "Epoch 108/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9878\n",
            "Epoch 108: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0436 - accuracy: 0.9878 - val_loss: 0.3724 - val_accuracy: 0.8424\n",
            "Epoch 109/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9851\n",
            "Epoch 109: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0442 - accuracy: 0.9851 - val_loss: 0.1927 - val_accuracy: 0.9076\n",
            "Epoch 110/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9864\n",
            "Epoch 110: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0450 - accuracy: 0.9864 - val_loss: 0.3078 - val_accuracy: 0.8587\n",
            "Epoch 111/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9837\n",
            "Epoch 111: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0444 - accuracy: 0.9837 - val_loss: 0.2990 - val_accuracy: 0.8696\n",
            "Epoch 112/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9878\n",
            "Epoch 112: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0410 - accuracy: 0.9878 - val_loss: 0.2897 - val_accuracy: 0.8750\n",
            "Epoch 113/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9864\n",
            "Epoch 113: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0405 - accuracy: 0.9864 - val_loss: 0.2658 - val_accuracy: 0.8750\n",
            "Epoch 114/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9878\n",
            "Epoch 114: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0407 - accuracy: 0.9878 - val_loss: 0.2885 - val_accuracy: 0.8696\n",
            "Epoch 115/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9878\n",
            "Epoch 115: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0368 - accuracy: 0.9878 - val_loss: 0.2958 - val_accuracy: 0.8587\n",
            "Epoch 116/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9851\n",
            "Epoch 116: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0376 - accuracy: 0.9851 - val_loss: 0.2489 - val_accuracy: 0.8967\n",
            "Epoch 117/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9851\n",
            "Epoch 117: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0386 - accuracy: 0.9851 - val_loss: 0.2596 - val_accuracy: 0.8859\n",
            "Epoch 118/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9878\n",
            "Epoch 118: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0386 - accuracy: 0.9878 - val_loss: 0.2827 - val_accuracy: 0.8859\n",
            "Epoch 119/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9864\n",
            "Epoch 119: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0389 - accuracy: 0.9864 - val_loss: 0.2160 - val_accuracy: 0.9130\n",
            "Epoch 120/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9864\n",
            "Epoch 120: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0349 - accuracy: 0.9864 - val_loss: 0.2677 - val_accuracy: 0.8859\n",
            "Epoch 121/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9864\n",
            "Epoch 121: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0381 - accuracy: 0.9864 - val_loss: 0.2905 - val_accuracy: 0.8859\n",
            "Epoch 122/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9837\n",
            "Epoch 122: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0417 - accuracy: 0.9837 - val_loss: 0.1994 - val_accuracy: 0.9076\n",
            "Epoch 123/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0400 - accuracy: 0.9851\n",
            "Epoch 123: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0400 - accuracy: 0.9851 - val_loss: 0.2122 - val_accuracy: 0.9130\n",
            "Epoch 124/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9878\n",
            "Epoch 124: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0329 - accuracy: 0.9878 - val_loss: 0.3137 - val_accuracy: 0.8750\n",
            "Epoch 125/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9891\n",
            "Epoch 125: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0342 - accuracy: 0.9891 - val_loss: 0.2287 - val_accuracy: 0.9022\n",
            "Epoch 126/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9878\n",
            "Epoch 126: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 17s 3s/step - loss: 0.0339 - accuracy: 0.9878 - val_loss: 0.2931 - val_accuracy: 0.8859\n",
            "Epoch 127/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9878\n",
            "Epoch 127: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0350 - accuracy: 0.9878 - val_loss: 0.2038 - val_accuracy: 0.9076\n",
            "Epoch 128/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9864\n",
            "Epoch 128: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0364 - accuracy: 0.9864 - val_loss: 0.2458 - val_accuracy: 0.8967\n",
            "Epoch 129/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9891\n",
            "Epoch 129: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0348 - accuracy: 0.9891 - val_loss: 0.2619 - val_accuracy: 0.8913\n",
            "Epoch 130/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9823\n",
            "Epoch 130: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0509 - accuracy: 0.9823 - val_loss: 0.1201 - val_accuracy: 0.9402\n",
            "Epoch 131/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9864\n",
            "Epoch 131: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0670 - accuracy: 0.9864 - val_loss: 0.1001 - val_accuracy: 0.9674\n",
            "Epoch 132/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9864\n",
            "Epoch 132: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0461 - accuracy: 0.9864 - val_loss: 0.4138 - val_accuracy: 0.8370\n",
            "Epoch 133/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9864\n",
            "Epoch 133: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0470 - accuracy: 0.9864 - val_loss: 0.1120 - val_accuracy: 0.9565\n",
            "Epoch 134/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9864\n",
            "Epoch 134: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 17s 3s/step - loss: 0.0440 - accuracy: 0.9864 - val_loss: 0.3355 - val_accuracy: 0.8641\n",
            "Epoch 135/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9918\n",
            "Epoch 135: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0344 - accuracy: 0.9918 - val_loss: 0.1469 - val_accuracy: 0.9239\n",
            "Epoch 136/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9864\n",
            "Epoch 136: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0468 - accuracy: 0.9864 - val_loss: 0.1266 - val_accuracy: 0.9348\n",
            "Epoch 137/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0358 - accuracy: 0.9891\n",
            "Epoch 137: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0358 - accuracy: 0.9891 - val_loss: 0.3140 - val_accuracy: 0.8804\n",
            "Epoch 138/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9837\n",
            "Epoch 138: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0397 - accuracy: 0.9837 - val_loss: 0.1443 - val_accuracy: 0.9185\n",
            "Epoch 139/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9878\n",
            "Epoch 139: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0399 - accuracy: 0.9878 - val_loss: 0.1938 - val_accuracy: 0.9076\n",
            "Epoch 140/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9878\n",
            "Epoch 140: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0327 - accuracy: 0.9878 - val_loss: 0.1714 - val_accuracy: 0.9185\n",
            "Epoch 141/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9905\n",
            "Epoch 141: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0336 - accuracy: 0.9905 - val_loss: 0.1834 - val_accuracy: 0.9185\n",
            "Epoch 142/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9891\n",
            "Epoch 142: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0353 - accuracy: 0.9891 - val_loss: 0.1501 - val_accuracy: 0.9185\n",
            "Epoch 143/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9918\n",
            "Epoch 143: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0341 - accuracy: 0.9918 - val_loss: 0.2127 - val_accuracy: 0.9076\n",
            "Epoch 144/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9891\n",
            "Epoch 144: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0356 - accuracy: 0.9891 - val_loss: 0.1266 - val_accuracy: 0.9402\n",
            "Epoch 145/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9878\n",
            "Epoch 145: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0301 - accuracy: 0.9878 - val_loss: 0.2237 - val_accuracy: 0.9076\n",
            "Epoch 146/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9878\n",
            "Epoch 146: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0317 - accuracy: 0.9878 - val_loss: 0.1434 - val_accuracy: 0.9239\n",
            "Epoch 147/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9891\n",
            "Epoch 147: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0361 - accuracy: 0.9891 - val_loss: 0.1734 - val_accuracy: 0.9185\n",
            "Epoch 148/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9891\n",
            "Epoch 148: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 17s 3s/step - loss: 0.0314 - accuracy: 0.9891 - val_loss: 0.1885 - val_accuracy: 0.9185\n",
            "Epoch 149/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9918\n",
            "Epoch 149: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0309 - accuracy: 0.9918 - val_loss: 0.1271 - val_accuracy: 0.9402\n",
            "Epoch 150/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9864\n",
            "Epoch 150: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0320 - accuracy: 0.9864 - val_loss: 0.1934 - val_accuracy: 0.9130\n",
            "Epoch 151/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9905\n",
            "Epoch 151: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0307 - accuracy: 0.9905 - val_loss: 0.1618 - val_accuracy: 0.9185\n",
            "Epoch 152/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9864\n",
            "Epoch 152: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 17s 3s/step - loss: 0.0347 - accuracy: 0.9864 - val_loss: 0.1495 - val_accuracy: 0.9239\n",
            "Epoch 153/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9864\n",
            "Epoch 153: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0351 - accuracy: 0.9864 - val_loss: 0.1909 - val_accuracy: 0.9130\n",
            "Epoch 154/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9878\n",
            "Epoch 154: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0315 - accuracy: 0.9878 - val_loss: 0.1558 - val_accuracy: 0.9239\n",
            "Epoch 155/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9905\n",
            "Epoch 155: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0291 - accuracy: 0.9905 - val_loss: 0.1818 - val_accuracy: 0.9130\n",
            "Epoch 156/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9905\n",
            "Epoch 156: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0293 - accuracy: 0.9905 - val_loss: 0.1643 - val_accuracy: 0.9185\n",
            "Epoch 157/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9918\n",
            "Epoch 157: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0294 - accuracy: 0.9918 - val_loss: 0.1607 - val_accuracy: 0.9185\n",
            "Epoch 158/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9918\n",
            "Epoch 158: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0274 - accuracy: 0.9918 - val_loss: 0.1354 - val_accuracy: 0.9348\n",
            "Epoch 159/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9891\n",
            "Epoch 159: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0327 - accuracy: 0.9891 - val_loss: 0.1940 - val_accuracy: 0.9130\n",
            "Epoch 160/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9918\n",
            "Epoch 160: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0285 - accuracy: 0.9918 - val_loss: 0.1561 - val_accuracy: 0.9239\n",
            "Epoch 161/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0293 - accuracy: 0.9891\n",
            "Epoch 161: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0293 - accuracy: 0.9891 - val_loss: 0.1296 - val_accuracy: 0.9402\n",
            "Epoch 162/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9864\n",
            "Epoch 162: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0344 - accuracy: 0.9864 - val_loss: 0.1608 - val_accuracy: 0.9239\n",
            "Epoch 163/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9891\n",
            "Epoch 163: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0298 - accuracy: 0.9891 - val_loss: 0.1692 - val_accuracy: 0.9239\n",
            "Epoch 164/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9918\n",
            "Epoch 164: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0277 - accuracy: 0.9918 - val_loss: 0.1255 - val_accuracy: 0.9457\n",
            "Epoch 165/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9918\n",
            "Epoch 165: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0327 - accuracy: 0.9918 - val_loss: 0.2031 - val_accuracy: 0.9130\n",
            "Epoch 166/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0248 - accuracy: 0.9905\n",
            "Epoch 166: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0248 - accuracy: 0.9905 - val_loss: 0.1466 - val_accuracy: 0.9293\n",
            "Epoch 167/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9891\n",
            "Epoch 167: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0314 - accuracy: 0.9891 - val_loss: 0.1757 - val_accuracy: 0.9239\n",
            "Epoch 168/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9878\n",
            "Epoch 168: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 17s 3s/step - loss: 0.0330 - accuracy: 0.9878 - val_loss: 0.1728 - val_accuracy: 0.9239\n",
            "Epoch 169/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9918\n",
            "Epoch 169: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0272 - accuracy: 0.9918 - val_loss: 0.1707 - val_accuracy: 0.9239\n",
            "Epoch 170/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9891\n",
            "Epoch 170: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0266 - accuracy: 0.9891 - val_loss: 0.1396 - val_accuracy: 0.9293\n",
            "Epoch 171/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9918\n",
            "Epoch 171: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0266 - accuracy: 0.9918 - val_loss: 0.2057 - val_accuracy: 0.9076\n",
            "Epoch 172/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9905\n",
            "Epoch 172: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0268 - accuracy: 0.9905 - val_loss: 0.1367 - val_accuracy: 0.9402\n",
            "Epoch 173/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9918\n",
            "Epoch 173: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0257 - accuracy: 0.9918 - val_loss: 0.1881 - val_accuracy: 0.9185\n",
            "Epoch 174/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9878\n",
            "Epoch 174: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0263 - accuracy: 0.9878 - val_loss: 0.1545 - val_accuracy: 0.9293\n",
            "Epoch 175/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9932\n",
            "Epoch 175: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0239 - accuracy: 0.9932 - val_loss: 0.1691 - val_accuracy: 0.9293\n",
            "Epoch 176/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9918\n",
            "Epoch 176: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0240 - accuracy: 0.9918 - val_loss: 0.1702 - val_accuracy: 0.9293\n",
            "Epoch 177/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9891\n",
            "Epoch 177: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0287 - accuracy: 0.9891 - val_loss: 0.1465 - val_accuracy: 0.9293\n",
            "Epoch 178/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9864\n",
            "Epoch 178: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0414 - accuracy: 0.9864 - val_loss: 0.1305 - val_accuracy: 0.9402\n",
            "Epoch 179/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9769\n",
            "Epoch 179: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0545 - accuracy: 0.9769 - val_loss: 0.0963 - val_accuracy: 0.9620\n",
            "Epoch 180/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9864\n",
            "Epoch 180: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0716 - accuracy: 0.9864 - val_loss: 0.0806 - val_accuracy: 0.9728\n",
            "Epoch 181/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9905\n",
            "Epoch 181: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 17s 3s/step - loss: 0.0345 - accuracy: 0.9905 - val_loss: 0.2481 - val_accuracy: 0.8913\n",
            "Epoch 182/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9891\n",
            "Epoch 182: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0331 - accuracy: 0.9891 - val_loss: 0.1263 - val_accuracy: 0.9457\n",
            "Epoch 183/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9878\n",
            "Epoch 183: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0349 - accuracy: 0.9878 - val_loss: 0.1220 - val_accuracy: 0.9457\n",
            "Epoch 184/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9891\n",
            "Epoch 184: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0344 - accuracy: 0.9891 - val_loss: 0.2069 - val_accuracy: 0.9185\n",
            "Epoch 185/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9905\n",
            "Epoch 185: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0279 - accuracy: 0.9905 - val_loss: 0.1339 - val_accuracy: 0.9293\n",
            "Epoch 186/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9918\n",
            "Epoch 186: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0266 - accuracy: 0.9918 - val_loss: 0.1360 - val_accuracy: 0.9293\n",
            "Epoch 187/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9905\n",
            "Epoch 187: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0257 - accuracy: 0.9905 - val_loss: 0.1553 - val_accuracy: 0.9293\n",
            "Epoch 188/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9918\n",
            "Epoch 188: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0277 - accuracy: 0.9918 - val_loss: 0.1062 - val_accuracy: 0.9511\n",
            "Epoch 189/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9905\n",
            "Epoch 189: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0285 - accuracy: 0.9905 - val_loss: 0.1531 - val_accuracy: 0.9293\n",
            "Epoch 190/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9932\n",
            "Epoch 190: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0264 - accuracy: 0.9932 - val_loss: 0.1227 - val_accuracy: 0.9402\n",
            "Epoch 191/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9905\n",
            "Epoch 191: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0277 - accuracy: 0.9905 - val_loss: 0.1196 - val_accuracy: 0.9402\n",
            "Epoch 192/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9918\n",
            "Epoch 192: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0246 - accuracy: 0.9918 - val_loss: 0.1368 - val_accuracy: 0.9402\n",
            "Epoch 193/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9918\n",
            "Epoch 193: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0243 - accuracy: 0.9918 - val_loss: 0.1471 - val_accuracy: 0.9348\n",
            "Epoch 194/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9905\n",
            "Epoch 194: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0238 - accuracy: 0.9905 - val_loss: 0.1171 - val_accuracy: 0.9457\n",
            "Epoch 195/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9891\n",
            "Epoch 195: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0244 - accuracy: 0.9891 - val_loss: 0.1319 - val_accuracy: 0.9402\n",
            "Epoch 196/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9918\n",
            "Epoch 196: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0228 - accuracy: 0.9918 - val_loss: 0.1409 - val_accuracy: 0.9348\n",
            "Epoch 197/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9905\n",
            "Epoch 197: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0250 - accuracy: 0.9905 - val_loss: 0.1446 - val_accuracy: 0.9402\n",
            "Epoch 198/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9918\n",
            "Epoch 198: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0235 - accuracy: 0.9918 - val_loss: 0.1111 - val_accuracy: 0.9457\n",
            "Epoch 199/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0198 - accuracy: 0.9918\n",
            "Epoch 199: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0198 - accuracy: 0.9918 - val_loss: 0.1425 - val_accuracy: 0.9348\n",
            "Epoch 200/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9891\n",
            "Epoch 200: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0267 - accuracy: 0.9891 - val_loss: 0.1162 - val_accuracy: 0.9457\n",
            "Epoch 201/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9891\n",
            "Epoch 201: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0252 - accuracy: 0.9891 - val_loss: 0.1486 - val_accuracy: 0.9348\n",
            "Epoch 202/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9946\n",
            "Epoch 202: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0224 - accuracy: 0.9946 - val_loss: 0.1312 - val_accuracy: 0.9402\n",
            "Epoch 203/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9891\n",
            "Epoch 203: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0257 - accuracy: 0.9891 - val_loss: 0.1331 - val_accuracy: 0.9402\n",
            "Epoch 204/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9932\n",
            "Epoch 204: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0218 - accuracy: 0.9932 - val_loss: 0.1523 - val_accuracy: 0.9293\n",
            "Epoch 205/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9905\n",
            "Epoch 205: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 17s 3s/step - loss: 0.0234 - accuracy: 0.9905 - val_loss: 0.1163 - val_accuracy: 0.9457\n",
            "Epoch 206/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9878\n",
            "Epoch 206: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0288 - accuracy: 0.9878 - val_loss: 0.1107 - val_accuracy: 0.9511\n",
            "Epoch 207/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0277 - accuracy: 0.9905\n",
            "Epoch 207: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0277 - accuracy: 0.9905 - val_loss: 0.1814 - val_accuracy: 0.9185\n",
            "Epoch 208/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9946\n",
            "Epoch 208: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0155 - accuracy: 0.9946 - val_loss: 0.0966 - val_accuracy: 0.9511\n",
            "Epoch 209/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9891\n",
            "Epoch 209: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0317 - accuracy: 0.9891 - val_loss: 0.1364 - val_accuracy: 0.9348\n",
            "Epoch 210/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9932\n",
            "Epoch 210: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0221 - accuracy: 0.9932 - val_loss: 0.1595 - val_accuracy: 0.9293\n",
            "Epoch 211/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.9932\n",
            "Epoch 211: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0187 - accuracy: 0.9932 - val_loss: 0.1235 - val_accuracy: 0.9457\n",
            "Epoch 212/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9918\n",
            "Epoch 212: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0228 - accuracy: 0.9918 - val_loss: 0.1339 - val_accuracy: 0.9402\n",
            "Epoch 213/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9918\n",
            "Epoch 213: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 17s 3s/step - loss: 0.0246 - accuracy: 0.9918 - val_loss: 0.1620 - val_accuracy: 0.9293\n",
            "Epoch 214/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9905\n",
            "Epoch 214: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0224 - accuracy: 0.9905 - val_loss: 0.1056 - val_accuracy: 0.9511\n",
            "Epoch 215/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9891\n",
            "Epoch 215: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0251 - accuracy: 0.9891 - val_loss: 0.1542 - val_accuracy: 0.9293\n",
            "Epoch 216/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9905\n",
            "Epoch 216: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0236 - accuracy: 0.9905 - val_loss: 0.1384 - val_accuracy: 0.9348\n",
            "Epoch 217/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9905\n",
            "Epoch 217: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 17s 3s/step - loss: 0.0232 - accuracy: 0.9905 - val_loss: 0.1285 - val_accuracy: 0.9348\n",
            "Epoch 218/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9932\n",
            "Epoch 218: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0201 - accuracy: 0.9932 - val_loss: 0.1584 - val_accuracy: 0.9293\n",
            "Epoch 219/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9918\n",
            "Epoch 219: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0206 - accuracy: 0.9918 - val_loss: 0.1264 - val_accuracy: 0.9348\n",
            "Epoch 220/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9918\n",
            "Epoch 220: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0202 - accuracy: 0.9918 - val_loss: 0.1555 - val_accuracy: 0.9293\n",
            "Epoch 221/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9932\n",
            "Epoch 221: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0238 - accuracy: 0.9932 - val_loss: 0.1165 - val_accuracy: 0.9457\n",
            "Epoch 222/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9918\n",
            "Epoch 222: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0213 - accuracy: 0.9918 - val_loss: 0.1228 - val_accuracy: 0.9348\n",
            "Epoch 223/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9918\n",
            "Epoch 223: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0227 - accuracy: 0.9918 - val_loss: 0.1680 - val_accuracy: 0.9239\n",
            "Epoch 224/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9891\n",
            "Epoch 224: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0226 - accuracy: 0.9891 - val_loss: 0.1222 - val_accuracy: 0.9402\n",
            "Epoch 225/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9932\n",
            "Epoch 225: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 0.1275 - val_accuracy: 0.9402\n",
            "Epoch 226/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9918\n",
            "Epoch 226: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0192 - accuracy: 0.9918 - val_loss: 0.1663 - val_accuracy: 0.9293\n",
            "Epoch 227/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9918\n",
            "Epoch 227: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0199 - accuracy: 0.9918 - val_loss: 0.1304 - val_accuracy: 0.9402\n",
            "Epoch 228/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0175 - accuracy: 0.9932\n",
            "Epoch 228: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0175 - accuracy: 0.9932 - val_loss: 0.1485 - val_accuracy: 0.9293\n",
            "Epoch 229/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9905\n",
            "Epoch 229: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0224 - accuracy: 0.9905 - val_loss: 0.1122 - val_accuracy: 0.9511\n",
            "Epoch 230/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9851\n",
            "Epoch 230: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0278 - accuracy: 0.9851 - val_loss: 0.1356 - val_accuracy: 0.9348\n",
            "Epoch 231/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9878\n",
            "Epoch 231: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0349 - accuracy: 0.9878 - val_loss: 0.1195 - val_accuracy: 0.9511\n",
            "Epoch 232/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9878\n",
            "Epoch 232: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0302 - accuracy: 0.9878 - val_loss: 0.1159 - val_accuracy: 0.9511\n",
            "Epoch 233/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9891\n",
            "Epoch 233: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0332 - accuracy: 0.9891 - val_loss: 0.0882 - val_accuracy: 0.9728\n",
            "Epoch 234/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9891\n",
            "Epoch 234: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0285 - accuracy: 0.9891 - val_loss: 0.2203 - val_accuracy: 0.9185\n",
            "Epoch 235/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9918\n",
            "Epoch 235: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0231 - accuracy: 0.9918 - val_loss: 0.1060 - val_accuracy: 0.9565\n",
            "Epoch 236/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9932\n",
            "Epoch 236: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0214 - accuracy: 0.9932 - val_loss: 0.1646 - val_accuracy: 0.9293\n",
            "Epoch 237/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9946\n",
            "Epoch 237: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0193 - accuracy: 0.9946 - val_loss: 0.1260 - val_accuracy: 0.9402\n",
            "Epoch 238/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9932\n",
            "Epoch 238: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0189 - accuracy: 0.9932 - val_loss: 0.1411 - val_accuracy: 0.9348\n",
            "Epoch 239/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9905\n",
            "Epoch 239: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0238 - accuracy: 0.9905 - val_loss: 0.1336 - val_accuracy: 0.9348\n",
            "Epoch 240/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9891\n",
            "Epoch 240: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0320 - accuracy: 0.9891 - val_loss: 0.1365 - val_accuracy: 0.9348\n",
            "Epoch 241/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9918\n",
            "Epoch 241: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0281 - accuracy: 0.9918 - val_loss: 0.0936 - val_accuracy: 0.9674\n",
            "Epoch 242/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9905\n",
            "Epoch 242: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0274 - accuracy: 0.9905 - val_loss: 0.2111 - val_accuracy: 0.9239\n",
            "Epoch 243/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9918\n",
            "Epoch 243: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0186 - accuracy: 0.9918 - val_loss: 0.1076 - val_accuracy: 0.9511\n",
            "Epoch 244/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0235 - accuracy: 0.9905\n",
            "Epoch 244: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0235 - accuracy: 0.9905 - val_loss: 0.1515 - val_accuracy: 0.9293\n",
            "Epoch 245/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9918\n",
            "Epoch 245: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0218 - accuracy: 0.9918 - val_loss: 0.1621 - val_accuracy: 0.9239\n",
            "Epoch 246/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9946\n",
            "Epoch 246: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0147 - accuracy: 0.9946 - val_loss: 0.1158 - val_accuracy: 0.9457\n",
            "Epoch 247/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0208 - accuracy: 0.9932\n",
            "Epoch 247: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0208 - accuracy: 0.9932 - val_loss: 0.1488 - val_accuracy: 0.9293\n",
            "Epoch 248/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0144 - accuracy: 0.9918\n",
            "Epoch 248: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0144 - accuracy: 0.9918 - val_loss: 0.1421 - val_accuracy: 0.9348\n",
            "Epoch 249/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9905\n",
            "Epoch 249: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 15s 3s/step - loss: 0.0165 - accuracy: 0.9905 - val_loss: 0.1471 - val_accuracy: 0.9348\n",
            "Epoch 250/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9918\n",
            "Epoch 250: val_accuracy did not improve from 0.98370\n",
            "6/6 [==============================] - 16s 3s/step - loss: 0.0193 - accuracy: 0.9918 - val_loss: 0.1676 - val_accuracy: 0.9293\n",
            "Training completed in time:  1:05:23.243335\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "num_epochs = 250\n",
        "num_batch_size = 128\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        filepath='mymodel2_{epoch:02d}.h5',\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_accuracy` score has improved.\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=1)\n",
        "]\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
        "          validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n",
        "\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v2oMG0wh3MM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf4814a-4c96-4629-a8b0-99baa8adb82f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  0.967391312122345\n",
            "Testing Accuracy:  0.929347813129425\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt1PdTurh6Gt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5af9bc-f6e3-40d4-bced-f148aef49b60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 1s 144ms/step\n"
          ]
        }
      ],
      "source": [
        "preds = model.predict(x_test) # label scores\n",
        "\n",
        "classpreds = np.argmax(preds, axis=1) # predicted classes\n",
        "\n",
        "y_testclass = np.argmax(y_test, axis=1) # true classes\n",
        "\n",
        "n_classes=6 # number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfnTg3zeidzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b41f435-2857-4a10-fdb0-624f404b67ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "y_testclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eitC745jifuT"
      },
      "outputs": [],
      "source": [
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(2):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], preds[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVmVmTYhiiMK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "d0110da9-9198-4fed-8d68-b9996fb4d5be"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "c_names = [\"NO\", 'Bronchiolitis']\n",
        "\n",
        "# Plot ROC curves\n",
        "fig, ax = plt.subplots(figsize=(16, 10))\n",
        "ax.plot([0, 1], [0, 1], 'k--')\n",
        "ax.set_xlim([0.0, 1.0])\n",
        "ax.set_ylim([0.0, 1.05])\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curve for Each Class')\n",
        "for i in range(2):\n",
        "    ax.plot(fpr[i], tpr[i], linewidth=3, label='ROC curve (area = %0.2f) for %s' % (roc_auc[i], c_names[i]))\n",
        "ax.legend(loc=\"best\", fontsize='x-large')\n",
        "ax.grid(alpha=.4)\n",
        "sns.despine()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIwQjXdhikaO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc5e934-9f8d-4494-922d-45315f92a6c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NO       0.08      0.33      0.13         3\n",
            "        URTI       0.99      0.94      0.96       181\n",
            "\n",
            "    accuracy                           0.93       184\n",
            "   macro avg       0.54      0.64      0.55       184\n",
            "weighted avg       0.97      0.93      0.95       184\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Classification Report\n",
        "print(classification_report(y_testclass, classpreds, target_names=c_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQAcrVtQim3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98c60a9d-5b8f-4d6f-eb0c-0d3fafb8374a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1   2]\n",
            " [ 11 170]]\n"
          ]
        }
      ],
      "source": [
        "# Confusion Matrix\n",
        "print(confusion_matrix(y_testclass, classpreds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zs2m2lNKipHX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "87673e6f-25f4-4c2e-b59f-bf624e9544f9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "cm = confusion_matrix(y_testclass, classpreds)\n",
        "matrix_index = [\"Bronchiolitis\",\"No Bronchiolitis\"]\n",
        "df_cm = pd.DataFrame(cm, index = matrix_index, columns = matrix_index)\n",
        "df_cm.index.name = 'Actual'\n",
        "df_cm.columns.name = 'Predicted'\n",
        "fig, ax = plt.subplots(figsize=(10,7))\n",
        "\n",
        "cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "cm_perc = cm / cm_sum.astype(float) * 100\n",
        "annot = np.empty_like(cm).astype(str)\n",
        "nrows, ncols = cm.shape\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        c = cm[i, j]\n",
        "        p = cm_perc[i, j]\n",
        "        if i == j:\n",
        "            s = cm_sum[i]\n",
        "            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "        elif c == 0:\n",
        "            annot[i, j] = ''\n",
        "        else:\n",
        "            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "\n",
        "sn.heatmap(df_cm, annot=annot, fmt='')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYtyK2P-iro4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}