{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O37AJd3aMwMM",
        "outputId": "3054d00c-3620-45a5-db9a-6a2795276a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.7.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.6)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjQ0O6MzfJYu",
        "outputId": "1d882e1a-bced-4bb8-e3ec-8db73077e44a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Load various imports\n",
        "from datetime import datetime\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GVHPKtVlwxQ"
      },
      "outputs": [],
      "source": [
        "mypath = \"/content/gdrive/MyDrive/Major_Project/ICBHI_final_database/audio_and_txt_files/\"\n",
        "filenames = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f.endswith('.wav'))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCOe50bvlzVm"
      },
      "outputs": [],
      "source": [
        "p_id_in_file = [] # patient IDs corresponding to each file\n",
        "for name in filenames:\n",
        "    p_id_in_file.append(int(name[:3]))\n",
        "\n",
        "p_id_in_file = np.array(p_id_in_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2Ux2Vz0l1Rf"
      },
      "outputs": [],
      "source": [
        "max_pad_len = 862 # to make the length of all MFCC equal\n",
        "\n",
        "def extract_features(file_name):\n",
        "    \"\"\"\n",
        "    This function takes in the path for an audio file as a string, loads it, and returns the MFCC\n",
        "    of the audio\"\"\"\n",
        "\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, duration=20)\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        pad_width = max_pad_len - mfccs.shape[1]\n",
        "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file_name)\n",
        "        return None\n",
        "\n",
        "    return mfccs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHlRodIfl6ug"
      },
      "outputs": [],
      "source": [
        "filepaths = [join(mypath, f) for f in filenames] # full paths of files\n",
        "p_diag = pd.read_csv(\"/content/gdrive/MyDrive/Major_Project/ICBHI_final_database/patient_diagnosis.csv\") # patient diagnosis file # labels for audio files\n",
        "labels = []\n",
        "for x in p_id_in_file:\n",
        "  labels.append(p_diag[p_diag['patient_Id']==x]['Disease'].values[0])\n",
        "label = []\n",
        "for i in  labels:\n",
        "    if i != 'Healthy':\n",
        "        i=\"NO\"\n",
        "    label.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG0_cAfzl8uX",
        "outputId": "2932014c-812f-4b65-b932-5250b9c71b8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n"
          ]
        }
      ],
      "source": [
        "count=0\n",
        "for i in labels:\n",
        "  if(i=='Healthy'):\n",
        "    count+=1\n",
        "print(count)\n",
        "labels=np.array(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tU6131hxl-tv",
        "outputId": "411d132b-11da-4d0c-a76b-6238346a2b4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished feature extraction from  920  files\n"
          ]
        }
      ],
      "source": [
        "features = []\n",
        "\n",
        "# Iterate through each sound file and extract the features\n",
        "for file_name in filepaths:\n",
        "    data = extract_features(file_name)\n",
        "    features.append(data)\n",
        "\n",
        "print('Finished feature extraction from ', len(features), ' files')\n",
        "features = np.array(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQDbBkIumHLp",
        "outputId": "dd1aa665-f7b2-4958-9d3b-23c57e2c2e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Healthy' 'NO']\n",
            " ['35' '885']]\n"
          ]
        }
      ],
      "source": [
        "features= np.array(features) # convert to numpy array\n",
        "\n",
        "# delete the very rare diseases\n",
        "features1 = np.delete(features, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)\n",
        "\n",
        "labels1 = np.delete(labels, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)\n",
        "\n",
        "# print class counts\n",
        "unique_elements, counts_elements = np.unique(labels, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "4D6E48inmKu9",
        "outputId": "7c56bbc8-71b9-40e4-ec5b-2468a97fd683"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# plot class counts\n",
        "y_pos = np.arange(len(unique_elements))\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.bar(unique_elements, counts_elements, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, unique_elements)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Disease')\n",
        "plt.title('Disease Count in Sound Files (No Asthma or LRTI)')\n",
        "plt.show()\n",
        "\n",
        "# One-hot encode labels\n",
        "le = LabelEncoder()\n",
        "i_labels = le.fit_transform(labels)\n",
        "oh_labels = to_categorical(i_labels)\n",
        "\n",
        "# add channel dimension for CNN\n",
        "features1 = np.reshape(features, (*features.shape,1))\n",
        "\n",
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(features1, oh_labels, stratify=oh_labels,\n",
        "                                                    test_size=0.2, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6440Lci5mOCK"
      },
      "outputs": [],
      "source": [
        "num_rows = 40\n",
        "num_columns = 862\n",
        "num_channels = 1\n",
        "\n",
        "num_labels = oh_labels.shape[1]\n",
        "filter_size = 2\n",
        "\n",
        "# Construct model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=filter_size,\n",
        "                 input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfYQYXKQmQz4",
        "outputId": "c366451e-0221-4dd0-8ac6-c4a246955e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 39, 861, 16)       80        \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 19, 430, 16)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 19, 430, 16)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 18, 429, 32)       2080      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 9, 214, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 9, 214, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 213, 64)        8256      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 106, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 106, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 3, 105, 128)       32896     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 1, 52, 128)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1, 52, 128)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 128)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43,570\n",
            "Trainable params: 43,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "6/6 [==============================] - 2s 229ms/step - loss: 13.8404 - accuracy: 0.0380\n",
            "Pre-training accuracy: 3.8043%\n"
          ]
        }
      ],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# Display model architecture summary\n",
        "model.summary()\n",
        "\n",
        "# Calculate pre-training accuracy\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1A_TVUcomTBm",
        "outputId": "dbcae492-7aa1-4e5d-849f-b26da14ae718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 7.3850 - accuracy: 0.8098\n",
            "Epoch 1: val_accuracy improved from -inf to 0.96196, saving model to mymodel2_01.h5\n",
            "6/6 [==============================] - 23s 4s/step - loss: 7.3850 - accuracy: 0.8098 - val_loss: 2.1128 - val_accuracy: 0.9620\n",
            "Epoch 2/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 5.0439 - accuracy: 0.9620\n",
            "Epoch 2: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 5.0439 - accuracy: 0.9620 - val_loss: 1.8998 - val_accuracy: 0.9620\n",
            "Epoch 3/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 3.9758 - accuracy: 0.9620\n",
            "Epoch 3: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 3.9758 - accuracy: 0.9620 - val_loss: 1.2109 - val_accuracy: 0.9620\n",
            "Epoch 4/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.3833 - accuracy: 0.9620\n",
            "Epoch 4: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 2.3833 - accuracy: 0.9620 - val_loss: 0.6401 - val_accuracy: 0.9620\n",
            "Epoch 5/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.1020 - accuracy: 0.9620\n",
            "Epoch 5: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 1.1020 - accuracy: 0.9620 - val_loss: 0.1918 - val_accuracy: 0.9620\n",
            "Epoch 6/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4816 - accuracy: 0.8397\n",
            "Epoch 6: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.4816 - accuracy: 0.8397 - val_loss: 0.3353 - val_accuracy: 0.9620\n",
            "Epoch 7/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.9620\n",
            "Epoch 7: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.3455 - accuracy: 0.9620 - val_loss: 0.1818 - val_accuracy: 0.9620\n",
            "Epoch 8/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3675 - accuracy: 0.9620\n",
            "Epoch 8: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.3675 - accuracy: 0.9620 - val_loss: 0.2901 - val_accuracy: 0.9620\n",
            "Epoch 9/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2368 - accuracy: 0.9402\n",
            "Epoch 9: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.2368 - accuracy: 0.9402 - val_loss: 0.3081 - val_accuracy: 0.9620\n",
            "Epoch 10/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1967 - accuracy: 0.9606\n",
            "Epoch 10: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1967 - accuracy: 0.9606 - val_loss: 0.2050 - val_accuracy: 0.9620\n",
            "Epoch 11/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2261 - accuracy: 0.9592\n",
            "Epoch 11: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.2261 - accuracy: 0.9592 - val_loss: 0.3331 - val_accuracy: 0.9620\n",
            "Epoch 12/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1879 - accuracy: 0.9592\n",
            "Epoch 12: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1879 - accuracy: 0.9592 - val_loss: 0.1936 - val_accuracy: 0.9620\n",
            "Epoch 13/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1672 - accuracy: 0.9606\n",
            "Epoch 13: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1672 - accuracy: 0.9606 - val_loss: 0.2357 - val_accuracy: 0.9620\n",
            "Epoch 14/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9457\n",
            "Epoch 14: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 27s 4s/step - loss: 0.1815 - accuracy: 0.9457 - val_loss: 0.1763 - val_accuracy: 0.9620\n",
            "Epoch 15/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1647 - accuracy: 0.9579\n",
            "Epoch 15: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1647 - accuracy: 0.9579 - val_loss: 0.1671 - val_accuracy: 0.9620\n",
            "Epoch 16/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1673 - accuracy: 0.9511\n",
            "Epoch 16: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1673 - accuracy: 0.9511 - val_loss: 0.1469 - val_accuracy: 0.9620\n",
            "Epoch 17/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9565\n",
            "Epoch 17: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1379 - accuracy: 0.9565 - val_loss: 0.1255 - val_accuracy: 0.9620\n",
            "Epoch 18/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9606\n",
            "Epoch 18: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1381 - accuracy: 0.9606 - val_loss: 0.1366 - val_accuracy: 0.9620\n",
            "Epoch 19/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9497\n",
            "Epoch 19: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1337 - accuracy: 0.9497 - val_loss: 0.1222 - val_accuracy: 0.9620\n",
            "Epoch 20/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.9592\n",
            "Epoch 20: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1320 - accuracy: 0.9592 - val_loss: 0.1166 - val_accuracy: 0.9620\n",
            "Epoch 21/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.9443\n",
            "Epoch 21: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1413 - accuracy: 0.9443 - val_loss: 0.1146 - val_accuracy: 0.9620\n",
            "Epoch 22/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1324 - accuracy: 0.9565\n",
            "Epoch 22: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1324 - accuracy: 0.9565 - val_loss: 0.1159 - val_accuracy: 0.9620\n",
            "Epoch 23/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1480 - accuracy: 0.9389\n",
            "Epoch 23: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1480 - accuracy: 0.9389 - val_loss: 0.1064 - val_accuracy: 0.9620\n",
            "Epoch 24/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9606\n",
            "Epoch 24: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1279 - accuracy: 0.9606 - val_loss: 0.1176 - val_accuracy: 0.9620\n",
            "Epoch 25/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1235 - accuracy: 0.9538\n",
            "Epoch 25: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1235 - accuracy: 0.9538 - val_loss: 0.1068 - val_accuracy: 0.9620\n",
            "Epoch 26/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1257 - accuracy: 0.9579\n",
            "Epoch 26: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1257 - accuracy: 0.9579 - val_loss: 0.1051 - val_accuracy: 0.9620\n",
            "Epoch 27/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9443\n",
            "Epoch 27: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1338 - accuracy: 0.9443 - val_loss: 0.1076 - val_accuracy: 0.9620\n",
            "Epoch 28/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9579\n",
            "Epoch 28: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1260 - accuracy: 0.9579 - val_loss: 0.1063 - val_accuracy: 0.9620\n",
            "Epoch 29/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9579\n",
            "Epoch 29: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1211 - accuracy: 0.9579 - val_loss: 0.1081 - val_accuracy: 0.9620\n",
            "Epoch 30/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1207 - accuracy: 0.9511\n",
            "Epoch 30: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1207 - accuracy: 0.9511 - val_loss: 0.1126 - val_accuracy: 0.9620\n",
            "Epoch 31/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9606\n",
            "Epoch 31: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1289 - accuracy: 0.9606 - val_loss: 0.1069 - val_accuracy: 0.9620\n",
            "Epoch 32/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1134 - accuracy: 0.9579\n",
            "Epoch 32: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1134 - accuracy: 0.9579 - val_loss: 0.1085 - val_accuracy: 0.9620\n",
            "Epoch 33/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1279 - accuracy: 0.9633\n",
            "Epoch 33: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1279 - accuracy: 0.9633 - val_loss: 0.1051 - val_accuracy: 0.9620\n",
            "Epoch 34/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1168 - accuracy: 0.9511\n",
            "Epoch 34: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1168 - accuracy: 0.9511 - val_loss: 0.1126 - val_accuracy: 0.9620\n",
            "Epoch 35/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9606\n",
            "Epoch 35: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1148 - accuracy: 0.9606 - val_loss: 0.1084 - val_accuracy: 0.9620\n",
            "Epoch 36/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9511\n",
            "Epoch 36: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1196 - accuracy: 0.9511 - val_loss: 0.1161 - val_accuracy: 0.9620\n",
            "Epoch 37/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9701\n",
            "Epoch 37: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1029 - accuracy: 0.9701 - val_loss: 0.1088 - val_accuracy: 0.9620\n",
            "Epoch 38/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9552\n",
            "Epoch 38: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1268 - accuracy: 0.9552 - val_loss: 0.1142 - val_accuracy: 0.9620\n",
            "Epoch 39/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9660\n",
            "Epoch 39: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0986 - accuracy: 0.9660 - val_loss: 0.1078 - val_accuracy: 0.9565\n",
            "Epoch 40/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1447 - accuracy: 0.9402\n",
            "Epoch 40: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1447 - accuracy: 0.9402 - val_loss: 0.1237 - val_accuracy: 0.9620\n",
            "Epoch 41/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9620\n",
            "Epoch 41: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.1247 - accuracy: 0.9620 - val_loss: 0.1089 - val_accuracy: 0.9565\n",
            "Epoch 42/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9565\n",
            "Epoch 42: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 28s 5s/step - loss: 0.1014 - accuracy: 0.9565 - val_loss: 0.1189 - val_accuracy: 0.9620\n",
            "Epoch 43/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9633\n",
            "Epoch 43: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1113 - accuracy: 0.9633 - val_loss: 0.1066 - val_accuracy: 0.9620\n",
            "Epoch 44/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9538\n",
            "Epoch 44: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1081 - accuracy: 0.9538 - val_loss: 0.1183 - val_accuracy: 0.9620\n",
            "Epoch 45/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1356 - accuracy: 0.9647\n",
            "Epoch 45: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1356 - accuracy: 0.9647 - val_loss: 0.1090 - val_accuracy: 0.9620\n",
            "Epoch 46/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9321\n",
            "Epoch 46: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1491 - accuracy: 0.9321 - val_loss: 0.1209 - val_accuracy: 0.9620\n",
            "Epoch 47/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1411 - accuracy: 0.9620\n",
            "Epoch 47: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1411 - accuracy: 0.9620 - val_loss: 0.1183 - val_accuracy: 0.9620\n",
            "Epoch 48/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9538\n",
            "Epoch 48: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1012 - accuracy: 0.9538 - val_loss: 0.1064 - val_accuracy: 0.9620\n",
            "Epoch 49/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9688\n",
            "Epoch 49: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1141 - accuracy: 0.9688 - val_loss: 0.1144 - val_accuracy: 0.9620\n",
            "Epoch 50/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9674\n",
            "Epoch 50: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0822 - accuracy: 0.9674 - val_loss: 0.1075 - val_accuracy: 0.9620\n",
            "Epoch 51/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1068 - accuracy: 0.9592\n",
            "Epoch 51: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1068 - accuracy: 0.9592 - val_loss: 0.1154 - val_accuracy: 0.9620\n",
            "Epoch 52/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9633\n",
            "Epoch 52: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0948 - accuracy: 0.9633 - val_loss: 0.1062 - val_accuracy: 0.9565\n",
            "Epoch 53/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.9565\n",
            "Epoch 53: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1023 - accuracy: 0.9565 - val_loss: 0.1186 - val_accuracy: 0.9620\n",
            "Epoch 54/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9688\n",
            "Epoch 54: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0979 - accuracy: 0.9688 - val_loss: 0.1106 - val_accuracy: 0.9620\n",
            "Epoch 55/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9647\n",
            "Epoch 55: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0878 - accuracy: 0.9647 - val_loss: 0.1080 - val_accuracy: 0.9620\n",
            "Epoch 56/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1102 - accuracy: 0.9497\n",
            "Epoch 56: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1102 - accuracy: 0.9497 - val_loss: 0.1240 - val_accuracy: 0.9620\n",
            "Epoch 57/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9647\n",
            "Epoch 57: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1145 - accuracy: 0.9647 - val_loss: 0.1131 - val_accuracy: 0.9620\n",
            "Epoch 58/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1058 - accuracy: 0.9579\n",
            "Epoch 58: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1058 - accuracy: 0.9579 - val_loss: 0.1217 - val_accuracy: 0.9620\n",
            "Epoch 59/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9633\n",
            "Epoch 59: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0865 - accuracy: 0.9633 - val_loss: 0.1145 - val_accuracy: 0.9565\n",
            "Epoch 60/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9688\n",
            "Epoch 60: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0875 - accuracy: 0.9688 - val_loss: 0.1214 - val_accuracy: 0.9620\n",
            "Epoch 61/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9633\n",
            "Epoch 61: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0823 - accuracy: 0.9633 - val_loss: 0.1141 - val_accuracy: 0.9620\n",
            "Epoch 62/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9633\n",
            "Epoch 62: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0928 - accuracy: 0.9633 - val_loss: 0.1168 - val_accuracy: 0.9620\n",
            "Epoch 63/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9660\n",
            "Epoch 63: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0908 - accuracy: 0.9660 - val_loss: 0.1236 - val_accuracy: 0.9620\n",
            "Epoch 64/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9633\n",
            "Epoch 64: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0833 - accuracy: 0.9633 - val_loss: 0.1197 - val_accuracy: 0.9620\n",
            "Epoch 65/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9701\n",
            "Epoch 65: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0768 - accuracy: 0.9701 - val_loss: 0.1247 - val_accuracy: 0.9620\n",
            "Epoch 66/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9688\n",
            "Epoch 66: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0855 - accuracy: 0.9688 - val_loss: 0.1148 - val_accuracy: 0.9620\n",
            "Epoch 67/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0908 - accuracy: 0.9647\n",
            "Epoch 67: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0908 - accuracy: 0.9647 - val_loss: 0.1315 - val_accuracy: 0.9620\n",
            "Epoch 68/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9660\n",
            "Epoch 68: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0895 - accuracy: 0.9660 - val_loss: 0.1236 - val_accuracy: 0.9620\n",
            "Epoch 69/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9647\n",
            "Epoch 69: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 28s 4s/step - loss: 0.0774 - accuracy: 0.9647 - val_loss: 0.1313 - val_accuracy: 0.9620\n",
            "Epoch 70/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0878 - accuracy: 0.9660\n",
            "Epoch 70: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0878 - accuracy: 0.9660 - val_loss: 0.1176 - val_accuracy: 0.9620\n",
            "Epoch 71/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9606\n",
            "Epoch 71: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1014 - accuracy: 0.9606 - val_loss: 0.1352 - val_accuracy: 0.9620\n",
            "Epoch 72/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9592\n",
            "Epoch 72: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0958 - accuracy: 0.9592 - val_loss: 0.1163 - val_accuracy: 0.9620\n",
            "Epoch 73/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9647\n",
            "Epoch 73: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0772 - accuracy: 0.9647 - val_loss: 0.1273 - val_accuracy: 0.9620\n",
            "Epoch 74/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.9524\n",
            "Epoch 74: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0991 - accuracy: 0.9524 - val_loss: 0.1211 - val_accuracy: 0.9620\n",
            "Epoch 75/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1043 - accuracy: 0.9688\n",
            "Epoch 75: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1043 - accuracy: 0.9688 - val_loss: 0.1271 - val_accuracy: 0.9620\n",
            "Epoch 76/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9660\n",
            "Epoch 76: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0778 - accuracy: 0.9660 - val_loss: 0.1150 - val_accuracy: 0.9620\n",
            "Epoch 77/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9701\n",
            "Epoch 77: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0735 - accuracy: 0.9701 - val_loss: 0.1265 - val_accuracy: 0.9620\n",
            "Epoch 78/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9674\n",
            "Epoch 78: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0789 - accuracy: 0.9674 - val_loss: 0.1156 - val_accuracy: 0.9620\n",
            "Epoch 79/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9701\n",
            "Epoch 79: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0839 - accuracy: 0.9701 - val_loss: 0.1170 - val_accuracy: 0.9620\n",
            "Epoch 80/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9660\n",
            "Epoch 80: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0783 - accuracy: 0.9660 - val_loss: 0.1212 - val_accuracy: 0.9620\n",
            "Epoch 81/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9674\n",
            "Epoch 81: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0859 - accuracy: 0.9674 - val_loss: 0.1152 - val_accuracy: 0.9620\n",
            "Epoch 82/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9688\n",
            "Epoch 82: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0870 - accuracy: 0.9688 - val_loss: 0.1250 - val_accuracy: 0.9620\n",
            "Epoch 83/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.9701\n",
            "Epoch 83: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0805 - accuracy: 0.9701 - val_loss: 0.1164 - val_accuracy: 0.9620\n",
            "Epoch 84/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9701\n",
            "Epoch 84: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0710 - accuracy: 0.9701 - val_loss: 0.1149 - val_accuracy: 0.9620\n",
            "Epoch 85/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9701\n",
            "Epoch 85: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0836 - accuracy: 0.9701 - val_loss: 0.1191 - val_accuracy: 0.9620\n",
            "Epoch 86/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9620\n",
            "Epoch 86: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0809 - accuracy: 0.9620 - val_loss: 0.1130 - val_accuracy: 0.9620\n",
            "Epoch 87/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9647\n",
            "Epoch 87: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0837 - accuracy: 0.9647 - val_loss: 0.1176 - val_accuracy: 0.9620\n",
            "Epoch 88/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0845 - accuracy: 0.9674\n",
            "Epoch 88: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0845 - accuracy: 0.9674 - val_loss: 0.1226 - val_accuracy: 0.9620\n",
            "Epoch 89/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9715\n",
            "Epoch 89: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0761 - accuracy: 0.9715 - val_loss: 0.1159 - val_accuracy: 0.9620\n",
            "Epoch 90/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9701\n",
            "Epoch 90: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0721 - accuracy: 0.9701 - val_loss: 0.1168 - val_accuracy: 0.9620\n",
            "Epoch 91/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9688\n",
            "Epoch 91: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0647 - accuracy: 0.9688 - val_loss: 0.1181 - val_accuracy: 0.9620\n",
            "Epoch 92/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9688\n",
            "Epoch 92: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0778 - accuracy: 0.9688 - val_loss: 0.1220 - val_accuracy: 0.9620\n",
            "Epoch 93/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9728\n",
            "Epoch 93: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0749 - accuracy: 0.9728 - val_loss: 0.1157 - val_accuracy: 0.9620\n",
            "Epoch 94/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9674\n",
            "Epoch 94: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0673 - accuracy: 0.9674 - val_loss: 0.1136 - val_accuracy: 0.9620\n",
            "Epoch 95/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9660\n",
            "Epoch 95: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0770 - accuracy: 0.9660 - val_loss: 0.1226 - val_accuracy: 0.9620\n",
            "Epoch 96/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0855 - accuracy: 0.9674\n",
            "Epoch 96: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 29s 5s/step - loss: 0.0855 - accuracy: 0.9674 - val_loss: 0.1176 - val_accuracy: 0.9620\n",
            "Epoch 97/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9674\n",
            "Epoch 97: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0892 - accuracy: 0.9674 - val_loss: 0.1172 - val_accuracy: 0.9620\n",
            "Epoch 98/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9701\n",
            "Epoch 98: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0825 - accuracy: 0.9701 - val_loss: 0.1194 - val_accuracy: 0.9620\n",
            "Epoch 99/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9688\n",
            "Epoch 99: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0854 - accuracy: 0.9688 - val_loss: 0.1102 - val_accuracy: 0.9620\n",
            "Epoch 100/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9660\n",
            "Epoch 100: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0739 - accuracy: 0.9660 - val_loss: 0.1214 - val_accuracy: 0.9620\n",
            "Epoch 101/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9728\n",
            "Epoch 101: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0681 - accuracy: 0.9728 - val_loss: 0.1144 - val_accuracy: 0.9620\n",
            "Epoch 102/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9715\n",
            "Epoch 102: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0685 - accuracy: 0.9715 - val_loss: 0.1210 - val_accuracy: 0.9620\n",
            "Epoch 103/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9701\n",
            "Epoch 103: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0876 - accuracy: 0.9701 - val_loss: 0.1162 - val_accuracy: 0.9620\n",
            "Epoch 104/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9647\n",
            "Epoch 104: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0737 - accuracy: 0.9647 - val_loss: 0.1166 - val_accuracy: 0.9620\n",
            "Epoch 105/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9769\n",
            "Epoch 105: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0750 - accuracy: 0.9769 - val_loss: 0.1143 - val_accuracy: 0.9620\n",
            "Epoch 106/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9728\n",
            "Epoch 106: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0672 - accuracy: 0.9728 - val_loss: 0.1092 - val_accuracy: 0.9620\n",
            "Epoch 107/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9715\n",
            "Epoch 107: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0766 - accuracy: 0.9715 - val_loss: 0.1093 - val_accuracy: 0.9620\n",
            "Epoch 108/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9728\n",
            "Epoch 108: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0642 - accuracy: 0.9728 - val_loss: 0.1130 - val_accuracy: 0.9620\n",
            "Epoch 109/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9688\n",
            "Epoch 109: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0668 - accuracy: 0.9688 - val_loss: 0.1146 - val_accuracy: 0.9620\n",
            "Epoch 110/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9715\n",
            "Epoch 110: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0776 - accuracy: 0.9715 - val_loss: 0.1128 - val_accuracy: 0.9620\n",
            "Epoch 111/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9647\n",
            "Epoch 111: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0799 - accuracy: 0.9647 - val_loss: 0.1162 - val_accuracy: 0.9620\n",
            "Epoch 112/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9715\n",
            "Epoch 112: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0782 - accuracy: 0.9715 - val_loss: 0.1128 - val_accuracy: 0.9620\n",
            "Epoch 113/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9688\n",
            "Epoch 113: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0714 - accuracy: 0.9688 - val_loss: 0.1134 - val_accuracy: 0.9620\n",
            "Epoch 114/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9742\n",
            "Epoch 114: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0732 - accuracy: 0.9742 - val_loss: 0.1111 - val_accuracy: 0.9620\n",
            "Epoch 115/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9742\n",
            "Epoch 115: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0668 - accuracy: 0.9742 - val_loss: 0.1154 - val_accuracy: 0.9620\n",
            "Epoch 116/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9755\n",
            "Epoch 116: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0669 - accuracy: 0.9755 - val_loss: 0.1106 - val_accuracy: 0.9620\n",
            "Epoch 117/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9647\n",
            "Epoch 117: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0734 - accuracy: 0.9647 - val_loss: 0.1195 - val_accuracy: 0.9620\n",
            "Epoch 118/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9701\n",
            "Epoch 118: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0755 - accuracy: 0.9701 - val_loss: 0.1098 - val_accuracy: 0.9620\n",
            "Epoch 119/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9715\n",
            "Epoch 119: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0734 - accuracy: 0.9715 - val_loss: 0.1133 - val_accuracy: 0.9620\n",
            "Epoch 120/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9688\n",
            "Epoch 120: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0712 - accuracy: 0.9688 - val_loss: 0.1146 - val_accuracy: 0.9620\n",
            "Epoch 121/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9701\n",
            "Epoch 121: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0808 - accuracy: 0.9701 - val_loss: 0.1111 - val_accuracy: 0.9620\n",
            "Epoch 122/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0634 - accuracy: 0.9728\n",
            "Epoch 122: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0634 - accuracy: 0.9728 - val_loss: 0.1115 - val_accuracy: 0.9620\n",
            "Epoch 123/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9715\n",
            "Epoch 123: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 28s 5s/step - loss: 0.0724 - accuracy: 0.9715 - val_loss: 0.1135 - val_accuracy: 0.9620\n",
            "Epoch 124/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9688\n",
            "Epoch 124: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0673 - accuracy: 0.9688 - val_loss: 0.1170 - val_accuracy: 0.9620\n",
            "Epoch 125/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9728\n",
            "Epoch 125: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0713 - accuracy: 0.9728 - val_loss: 0.1185 - val_accuracy: 0.9620\n",
            "Epoch 126/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9742\n",
            "Epoch 126: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0610 - accuracy: 0.9742 - val_loss: 0.1160 - val_accuracy: 0.9620\n",
            "Epoch 127/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9742\n",
            "Epoch 127: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0659 - accuracy: 0.9742 - val_loss: 0.1216 - val_accuracy: 0.9620\n",
            "Epoch 128/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9728\n",
            "Epoch 128: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0635 - accuracy: 0.9728 - val_loss: 0.1139 - val_accuracy: 0.9620\n",
            "Epoch 129/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9701\n",
            "Epoch 129: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0611 - accuracy: 0.9701 - val_loss: 0.1237 - val_accuracy: 0.9620\n",
            "Epoch 130/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9728\n",
            "Epoch 130: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0758 - accuracy: 0.9728 - val_loss: 0.1112 - val_accuracy: 0.9565\n",
            "Epoch 131/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9701\n",
            "Epoch 131: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0613 - accuracy: 0.9701 - val_loss: 0.1213 - val_accuracy: 0.9620\n",
            "Epoch 132/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9755\n",
            "Epoch 132: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0625 - accuracy: 0.9755 - val_loss: 0.1111 - val_accuracy: 0.9565\n",
            "Epoch 133/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9660\n",
            "Epoch 133: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0752 - accuracy: 0.9660 - val_loss: 0.1244 - val_accuracy: 0.9620\n",
            "Epoch 134/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9728\n",
            "Epoch 134: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0772 - accuracy: 0.9728 - val_loss: 0.1098 - val_accuracy: 0.9565\n",
            "Epoch 135/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0641 - accuracy: 0.9742\n",
            "Epoch 135: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0641 - accuracy: 0.9742 - val_loss: 0.1177 - val_accuracy: 0.9620\n",
            "Epoch 136/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9715\n",
            "Epoch 136: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0693 - accuracy: 0.9715 - val_loss: 0.1106 - val_accuracy: 0.9620\n",
            "Epoch 137/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0666 - accuracy: 0.9701\n",
            "Epoch 137: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0666 - accuracy: 0.9701 - val_loss: 0.1216 - val_accuracy: 0.9620\n",
            "Epoch 138/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9728\n",
            "Epoch 138: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0684 - accuracy: 0.9728 - val_loss: 0.1154 - val_accuracy: 0.9620\n",
            "Epoch 139/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9647\n",
            "Epoch 139: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0807 - accuracy: 0.9647 - val_loss: 0.1197 - val_accuracy: 0.9620\n",
            "Epoch 140/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9728\n",
            "Epoch 140: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0773 - accuracy: 0.9728 - val_loss: 0.1116 - val_accuracy: 0.9620\n",
            "Epoch 141/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9688\n",
            "Epoch 141: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0604 - accuracy: 0.9688 - val_loss: 0.1102 - val_accuracy: 0.9620\n",
            "Epoch 142/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9742\n",
            "Epoch 142: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0598 - accuracy: 0.9742 - val_loss: 0.1161 - val_accuracy: 0.9620\n",
            "Epoch 143/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9715\n",
            "Epoch 143: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0709 - accuracy: 0.9715 - val_loss: 0.1084 - val_accuracy: 0.9565\n",
            "Epoch 144/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9769\n",
            "Epoch 144: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0568 - accuracy: 0.9769 - val_loss: 0.1176 - val_accuracy: 0.9620\n",
            "Epoch 145/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9742\n",
            "Epoch 145: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0608 - accuracy: 0.9742 - val_loss: 0.1138 - val_accuracy: 0.9565\n",
            "Epoch 146/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9783\n",
            "Epoch 146: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0609 - accuracy: 0.9783 - val_loss: 0.1172 - val_accuracy: 0.9620\n",
            "Epoch 147/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9769\n",
            "Epoch 147: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0611 - accuracy: 0.9769 - val_loss: 0.1126 - val_accuracy: 0.9565\n",
            "Epoch 148/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9660\n",
            "Epoch 148: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0682 - accuracy: 0.9660 - val_loss: 0.1254 - val_accuracy: 0.9620\n",
            "Epoch 149/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9769\n",
            "Epoch 149: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0612 - accuracy: 0.9769 - val_loss: 0.1147 - val_accuracy: 0.9565\n",
            "Epoch 150/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9769\n",
            "Epoch 150: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0576 - accuracy: 0.9769 - val_loss: 0.1219 - val_accuracy: 0.9620\n",
            "Epoch 151/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9688\n",
            "Epoch 151: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 27s 4s/step - loss: 0.0613 - accuracy: 0.9688 - val_loss: 0.1155 - val_accuracy: 0.9565\n",
            "Epoch 152/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9660\n",
            "Epoch 152: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0680 - accuracy: 0.9660 - val_loss: 0.1248 - val_accuracy: 0.9620\n",
            "Epoch 153/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9688\n",
            "Epoch 153: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0676 - accuracy: 0.9688 - val_loss: 0.1151 - val_accuracy: 0.9565\n",
            "Epoch 154/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9728\n",
            "Epoch 154: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0625 - accuracy: 0.9728 - val_loss: 0.1154 - val_accuracy: 0.9565\n",
            "Epoch 155/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9701\n",
            "Epoch 155: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0584 - accuracy: 0.9701 - val_loss: 0.1191 - val_accuracy: 0.9620\n",
            "Epoch 156/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9769\n",
            "Epoch 156: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0555 - accuracy: 0.9769 - val_loss: 0.1172 - val_accuracy: 0.9565\n",
            "Epoch 157/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9701\n",
            "Epoch 157: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0603 - accuracy: 0.9701 - val_loss: 0.1188 - val_accuracy: 0.9565\n",
            "Epoch 158/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9728\n",
            "Epoch 158: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0551 - accuracy: 0.9728 - val_loss: 0.1160 - val_accuracy: 0.9565\n",
            "Epoch 159/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9755\n",
            "Epoch 159: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0663 - accuracy: 0.9755 - val_loss: 0.1238 - val_accuracy: 0.9620\n",
            "Epoch 160/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0693 - accuracy: 0.9755\n",
            "Epoch 160: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0693 - accuracy: 0.9755 - val_loss: 0.1124 - val_accuracy: 0.9565\n",
            "Epoch 161/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0743 - accuracy: 0.9620\n",
            "Epoch 161: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0743 - accuracy: 0.9620 - val_loss: 0.1183 - val_accuracy: 0.9620\n",
            "Epoch 162/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9796\n",
            "Epoch 162: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0542 - accuracy: 0.9796 - val_loss: 0.1115 - val_accuracy: 0.9565\n",
            "Epoch 163/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9742\n",
            "Epoch 163: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0593 - accuracy: 0.9742 - val_loss: 0.1117 - val_accuracy: 0.9565\n",
            "Epoch 164/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9755\n",
            "Epoch 164: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0591 - accuracy: 0.9755 - val_loss: 0.1125 - val_accuracy: 0.9565\n",
            "Epoch 165/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9783\n",
            "Epoch 165: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0563 - accuracy: 0.9783 - val_loss: 0.1118 - val_accuracy: 0.9565\n",
            "Epoch 166/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9769\n",
            "Epoch 166: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0508 - accuracy: 0.9769 - val_loss: 0.1107 - val_accuracy: 0.9565\n",
            "Epoch 167/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9810\n",
            "Epoch 167: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0610 - accuracy: 0.9810 - val_loss: 0.1095 - val_accuracy: 0.9565\n",
            "Epoch 168/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0607 - accuracy: 0.9701\n",
            "Epoch 168: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0607 - accuracy: 0.9701 - val_loss: 0.1129 - val_accuracy: 0.9565\n",
            "Epoch 169/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9769\n",
            "Epoch 169: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0567 - accuracy: 0.9769 - val_loss: 0.1111 - val_accuracy: 0.9565\n",
            "Epoch 170/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9755\n",
            "Epoch 170: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0590 - accuracy: 0.9755 - val_loss: 0.1094 - val_accuracy: 0.9565\n",
            "Epoch 171/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9823\n",
            "Epoch 171: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0549 - accuracy: 0.9823 - val_loss: 0.1200 - val_accuracy: 0.9565\n",
            "Epoch 172/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9755\n",
            "Epoch 172: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0526 - accuracy: 0.9755 - val_loss: 0.1142 - val_accuracy: 0.9565\n",
            "Epoch 173/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9796\n",
            "Epoch 173: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0475 - accuracy: 0.9796 - val_loss: 0.1120 - val_accuracy: 0.9565\n",
            "Epoch 174/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9742\n",
            "Epoch 174: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0502 - accuracy: 0.9742 - val_loss: 0.1182 - val_accuracy: 0.9565\n",
            "Epoch 175/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9728\n",
            "Epoch 175: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0655 - accuracy: 0.9728 - val_loss: 0.1146 - val_accuracy: 0.9565\n",
            "Epoch 176/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9715\n",
            "Epoch 176: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0704 - accuracy: 0.9715 - val_loss: 0.1179 - val_accuracy: 0.9565\n",
            "Epoch 177/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9660\n",
            "Epoch 177: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0755 - accuracy: 0.9660 - val_loss: 0.1226 - val_accuracy: 0.9565\n",
            "Epoch 178/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9728\n",
            "Epoch 178: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0692 - accuracy: 0.9728 - val_loss: 0.1090 - val_accuracy: 0.9565\n",
            "Epoch 179/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9728\n",
            "Epoch 179: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0654 - accuracy: 0.9728 - val_loss: 0.1128 - val_accuracy: 0.9565\n",
            "Epoch 180/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9796\n",
            "Epoch 180: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0630 - accuracy: 0.9796 - val_loss: 0.1138 - val_accuracy: 0.9565\n",
            "Epoch 181/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9688\n",
            "Epoch 181: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0538 - accuracy: 0.9688 - val_loss: 0.1028 - val_accuracy: 0.9565\n",
            "Epoch 182/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9728\n",
            "Epoch 182: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0579 - accuracy: 0.9728 - val_loss: 0.1117 - val_accuracy: 0.9565\n",
            "Epoch 183/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9769\n",
            "Epoch 183: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0581 - accuracy: 0.9769 - val_loss: 0.1094 - val_accuracy: 0.9565\n",
            "Epoch 184/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9633\n",
            "Epoch 184: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0797 - accuracy: 0.9633 - val_loss: 0.1109 - val_accuracy: 0.9565\n",
            "Epoch 185/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0754 - accuracy: 0.9728\n",
            "Epoch 185: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0754 - accuracy: 0.9728 - val_loss: 0.1133 - val_accuracy: 0.9565\n",
            "Epoch 186/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9592\n",
            "Epoch 186: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0828 - accuracy: 0.9592 - val_loss: 0.0991 - val_accuracy: 0.9565\n",
            "Epoch 187/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9620\n",
            "Epoch 187: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0992 - accuracy: 0.9620 - val_loss: 0.1137 - val_accuracy: 0.9620\n",
            "Epoch 188/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9810\n",
            "Epoch 188: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0679 - accuracy: 0.9810 - val_loss: 0.0921 - val_accuracy: 0.9565\n",
            "Epoch 189/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9674\n",
            "Epoch 189: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0676 - accuracy: 0.9674 - val_loss: 0.1016 - val_accuracy: 0.9565\n",
            "Epoch 190/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9715\n",
            "Epoch 190: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0615 - accuracy: 0.9715 - val_loss: 0.0945 - val_accuracy: 0.9565\n",
            "Epoch 191/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0576 - accuracy: 0.9688\n",
            "Epoch 191: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0576 - accuracy: 0.9688 - val_loss: 0.0974 - val_accuracy: 0.9565\n",
            "Epoch 192/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9783\n",
            "Epoch 192: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0501 - accuracy: 0.9783 - val_loss: 0.1015 - val_accuracy: 0.9565\n",
            "Epoch 193/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9769\n",
            "Epoch 193: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0455 - accuracy: 0.9769 - val_loss: 0.0980 - val_accuracy: 0.9565\n",
            "Epoch 194/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9769\n",
            "Epoch 194: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0546 - accuracy: 0.9769 - val_loss: 0.1051 - val_accuracy: 0.9565\n",
            "Epoch 195/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9715\n",
            "Epoch 195: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0549 - accuracy: 0.9715 - val_loss: 0.1058 - val_accuracy: 0.9565\n",
            "Epoch 196/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9755\n",
            "Epoch 196: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0553 - accuracy: 0.9755 - val_loss: 0.1077 - val_accuracy: 0.9565\n",
            "Epoch 197/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9823\n",
            "Epoch 197: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0467 - accuracy: 0.9823 - val_loss: 0.1114 - val_accuracy: 0.9565\n",
            "Epoch 198/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9823\n",
            "Epoch 198: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0489 - accuracy: 0.9823 - val_loss: 0.1110 - val_accuracy: 0.9565\n",
            "Epoch 199/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9796\n",
            "Epoch 199: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0501 - accuracy: 0.9796 - val_loss: 0.1120 - val_accuracy: 0.9565\n",
            "Epoch 200/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9837\n",
            "Epoch 200: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0464 - accuracy: 0.9837 - val_loss: 0.1149 - val_accuracy: 0.9565\n",
            "Epoch 201/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0499 - accuracy: 0.9755\n",
            "Epoch 201: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0499 - accuracy: 0.9755 - val_loss: 0.1191 - val_accuracy: 0.9565\n",
            "Epoch 202/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9769\n",
            "Epoch 202: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0526 - accuracy: 0.9769 - val_loss: 0.1155 - val_accuracy: 0.9565\n",
            "Epoch 203/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9783\n",
            "Epoch 203: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0483 - accuracy: 0.9783 - val_loss: 0.1176 - val_accuracy: 0.9565\n",
            "Epoch 204/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9837\n",
            "Epoch 204: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0467 - accuracy: 0.9837 - val_loss: 0.1197 - val_accuracy: 0.9565\n",
            "Epoch 205/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9769\n",
            "Epoch 205: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0490 - accuracy: 0.9769 - val_loss: 0.1189 - val_accuracy: 0.9565\n",
            "Epoch 206/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9796\n",
            "Epoch 206: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0491 - accuracy: 0.9796 - val_loss: 0.1200 - val_accuracy: 0.9565\n",
            "Epoch 207/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9837\n",
            "Epoch 207: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0430 - accuracy: 0.9837 - val_loss: 0.1194 - val_accuracy: 0.9565\n",
            "Epoch 208/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9783\n",
            "Epoch 208: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0458 - accuracy: 0.9783 - val_loss: 0.1155 - val_accuracy: 0.9565\n",
            "Epoch 209/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9796\n",
            "Epoch 209: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0463 - accuracy: 0.9796 - val_loss: 0.1217 - val_accuracy: 0.9565\n",
            "Epoch 210/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9755\n",
            "Epoch 210: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0542 - accuracy: 0.9755 - val_loss: 0.1198 - val_accuracy: 0.9565\n",
            "Epoch 211/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9769\n",
            "Epoch 211: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0472 - accuracy: 0.9769 - val_loss: 0.1239 - val_accuracy: 0.9565\n",
            "Epoch 212/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9755\n",
            "Epoch 212: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0525 - accuracy: 0.9755 - val_loss: 0.1209 - val_accuracy: 0.9565\n",
            "Epoch 213/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9783\n",
            "Epoch 213: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0493 - accuracy: 0.9783 - val_loss: 0.1245 - val_accuracy: 0.9565\n",
            "Epoch 214/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9755\n",
            "Epoch 214: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0533 - accuracy: 0.9755 - val_loss: 0.1182 - val_accuracy: 0.9565\n",
            "Epoch 215/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9851\n",
            "Epoch 215: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0425 - accuracy: 0.9851 - val_loss: 0.1226 - val_accuracy: 0.9565\n",
            "Epoch 216/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9851\n",
            "Epoch 216: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0366 - accuracy: 0.9851 - val_loss: 0.1240 - val_accuracy: 0.9565\n",
            "Epoch 217/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9796\n",
            "Epoch 217: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0441 - accuracy: 0.9796 - val_loss: 0.1201 - val_accuracy: 0.9565\n",
            "Epoch 218/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9823\n",
            "Epoch 218: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0462 - accuracy: 0.9823 - val_loss: 0.1292 - val_accuracy: 0.9565\n",
            "Epoch 219/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9837\n",
            "Epoch 219: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0427 - accuracy: 0.9837 - val_loss: 0.1217 - val_accuracy: 0.9565\n",
            "Epoch 220/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9823\n",
            "Epoch 220: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0430 - accuracy: 0.9823 - val_loss: 0.1309 - val_accuracy: 0.9565\n",
            "Epoch 221/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9823\n",
            "Epoch 221: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0401 - accuracy: 0.9823 - val_loss: 0.1212 - val_accuracy: 0.9565\n",
            "Epoch 222/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9742\n",
            "Epoch 222: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0509 - accuracy: 0.9742 - val_loss: 0.1339 - val_accuracy: 0.9565\n",
            "Epoch 223/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9796\n",
            "Epoch 223: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0473 - accuracy: 0.9796 - val_loss: 0.1243 - val_accuracy: 0.9565\n",
            "Epoch 224/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9810\n",
            "Epoch 224: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0415 - accuracy: 0.9810 - val_loss: 0.1252 - val_accuracy: 0.9565\n",
            "Epoch 225/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9864\n",
            "Epoch 225: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0385 - accuracy: 0.9864 - val_loss: 0.1288 - val_accuracy: 0.9565\n",
            "Epoch 226/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9837\n",
            "Epoch 226: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0403 - accuracy: 0.9837 - val_loss: 0.1268 - val_accuracy: 0.9565\n",
            "Epoch 227/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9823\n",
            "Epoch 227: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0381 - accuracy: 0.9823 - val_loss: 0.1323 - val_accuracy: 0.9565\n",
            "Epoch 228/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0430 - accuracy: 0.9837\n",
            "Epoch 228: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0430 - accuracy: 0.9837 - val_loss: 0.1270 - val_accuracy: 0.9565\n",
            "Epoch 229/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0399 - accuracy: 0.9837\n",
            "Epoch 229: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0399 - accuracy: 0.9837 - val_loss: 0.1285 - val_accuracy: 0.9565\n",
            "Epoch 230/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9864\n",
            "Epoch 230: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0413 - accuracy: 0.9864 - val_loss: 0.1333 - val_accuracy: 0.9565\n",
            "Epoch 231/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9823\n",
            "Epoch 231: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0392 - accuracy: 0.9823 - val_loss: 0.1272 - val_accuracy: 0.9565\n",
            "Epoch 232/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9783\n",
            "Epoch 232: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0435 - accuracy: 0.9783 - val_loss: 0.1309 - val_accuracy: 0.9565\n",
            "Epoch 233/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9851\n",
            "Epoch 233: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0356 - accuracy: 0.9851 - val_loss: 0.1295 - val_accuracy: 0.9565\n",
            "Epoch 234/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9823\n",
            "Epoch 234: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0350 - accuracy: 0.9823 - val_loss: 0.1307 - val_accuracy: 0.9565\n",
            "Epoch 235/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9891\n",
            "Epoch 235: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0339 - accuracy: 0.9891 - val_loss: 0.1312 - val_accuracy: 0.9565\n",
            "Epoch 236/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9851\n",
            "Epoch 236: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 3s/step - loss: 0.0404 - accuracy: 0.9851 - val_loss: 0.1272 - val_accuracy: 0.9565\n",
            "Epoch 237/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9769\n",
            "Epoch 237: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0514 - accuracy: 0.9769 - val_loss: 0.1354 - val_accuracy: 0.9565\n",
            "Epoch 238/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9823\n",
            "Epoch 238: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 20s 3s/step - loss: 0.0404 - accuracy: 0.9823 - val_loss: 0.1216 - val_accuracy: 0.9565\n",
            "Epoch 239/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9823\n",
            "Epoch 239: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0410 - accuracy: 0.9823 - val_loss: 0.1320 - val_accuracy: 0.9565\n",
            "Epoch 240/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0386 - accuracy: 0.9837\n",
            "Epoch 240: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0386 - accuracy: 0.9837 - val_loss: 0.1268 - val_accuracy: 0.9565\n",
            "Epoch 241/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9837\n",
            "Epoch 241: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0360 - accuracy: 0.9837 - val_loss: 0.1259 - val_accuracy: 0.9565\n",
            "Epoch 242/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9837\n",
            "Epoch 242: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0384 - accuracy: 0.9837 - val_loss: 0.1233 - val_accuracy: 0.9565\n",
            "Epoch 243/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9823\n",
            "Epoch 243: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0437 - accuracy: 0.9823 - val_loss: 0.1312 - val_accuracy: 0.9565\n",
            "Epoch 244/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9810\n",
            "Epoch 244: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0397 - accuracy: 0.9810 - val_loss: 0.1212 - val_accuracy: 0.9565\n",
            "Epoch 245/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9837\n",
            "Epoch 245: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 3s/step - loss: 0.0332 - accuracy: 0.9837 - val_loss: 0.1286 - val_accuracy: 0.9565\n",
            "Epoch 246/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9823\n",
            "Epoch 246: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0418 - accuracy: 0.9823 - val_loss: 0.1205 - val_accuracy: 0.9565\n",
            "Epoch 247/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9823\n",
            "Epoch 247: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0373 - accuracy: 0.9823 - val_loss: 0.1297 - val_accuracy: 0.9565\n",
            "Epoch 248/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9864\n",
            "Epoch 248: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0331 - accuracy: 0.9864 - val_loss: 0.1233 - val_accuracy: 0.9565\n",
            "Epoch 249/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9851\n",
            "Epoch 249: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0374 - accuracy: 0.9851 - val_loss: 0.1329 - val_accuracy: 0.9565\n",
            "Epoch 250/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9851\n",
            "Epoch 250: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0329 - accuracy: 0.9851 - val_loss: 0.1275 - val_accuracy: 0.9565\n",
            "Training completed in time:  1:31:23.388269\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "num_epochs = 250\n",
        "num_batch_size = 128\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        filepath='mymodel2_{epoch:02d}.h5',\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_accuracy` score has improved.\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=1)\n",
        "]\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
        "          validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n",
        "\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oWdTC3WCkmw"
      },
      "outputs": [],
      "source": [
        "model.save(\"Healthy_trained_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models\n",
        "model_1 = models.load_model(\"Healthy_trained_model.h5\")"
      ],
      "metadata": {
        "id": "IchLaVgMcJTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TiodZ_NmVfK",
        "outputId": "f75b3874-6a30-4403-ea59-534b7736fecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  0.9741848111152649\n",
            "Testing Accuracy:  0.95652174949646\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model_1.evaluate(x_train, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model_1.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0V7_xYHmYHn",
        "outputId": "91ae218c-c23f-4656-e4f5-71fbf3546db8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 2s 385ms/step\n"
          ]
        }
      ],
      "source": [
        "preds = model.predict(x_test) # label scores\n",
        "\n",
        "classpreds = np.argmax(preds, axis=1) # predicted classes\n",
        "\n",
        "y_testclass = np.argmax(y_test, axis=1) # true classes\n",
        "\n",
        "n_classes=6 # number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWJ74dQtmaSf"
      },
      "outputs": [],
      "source": [
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(2):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], preds[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "Z3HNA8wemcOg",
        "outputId": "dbfb2d2a-ec92-4b03-c45f-4f75dd4b6fe5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "c_names = [\"NO\", 'Healthy']\n",
        "\n",
        "# Plot ROC curves\n",
        "fig, ax = plt.subplots(figsize=(16, 10))\n",
        "ax.plot([0, 1], [0, 1], 'k--')\n",
        "ax.set_xlim([0.0, 1.0])\n",
        "ax.set_ylim([0.0, 1.05])\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curve for Each Class')\n",
        "for i in range(2):\n",
        "    ax.plot(fpr[i], tpr[i], linewidth=3, label='ROC curve (area = %0.2f) for %s' % (roc_auc[i], c_names[i]))\n",
        "ax.legend(loc=\"best\", fontsize='x-large')\n",
        "ax.grid(alpha=.4)\n",
        "sns.despine()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4iGiHUpmjcs",
        "outputId": "e6d53ca9-77a6-48f4-f637-9dbcb4fa7d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NO       0.50      0.29      0.36         7\n",
            "     Healthy       0.97      0.99      0.98       177\n",
            "\n",
            "    accuracy                           0.96       184\n",
            "   macro avg       0.74      0.64      0.67       184\n",
            "weighted avg       0.95      0.96      0.96       184\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Classification Report\n",
        "print(classification_report(y_testclass, classpreds, target_names=c_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_LSYhR3mkWQ",
        "outputId": "dffe43c8-0a4e-45a0-b60f-a1658f0f9d8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[  2   5]\n",
            " [  2 175]]\n"
          ]
        }
      ],
      "source": [
        "# Confusion Matrix\n",
        "print(confusion_matrix(y_testclass, classpreds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "6LM-S4BvmmhD",
        "outputId": "d3a9bd43-4c87-4009-96b3-c0b8fb52cb16"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "cm = confusion_matrix(y_testclass, classpreds)\n",
        "matrix_index = [\"Healthy\",\"No Healthy\"]\n",
        "df_cm = pd.DataFrame(cm, index = matrix_index, columns = matrix_index)\n",
        "df_cm.index.name = 'Actual'\n",
        "df_cm.columns.name = 'Predicted'\n",
        "fig, ax = plt.subplots(figsize=(10,7))\n",
        "\n",
        "cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "cm_perc = cm / cm_sum.astype(float) * 100\n",
        "annot = np.empty_like(cm).astype(str)\n",
        "nrows, ncols = cm.shape\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        c = cm[i, j]\n",
        "        p = cm_perc[i, j]\n",
        "        if i == j:\n",
        "            s = cm_sum[i]\n",
        "            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "        elif c == 0:\n",
        "            annot[i, j] = ''\n",
        "        else:\n",
        "            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "\n",
        "sn.heatmap(df_cm, annot=annot, fmt='')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FpCaNqT-msUZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}