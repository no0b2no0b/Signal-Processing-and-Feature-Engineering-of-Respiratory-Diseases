{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExbKbtDfarJ9",
        "outputId": "1e15b38f-94a9-4526-a1d9-aede79db2c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Load various imports\n",
        "from datetime import datetime\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mypath = \"/content/gdrive/MyDrive/Major_Project/ICBHI_final_database/audio_and_txt_files/\"\n",
        "filenames = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f.endswith('.wav'))]"
      ],
      "metadata": {
        "id": "zb3nabEUcNCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_id_in_file = [] # patient IDs corresponding to each file\n",
        "for name in filenames:\n",
        "    p_id_in_file.append(int(name[:3]))\n",
        "\n",
        "p_id_in_file = np.array(p_id_in_file)"
      ],
      "metadata": {
        "id": "3ZJpQCy-cPpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_pad_len = 862 # to make the length of all MFCC equal\n",
        "\n",
        "def extract_features(file_name):\n",
        "    \"\"\"\n",
        "    This function takes in the path for an audio file as a string, loads it, and returns the MFCC\n",
        "    of the audio\"\"\"\n",
        "\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast', duration=20)\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        pad_width = max_pad_len - mfccs.shape[1]\n",
        "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file_name)\n",
        "        return None\n",
        "\n",
        "    return mfccs"
      ],
      "metadata": {
        "id": "V2dBSMb9cSJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepaths = [join(mypath, f) for f in filenames] # full paths of files\n",
        "p_diag = pd.read_csv(\"/content/gdrive/MyDrive/Major_Project/ICBHI_final_database/patient_diagnosis.csv\") # patient diagnosis file\n",
        "labels = []\n",
        "for x in p_id_in_file:\n",
        "  labels.append(p_diag[p_diag['patient_Id']==x]['Disease'].values[0]) # labels for audio files\n",
        "label=[]\n",
        "for i in  labels:\n",
        "    if i != 'COPD':\n",
        "        i=\"NO\"\n",
        "    label.append(i)"
      ],
      "metadata": {
        "id": "SHUCPXORcirK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels=np.array(label)"
      ],
      "metadata": {
        "id": "r6hHBBPYdDw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "for i in labels:\n",
        "  if(i=='COPD'):\n",
        "    count+=1\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjcw_heJVuqn",
        "outputId": "2d0dcb82-c40e-4bd5-89f6-6f2db692da42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = []\n",
        "\n",
        "# Iterate through each sound file and extract the features\n",
        "for file_name in filepaths:\n",
        "    data = extract_features(file_name)\n",
        "    features.append(data)\n",
        "\n",
        "print('Finished feature extraction from ', len(features), ' files')\n",
        "features = np.array(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIsMHeLrdF43",
        "outputId": "0234daab-f987-408f-a0a6-62f976940187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished feature extraction from  920  files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features= np.array(features) # convert to numpy array"
      ],
      "metadata": {
        "id": "cIYWvtzKdKFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# delete the very rare diseases\n",
        "features1 = np.delete(features, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)\n",
        "\n",
        "labels1 = np.delete(labels, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)"
      ],
      "metadata": {
        "id": "s4avQqDPdMbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print class counts\n",
        "unique_elements, counts_elements = np.unique(labels, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtSYsDiidRlJ",
        "outputId": "1a309c76-b834-4860-ef51-bb6f6ab58734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['COPD' 'NO']\n",
            " ['793' '127']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot class counts\n",
        "y_pos = np.arange(len(unique_elements))\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.bar(unique_elements, counts_elements, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, unique_elements)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Disease')\n",
        "plt.title('Disease Count in Sound Files of COPD')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "AjyguSG5dUEW",
        "outputId": "b971c088-0755-400f-ad8a-0da40150b03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode labels\n",
        "le = LabelEncoder()\n",
        "i_labels = le.fit_transform(labels)\n",
        "oh_labels = to_categorical(i_labels)"
      ],
      "metadata": {
        "id": "DcXfqy04dY1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add channel dimension for CNN\n",
        "features1 = np.reshape(features, (*features.shape,1))"
      ],
      "metadata": {
        "id": "RTAjNpN0dfr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(features1, oh_labels, stratify=oh_labels,\n",
        "                                                    test_size=0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "0ZxokMEtdhuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Network (CNN) model architecture"
      ],
      "metadata": {
        "id": "fKcpzeVGdsGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = 40\n",
        "num_columns = 862\n",
        "num_channels = 1\n",
        "\n",
        "num_labels = oh_labels.shape[1]\n",
        "filter_size = 2\n",
        "\n",
        "# Construct model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=filter_size,\n",
        "                 input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "metadata": {
        "id": "j2gusnuOdj35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# Display model architecture summary\n",
        "model.summary()\n",
        "\n",
        "# Calculate pre-training accuracy\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wlo1M7ugSbc",
        "outputId": "20438278-6460-4ada-a583-674b3d06da20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 39, 861, 16)       80        \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 19, 430, 16)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 19, 430, 16)       0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 18, 429, 32)       2080      \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 9, 214, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 9, 214, 32)        0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 8, 213, 64)        8256      \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 4, 106, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 4, 106, 64)        0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 3, 105, 128)       32896     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 1, 52, 128)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 1, 52, 128)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d_1   (None, 128)              0         \n",
            " (GlobalAveragePooling2D)                                        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43,570\n",
            "Trainable params: 43,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "6/6 [==============================] - 3s 346ms/step - loss: 6.1877 - accuracy: 0.1359\n",
            "Pre-training accuracy: 13.5870%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n",
        "Here we will train the model. If we have a trained model, we can load it instead from the next cell."
      ],
      "metadata": {
        "id": "MLcxoAcwgcg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "num_epochs = 250\n",
        "num_batch_size = 128\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        filepath='mymodel2_{epoch:02d}.h5',\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_accuracy` score has improved.\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=1)\n",
        "]\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
        "          validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n",
        "\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgWWsNe8gXDf",
        "outputId": "2023d9d9-5749-4ec8-be33-894fdf964be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 6.7577 - accuracy: 0.6780\n",
            "Epoch 1: val_accuracy improved from -inf to 0.86413, saving model to mymodel2_01.h5\n",
            "6/6 [==============================] - 27s 4s/step - loss: 6.7577 - accuracy: 0.6780 - val_loss: 2.3413 - val_accuracy: 0.8641\n",
            "Epoch 2/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 4.3960 - accuracy: 0.8614\n",
            "Epoch 2: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 27s 5s/step - loss: 4.3960 - accuracy: 0.8614 - val_loss: 1.0150 - val_accuracy: 0.8641\n",
            "Epoch 3/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.6101 - accuracy: 0.8614\n",
            "Epoch 3: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 26s 5s/step - loss: 1.6101 - accuracy: 0.8614 - val_loss: 0.6628 - val_accuracy: 0.6467\n",
            "Epoch 4/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.8036 - accuracy: 0.4851\n",
            "Epoch 4: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.8036 - accuracy: 0.4851 - val_loss: 0.3982 - val_accuracy: 0.8641\n",
            "Epoch 5/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5871 - accuracy: 0.8614\n",
            "Epoch 5: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.5871 - accuracy: 0.8614 - val_loss: 0.3741 - val_accuracy: 0.8641\n",
            "Epoch 6/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.5310 - accuracy: 0.8614\n",
            "Epoch 6: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.5310 - accuracy: 0.8614 - val_loss: 0.4127 - val_accuracy: 0.8533\n",
            "Epoch 7/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.4015 - accuracy: 0.8438\n",
            "Epoch 7: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.4015 - accuracy: 0.8438 - val_loss: 0.4826 - val_accuracy: 0.8261\n",
            "Epoch 8/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3593 - accuracy: 0.8641\n",
            "Epoch 8: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.3593 - accuracy: 0.8641 - val_loss: 0.3818 - val_accuracy: 0.8641\n",
            "Epoch 9/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3644 - accuracy: 0.8614\n",
            "Epoch 9: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.3644 - accuracy: 0.8614 - val_loss: 0.3874 - val_accuracy: 0.8478\n",
            "Epoch 10/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3483 - accuracy: 0.8533\n",
            "Epoch 10: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.3483 - accuracy: 0.8533 - val_loss: 0.4325 - val_accuracy: 0.8315\n",
            "Epoch 11/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.8505\n",
            "Epoch 11: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.3216 - accuracy: 0.8505 - val_loss: 0.3836 - val_accuracy: 0.8207\n",
            "Epoch 12/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3093 - accuracy: 0.8505\n",
            "Epoch 12: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.3093 - accuracy: 0.8505 - val_loss: 0.3869 - val_accuracy: 0.8315\n",
            "Epoch 13/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.3072 - accuracy: 0.8560\n",
            "Epoch 13: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 27s 4s/step - loss: 0.3072 - accuracy: 0.8560 - val_loss: 0.3910 - val_accuracy: 0.8424\n",
            "Epoch 14/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2923 - accuracy: 0.8505\n",
            "Epoch 14: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.2923 - accuracy: 0.8505 - val_loss: 0.3542 - val_accuracy: 0.8315\n",
            "Epoch 15/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2791 - accuracy: 0.8601\n",
            "Epoch 15: val_accuracy did not improve from 0.86413\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.2791 - accuracy: 0.8601 - val_loss: 0.3564 - val_accuracy: 0.8533\n",
            "Epoch 16/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2795 - accuracy: 0.8614\n",
            "Epoch 16: val_accuracy improved from 0.86413 to 0.86957, saving model to mymodel2_16.h5\n",
            "6/6 [==============================] - 28s 5s/step - loss: 0.2795 - accuracy: 0.8614 - val_loss: 0.3250 - val_accuracy: 0.8696\n",
            "Epoch 17/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2602 - accuracy: 0.8845\n",
            "Epoch 17: val_accuracy improved from 0.86957 to 0.88587, saving model to mymodel2_17.h5\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.2602 - accuracy: 0.8845 - val_loss: 0.3250 - val_accuracy: 0.8859\n",
            "Epoch 18/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.8804\n",
            "Epoch 18: val_accuracy improved from 0.88587 to 0.89130, saving model to mymodel2_18.h5\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.2431 - accuracy: 0.8804 - val_loss: 0.2952 - val_accuracy: 0.8913\n",
            "Epoch 19/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2382 - accuracy: 0.8886\n",
            "Epoch 19: val_accuracy improved from 0.89130 to 0.89674, saving model to mymodel2_19.h5\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.2382 - accuracy: 0.8886 - val_loss: 0.2822 - val_accuracy: 0.8967\n",
            "Epoch 20/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2366 - accuracy: 0.8927\n",
            "Epoch 20: val_accuracy did not improve from 0.89674\n",
            "6/6 [==============================] - 30s 5s/step - loss: 0.2366 - accuracy: 0.8927 - val_loss: 0.2577 - val_accuracy: 0.8804\n",
            "Epoch 21/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2283 - accuracy: 0.8872\n",
            "Epoch 21: val_accuracy did not improve from 0.89674\n",
            "6/6 [==============================] - 34s 6s/step - loss: 0.2283 - accuracy: 0.8872 - val_loss: 0.2612 - val_accuracy: 0.8967\n",
            "Epoch 22/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2296 - accuracy: 0.8967\n",
            "Epoch 22: val_accuracy did not improve from 0.89674\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.2296 - accuracy: 0.8967 - val_loss: 0.2429 - val_accuracy: 0.8967\n",
            "Epoch 23/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2231 - accuracy: 0.8967\n",
            "Epoch 23: val_accuracy did not improve from 0.89674\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.2231 - accuracy: 0.8967 - val_loss: 0.2455 - val_accuracy: 0.8913\n",
            "Epoch 24/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2115 - accuracy: 0.9062\n",
            "Epoch 24: val_accuracy did not improve from 0.89674\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.2115 - accuracy: 0.9062 - val_loss: 0.2400 - val_accuracy: 0.8967\n",
            "Epoch 25/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.9008\n",
            "Epoch 25: val_accuracy improved from 0.89674 to 0.90217, saving model to mymodel2_25.h5\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.2188 - accuracy: 0.9008 - val_loss: 0.2411 - val_accuracy: 0.9022\n",
            "Epoch 26/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2108 - accuracy: 0.8981\n",
            "Epoch 26: val_accuracy did not improve from 0.90217\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.2108 - accuracy: 0.8981 - val_loss: 0.2428 - val_accuracy: 0.9022\n",
            "Epoch 27/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2180 - accuracy: 0.8913\n",
            "Epoch 27: val_accuracy did not improve from 0.90217\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.2180 - accuracy: 0.8913 - val_loss: 0.2414 - val_accuracy: 0.8967\n",
            "Epoch 28/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2012 - accuracy: 0.9062\n",
            "Epoch 28: val_accuracy improved from 0.90217 to 0.90761, saving model to mymodel2_28.h5\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.2012 - accuracy: 0.9062 - val_loss: 0.2332 - val_accuracy: 0.9076\n",
            "Epoch 29/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2034 - accuracy: 0.9090\n",
            "Epoch 29: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.2034 - accuracy: 0.9090 - val_loss: 0.2321 - val_accuracy: 0.9076\n",
            "Epoch 30/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2039 - accuracy: 0.8954\n",
            "Epoch 30: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.2039 - accuracy: 0.8954 - val_loss: 0.2426 - val_accuracy: 0.9076\n",
            "Epoch 31/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1923 - accuracy: 0.9103\n",
            "Epoch 31: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1923 - accuracy: 0.9103 - val_loss: 0.2251 - val_accuracy: 0.9076\n",
            "Epoch 32/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.8940\n",
            "Epoch 32: val_accuracy did not improve from 0.90761\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.2028 - accuracy: 0.8940 - val_loss: 0.2413 - val_accuracy: 0.9022\n",
            "Epoch 33/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9022\n",
            "Epoch 33: val_accuracy improved from 0.90761 to 0.91848, saving model to mymodel2_33.h5\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1925 - accuracy: 0.9022 - val_loss: 0.2264 - val_accuracy: 0.9185\n",
            "Epoch 34/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9158\n",
            "Epoch 34: val_accuracy did not improve from 0.91848\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1924 - accuracy: 0.9158 - val_loss: 0.2433 - val_accuracy: 0.9076\n",
            "Epoch 35/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9130\n",
            "Epoch 35: val_accuracy improved from 0.91848 to 0.92391, saving model to mymodel2_35.h5\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1891 - accuracy: 0.9130 - val_loss: 0.2108 - val_accuracy: 0.9239\n",
            "Epoch 36/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1794 - accuracy: 0.9185\n",
            "Epoch 36: val_accuracy did not improve from 0.92391\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1794 - accuracy: 0.9185 - val_loss: 0.2344 - val_accuracy: 0.9076\n",
            "Epoch 37/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1887 - accuracy: 0.9090\n",
            "Epoch 37: val_accuracy did not improve from 0.92391\n",
            "6/6 [==============================] - 26s 5s/step - loss: 0.1887 - accuracy: 0.9090 - val_loss: 0.2310 - val_accuracy: 0.9185\n",
            "Epoch 38/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1780 - accuracy: 0.9035\n",
            "Epoch 38: val_accuracy did not improve from 0.92391\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1780 - accuracy: 0.9035 - val_loss: 0.2010 - val_accuracy: 0.9239\n",
            "Epoch 39/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9144\n",
            "Epoch 39: val_accuracy improved from 0.92391 to 0.92935, saving model to mymodel2_39.h5\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1788 - accuracy: 0.9144 - val_loss: 0.2127 - val_accuracy: 0.9293\n",
            "Epoch 40/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1704 - accuracy: 0.9158\n",
            "Epoch 40: val_accuracy did not improve from 0.92935\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.1704 - accuracy: 0.9158 - val_loss: 0.2069 - val_accuracy: 0.9293\n",
            "Epoch 41/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9117\n",
            "Epoch 41: val_accuracy did not improve from 0.92935\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1747 - accuracy: 0.9117 - val_loss: 0.2031 - val_accuracy: 0.9293\n",
            "Epoch 42/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1593 - accuracy: 0.9307\n",
            "Epoch 42: val_accuracy improved from 0.92935 to 0.93478, saving model to mymodel2_42.h5\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1593 - accuracy: 0.9307 - val_loss: 0.1969 - val_accuracy: 0.9348\n",
            "Epoch 43/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9117\n",
            "Epoch 43: val_accuracy did not improve from 0.93478\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1707 - accuracy: 0.9117 - val_loss: 0.2085 - val_accuracy: 0.9348\n",
            "Epoch 44/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1688 - accuracy: 0.9198\n",
            "Epoch 44: val_accuracy did not improve from 0.93478\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1688 - accuracy: 0.9198 - val_loss: 0.2286 - val_accuracy: 0.9076\n",
            "Epoch 45/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1639 - accuracy: 0.9293\n",
            "Epoch 45: val_accuracy did not improve from 0.93478\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1639 - accuracy: 0.9293 - val_loss: 0.2142 - val_accuracy: 0.9239\n",
            "Epoch 46/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1640 - accuracy: 0.9239\n",
            "Epoch 46: val_accuracy did not improve from 0.93478\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1640 - accuracy: 0.9239 - val_loss: 0.1905 - val_accuracy: 0.9348\n",
            "Epoch 47/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1649 - accuracy: 0.9402\n",
            "Epoch 47: val_accuracy improved from 0.93478 to 0.94022, saving model to mymodel2_47.h5\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1649 - accuracy: 0.9402 - val_loss: 0.1840 - val_accuracy: 0.9402\n",
            "Epoch 48/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1621 - accuracy: 0.9198\n",
            "Epoch 48: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1621 - accuracy: 0.9198 - val_loss: 0.1895 - val_accuracy: 0.9402\n",
            "Epoch 49/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1485 - accuracy: 0.9253\n",
            "Epoch 49: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1485 - accuracy: 0.9253 - val_loss: 0.2137 - val_accuracy: 0.9239\n",
            "Epoch 50/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1628 - accuracy: 0.9253\n",
            "Epoch 50: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1628 - accuracy: 0.9253 - val_loss: 0.2560 - val_accuracy: 0.8696\n",
            "Epoch 51/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1746 - accuracy: 0.9185\n",
            "Epoch 51: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1746 - accuracy: 0.9185 - val_loss: 0.1815 - val_accuracy: 0.9293\n",
            "Epoch 52/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9049\n",
            "Epoch 52: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.2047 - accuracy: 0.9049 - val_loss: 0.1916 - val_accuracy: 0.9293\n",
            "Epoch 53/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9212\n",
            "Epoch 53: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1599 - accuracy: 0.9212 - val_loss: 0.2190 - val_accuracy: 0.9185\n",
            "Epoch 54/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1542 - accuracy: 0.9198\n",
            "Epoch 54: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1542 - accuracy: 0.9198 - val_loss: 0.1854 - val_accuracy: 0.9348\n",
            "Epoch 55/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9375\n",
            "Epoch 55: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1453 - accuracy: 0.9375 - val_loss: 0.2056 - val_accuracy: 0.9185\n",
            "Epoch 56/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1535 - accuracy: 0.9198\n",
            "Epoch 56: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1535 - accuracy: 0.9198 - val_loss: 0.2012 - val_accuracy: 0.9185\n",
            "Epoch 57/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1559 - accuracy: 0.9253\n",
            "Epoch 57: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1559 - accuracy: 0.9253 - val_loss: 0.1812 - val_accuracy: 0.9293\n",
            "Epoch 58/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9348\n",
            "Epoch 58: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1473 - accuracy: 0.9348 - val_loss: 0.1926 - val_accuracy: 0.9130\n",
            "Epoch 59/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9307\n",
            "Epoch 59: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1414 - accuracy: 0.9307 - val_loss: 0.1819 - val_accuracy: 0.9402\n",
            "Epoch 60/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9334\n",
            "Epoch 60: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1483 - accuracy: 0.9334 - val_loss: 0.1821 - val_accuracy: 0.9348\n",
            "Epoch 61/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9293\n",
            "Epoch 61: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1432 - accuracy: 0.9293 - val_loss: 0.1841 - val_accuracy: 0.9239\n",
            "Epoch 62/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9348\n",
            "Epoch 62: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1372 - accuracy: 0.9348 - val_loss: 0.1777 - val_accuracy: 0.9402\n",
            "Epoch 63/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9402\n",
            "Epoch 63: val_accuracy did not improve from 0.94022\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1439 - accuracy: 0.9402 - val_loss: 0.1891 - val_accuracy: 0.9293\n",
            "Epoch 64/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1363 - accuracy: 0.9416\n",
            "Epoch 64: val_accuracy improved from 0.94022 to 0.94565, saving model to mymodel2_64.h5\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1363 - accuracy: 0.9416 - val_loss: 0.1640 - val_accuracy: 0.9457\n",
            "Epoch 65/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1369 - accuracy: 0.9416\n",
            "Epoch 65: val_accuracy did not improve from 0.94565\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1369 - accuracy: 0.9416 - val_loss: 0.1775 - val_accuracy: 0.9348\n",
            "Epoch 66/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9375\n",
            "Epoch 66: val_accuracy did not improve from 0.94565\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1372 - accuracy: 0.9375 - val_loss: 0.1903 - val_accuracy: 0.9185\n",
            "Epoch 67/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9429\n",
            "Epoch 67: val_accuracy did not improve from 0.94565\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1388 - accuracy: 0.9429 - val_loss: 0.1877 - val_accuracy: 0.9130\n",
            "Epoch 68/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1338 - accuracy: 0.9239\n",
            "Epoch 68: val_accuracy did not improve from 0.94565\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1338 - accuracy: 0.9239 - val_loss: 0.1892 - val_accuracy: 0.9130\n",
            "Epoch 69/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1328 - accuracy: 0.9375\n",
            "Epoch 69: val_accuracy did not improve from 0.94565\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.1328 - accuracy: 0.9375 - val_loss: 0.1757 - val_accuracy: 0.9185\n",
            "Epoch 70/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9402\n",
            "Epoch 70: val_accuracy did not improve from 0.94565\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1287 - accuracy: 0.9402 - val_loss: 0.1772 - val_accuracy: 0.9293\n",
            "Epoch 71/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1247 - accuracy: 0.9511\n",
            "Epoch 71: val_accuracy did not improve from 0.94565\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1247 - accuracy: 0.9511 - val_loss: 0.1690 - val_accuracy: 0.9293\n",
            "Epoch 72/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1260 - accuracy: 0.9361\n",
            "Epoch 72: val_accuracy did not improve from 0.94565\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1260 - accuracy: 0.9361 - val_loss: 0.1977 - val_accuracy: 0.9076\n",
            "Epoch 73/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9443\n",
            "Epoch 73: val_accuracy did not improve from 0.94565\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1404 - accuracy: 0.9443 - val_loss: 0.1695 - val_accuracy: 0.9239\n",
            "Epoch 74/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9307\n",
            "Epoch 74: val_accuracy improved from 0.94565 to 0.95109, saving model to mymodel2_74.h5\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1479 - accuracy: 0.9307 - val_loss: 0.1492 - val_accuracy: 0.9511\n",
            "Epoch 75/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1475 - accuracy: 0.9389\n",
            "Epoch 75: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1475 - accuracy: 0.9389 - val_loss: 0.1967 - val_accuracy: 0.8967\n",
            "Epoch 76/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9443\n",
            "Epoch 76: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1344 - accuracy: 0.9443 - val_loss: 0.1769 - val_accuracy: 0.9185\n",
            "Epoch 77/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1263 - accuracy: 0.9416\n",
            "Epoch 77: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1263 - accuracy: 0.9416 - val_loss: 0.1551 - val_accuracy: 0.9402\n",
            "Epoch 78/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1278 - accuracy: 0.9457\n",
            "Epoch 78: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 27s 4s/step - loss: 0.1278 - accuracy: 0.9457 - val_loss: 0.1805 - val_accuracy: 0.9239\n",
            "Epoch 79/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9402\n",
            "Epoch 79: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1182 - accuracy: 0.9402 - val_loss: 0.1786 - val_accuracy: 0.9185\n",
            "Epoch 80/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.9484\n",
            "Epoch 80: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1254 - accuracy: 0.9484 - val_loss: 0.1669 - val_accuracy: 0.9402\n",
            "Epoch 81/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9484\n",
            "Epoch 81: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1129 - accuracy: 0.9484 - val_loss: 0.1901 - val_accuracy: 0.9185\n",
            "Epoch 82/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1171 - accuracy: 0.9429\n",
            "Epoch 82: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1171 - accuracy: 0.9429 - val_loss: 0.2036 - val_accuracy: 0.9076\n",
            "Epoch 83/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.9402\n",
            "Epoch 83: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1341 - accuracy: 0.9402 - val_loss: 0.1720 - val_accuracy: 0.9185\n",
            "Epoch 84/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1267 - accuracy: 0.9429\n",
            "Epoch 84: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1267 - accuracy: 0.9429 - val_loss: 0.1612 - val_accuracy: 0.9348\n",
            "Epoch 85/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1482 - accuracy: 0.9416\n",
            "Epoch 85: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1482 - accuracy: 0.9416 - val_loss: 0.2046 - val_accuracy: 0.9185\n",
            "Epoch 86/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1413 - accuracy: 0.9321\n",
            "Epoch 86: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.1413 - accuracy: 0.9321 - val_loss: 0.1758 - val_accuracy: 0.9185\n",
            "Epoch 87/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1224 - accuracy: 0.9416\n",
            "Epoch 87: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1224 - accuracy: 0.9416 - val_loss: 0.1710 - val_accuracy: 0.9239\n",
            "Epoch 88/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1086 - accuracy: 0.9552\n",
            "Epoch 88: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1086 - accuracy: 0.9552 - val_loss: 0.1847 - val_accuracy: 0.9130\n",
            "Epoch 89/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1090 - accuracy: 0.9497\n",
            "Epoch 89: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1090 - accuracy: 0.9497 - val_loss: 0.1590 - val_accuracy: 0.9348\n",
            "Epoch 90/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1137 - accuracy: 0.9457\n",
            "Epoch 90: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1137 - accuracy: 0.9457 - val_loss: 0.1917 - val_accuracy: 0.9076\n",
            "Epoch 91/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1178 - accuracy: 0.9457\n",
            "Epoch 91: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1178 - accuracy: 0.9457 - val_loss: 0.1964 - val_accuracy: 0.9022\n",
            "Epoch 92/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9470\n",
            "Epoch 92: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1292 - accuracy: 0.9470 - val_loss: 0.1596 - val_accuracy: 0.9293\n",
            "Epoch 93/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9511\n",
            "Epoch 93: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1112 - accuracy: 0.9511 - val_loss: 0.1783 - val_accuracy: 0.9185\n",
            "Epoch 94/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9470\n",
            "Epoch 94: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1129 - accuracy: 0.9470 - val_loss: 0.1668 - val_accuracy: 0.9293\n",
            "Epoch 95/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9429\n",
            "Epoch 95: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1154 - accuracy: 0.9429 - val_loss: 0.1940 - val_accuracy: 0.9130\n",
            "Epoch 96/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1158 - accuracy: 0.9443\n",
            "Epoch 96: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.1158 - accuracy: 0.9443 - val_loss: 0.1830 - val_accuracy: 0.9185\n",
            "Epoch 97/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1118 - accuracy: 0.9552\n",
            "Epoch 97: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1118 - accuracy: 0.9552 - val_loss: 0.1546 - val_accuracy: 0.9402\n",
            "Epoch 98/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9443\n",
            "Epoch 98: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1233 - accuracy: 0.9443 - val_loss: 0.1874 - val_accuracy: 0.9076\n",
            "Epoch 99/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9443\n",
            "Epoch 99: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1099 - accuracy: 0.9443 - val_loss: 0.1904 - val_accuracy: 0.9130\n",
            "Epoch 100/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9484\n",
            "Epoch 100: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1201 - accuracy: 0.9484 - val_loss: 0.1566 - val_accuracy: 0.9293\n",
            "Epoch 101/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1074 - accuracy: 0.9552\n",
            "Epoch 101: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1074 - accuracy: 0.9552 - val_loss: 0.1905 - val_accuracy: 0.9076\n",
            "Epoch 102/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9592\n",
            "Epoch 102: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1027 - accuracy: 0.9592 - val_loss: 0.1874 - val_accuracy: 0.9130\n",
            "Epoch 103/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9579\n",
            "Epoch 103: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1039 - accuracy: 0.9579 - val_loss: 0.1539 - val_accuracy: 0.9457\n",
            "Epoch 104/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.9592\n",
            "Epoch 104: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1053 - accuracy: 0.9592 - val_loss: 0.1984 - val_accuracy: 0.8859\n",
            "Epoch 105/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9524\n",
            "Epoch 105: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 26s 5s/step - loss: 0.1071 - accuracy: 0.9524 - val_loss: 0.1783 - val_accuracy: 0.9076\n",
            "Epoch 106/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0892 - accuracy: 0.9633\n",
            "Epoch 106: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0892 - accuracy: 0.9633 - val_loss: 0.1682 - val_accuracy: 0.9293\n",
            "Epoch 107/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1002 - accuracy: 0.9538\n",
            "Epoch 107: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1002 - accuracy: 0.9538 - val_loss: 0.1539 - val_accuracy: 0.9293\n",
            "Epoch 108/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1007 - accuracy: 0.9565\n",
            "Epoch 108: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1007 - accuracy: 0.9565 - val_loss: 0.1620 - val_accuracy: 0.9239\n",
            "Epoch 109/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9457\n",
            "Epoch 109: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1091 - accuracy: 0.9457 - val_loss: 0.2015 - val_accuracy: 0.8913\n",
            "Epoch 110/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9579\n",
            "Epoch 110: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1059 - accuracy: 0.9579 - val_loss: 0.1764 - val_accuracy: 0.9076\n",
            "Epoch 111/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9552\n",
            "Epoch 111: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1051 - accuracy: 0.9552 - val_loss: 0.1609 - val_accuracy: 0.9239\n",
            "Epoch 112/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9620\n",
            "Epoch 112: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0929 - accuracy: 0.9620 - val_loss: 0.1651 - val_accuracy: 0.9293\n",
            "Epoch 113/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0928 - accuracy: 0.9620\n",
            "Epoch 113: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0928 - accuracy: 0.9620 - val_loss: 0.1548 - val_accuracy: 0.9348\n",
            "Epoch 114/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9660\n",
            "Epoch 114: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0906 - accuracy: 0.9660 - val_loss: 0.1863 - val_accuracy: 0.9076\n",
            "Epoch 115/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9579\n",
            "Epoch 115: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0997 - accuracy: 0.9579 - val_loss: 0.1765 - val_accuracy: 0.9022\n",
            "Epoch 116/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0952 - accuracy: 0.9660\n",
            "Epoch 116: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0952 - accuracy: 0.9660 - val_loss: 0.1366 - val_accuracy: 0.9511\n",
            "Epoch 117/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9606\n",
            "Epoch 117: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0918 - accuracy: 0.9606 - val_loss: 0.1647 - val_accuracy: 0.9239\n",
            "Epoch 118/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9538\n",
            "Epoch 118: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0957 - accuracy: 0.9538 - val_loss: 0.1637 - val_accuracy: 0.9239\n",
            "Epoch 119/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1004 - accuracy: 0.9579\n",
            "Epoch 119: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1004 - accuracy: 0.9579 - val_loss: 0.1362 - val_accuracy: 0.9511\n",
            "Epoch 120/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1014 - accuracy: 0.9579\n",
            "Epoch 120: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1014 - accuracy: 0.9579 - val_loss: 0.1485 - val_accuracy: 0.9348\n",
            "Epoch 121/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9592\n",
            "Epoch 121: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0955 - accuracy: 0.9592 - val_loss: 0.1611 - val_accuracy: 0.9185\n",
            "Epoch 122/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9606\n",
            "Epoch 122: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0914 - accuracy: 0.9606 - val_loss: 0.1499 - val_accuracy: 0.9293\n",
            "Epoch 123/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9715\n",
            "Epoch 123: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0807 - accuracy: 0.9715 - val_loss: 0.1461 - val_accuracy: 0.9348\n",
            "Epoch 124/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0923 - accuracy: 0.9633\n",
            "Epoch 124: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0923 - accuracy: 0.9633 - val_loss: 0.1536 - val_accuracy: 0.9348\n",
            "Epoch 125/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9660\n",
            "Epoch 125: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0858 - accuracy: 0.9660 - val_loss: 0.1533 - val_accuracy: 0.9293\n",
            "Epoch 126/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0887 - accuracy: 0.9620\n",
            "Epoch 126: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0887 - accuracy: 0.9620 - val_loss: 0.2549 - val_accuracy: 0.8641\n",
            "Epoch 127/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9293\n",
            "Epoch 127: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1461 - accuracy: 0.9293 - val_loss: 0.1477 - val_accuracy: 0.9293\n",
            "Epoch 128/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1099 - accuracy: 0.9579\n",
            "Epoch 128: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1099 - accuracy: 0.9579 - val_loss: 0.1347 - val_accuracy: 0.9457\n",
            "Epoch 129/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9484\n",
            "Epoch 129: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1066 - accuracy: 0.9484 - val_loss: 0.1473 - val_accuracy: 0.9348\n",
            "Epoch 130/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9592\n",
            "Epoch 130: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0858 - accuracy: 0.9592 - val_loss: 0.1518 - val_accuracy: 0.9293\n",
            "Epoch 131/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9606\n",
            "Epoch 131: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0956 - accuracy: 0.9606 - val_loss: 0.1382 - val_accuracy: 0.9511\n",
            "Epoch 132/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9511\n",
            "Epoch 132: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1016 - accuracy: 0.9511 - val_loss: 0.1666 - val_accuracy: 0.9185\n",
            "Epoch 133/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0995 - accuracy: 0.9565\n",
            "Epoch 133: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0995 - accuracy: 0.9565 - val_loss: 0.1435 - val_accuracy: 0.9402\n",
            "Epoch 134/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9647\n",
            "Epoch 134: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0882 - accuracy: 0.9647 - val_loss: 0.1587 - val_accuracy: 0.9293\n",
            "Epoch 135/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9592\n",
            "Epoch 135: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0850 - accuracy: 0.9592 - val_loss: 0.1305 - val_accuracy: 0.9511\n",
            "Epoch 136/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9579\n",
            "Epoch 136: val_accuracy did not improve from 0.95109\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0890 - accuracy: 0.9579 - val_loss: 0.1417 - val_accuracy: 0.9511\n",
            "Epoch 137/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9606\n",
            "Epoch 137: val_accuracy improved from 0.95109 to 0.96196, saving model to mymodel2_137.h5\n",
            "6/6 [==============================] - 26s 5s/step - loss: 0.0850 - accuracy: 0.9606 - val_loss: 0.1235 - val_accuracy: 0.9620\n",
            "Epoch 138/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9606\n",
            "Epoch 138: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0947 - accuracy: 0.9606 - val_loss: 0.1429 - val_accuracy: 0.9565\n",
            "Epoch 139/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9606\n",
            "Epoch 139: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0909 - accuracy: 0.9606 - val_loss: 0.1351 - val_accuracy: 0.9457\n",
            "Epoch 140/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9660\n",
            "Epoch 140: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0860 - accuracy: 0.9660 - val_loss: 0.1514 - val_accuracy: 0.9348\n",
            "Epoch 141/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0757 - accuracy: 0.9633\n",
            "Epoch 141: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0757 - accuracy: 0.9633 - val_loss: 0.1304 - val_accuracy: 0.9620\n",
            "Epoch 142/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.9606\n",
            "Epoch 142: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0813 - accuracy: 0.9606 - val_loss: 0.1402 - val_accuracy: 0.9457\n",
            "Epoch 143/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9647\n",
            "Epoch 143: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0783 - accuracy: 0.9647 - val_loss: 0.1323 - val_accuracy: 0.9565\n",
            "Epoch 144/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9660\n",
            "Epoch 144: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0765 - accuracy: 0.9660 - val_loss: 0.1535 - val_accuracy: 0.9239\n",
            "Epoch 145/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9660\n",
            "Epoch 145: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0869 - accuracy: 0.9660 - val_loss: 0.1374 - val_accuracy: 0.9457\n",
            "Epoch 146/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9647\n",
            "Epoch 146: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0732 - accuracy: 0.9647 - val_loss: 0.1468 - val_accuracy: 0.9348\n",
            "Epoch 147/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9715\n",
            "Epoch 147: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0751 - accuracy: 0.9715 - val_loss: 0.1521 - val_accuracy: 0.9076\n",
            "Epoch 148/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9728\n",
            "Epoch 148: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0736 - accuracy: 0.9728 - val_loss: 0.1342 - val_accuracy: 0.9511\n",
            "Epoch 149/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9538\n",
            "Epoch 149: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0958 - accuracy: 0.9538 - val_loss: 0.1854 - val_accuracy: 0.8913\n",
            "Epoch 150/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1062 - accuracy: 0.9524\n",
            "Epoch 150: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1062 - accuracy: 0.9524 - val_loss: 0.1237 - val_accuracy: 0.9511\n",
            "Epoch 151/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0916 - accuracy: 0.9606\n",
            "Epoch 151: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0916 - accuracy: 0.9606 - val_loss: 0.1059 - val_accuracy: 0.9565\n",
            "Epoch 152/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9538\n",
            "Epoch 152: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0957 - accuracy: 0.9538 - val_loss: 0.1543 - val_accuracy: 0.9239\n",
            "Epoch 153/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0929 - accuracy: 0.9606\n",
            "Epoch 153: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0929 - accuracy: 0.9606 - val_loss: 0.1307 - val_accuracy: 0.9402\n",
            "Epoch 154/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9579\n",
            "Epoch 154: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0922 - accuracy: 0.9579 - val_loss: 0.1181 - val_accuracy: 0.9511\n",
            "Epoch 155/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9620\n",
            "Epoch 155: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0875 - accuracy: 0.9620 - val_loss: 0.1351 - val_accuracy: 0.9457\n",
            "Epoch 156/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1022 - accuracy: 0.9511\n",
            "Epoch 156: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1022 - accuracy: 0.9511 - val_loss: 0.1686 - val_accuracy: 0.9130\n",
            "Epoch 157/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9633\n",
            "Epoch 157: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0856 - accuracy: 0.9633 - val_loss: 0.1234 - val_accuracy: 0.9620\n",
            "Epoch 158/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9674\n",
            "Epoch 158: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0775 - accuracy: 0.9674 - val_loss: 0.1314 - val_accuracy: 0.9402\n",
            "Epoch 159/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9660\n",
            "Epoch 159: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0785 - accuracy: 0.9660 - val_loss: 0.1372 - val_accuracy: 0.9457\n",
            "Epoch 160/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9688\n",
            "Epoch 160: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 26s 5s/step - loss: 0.0809 - accuracy: 0.9688 - val_loss: 0.1366 - val_accuracy: 0.9511\n",
            "Epoch 161/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9592\n",
            "Epoch 161: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0884 - accuracy: 0.9592 - val_loss: 0.1172 - val_accuracy: 0.9565\n",
            "Epoch 162/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9769\n",
            "Epoch 162: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0681 - accuracy: 0.9769 - val_loss: 0.1175 - val_accuracy: 0.9620\n",
            "Epoch 163/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9715\n",
            "Epoch 163: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0763 - accuracy: 0.9715 - val_loss: 0.1292 - val_accuracy: 0.9620\n",
            "Epoch 164/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9674\n",
            "Epoch 164: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0687 - accuracy: 0.9674 - val_loss: 0.1238 - val_accuracy: 0.9565\n",
            "Epoch 165/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9674\n",
            "Epoch 165: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0739 - accuracy: 0.9674 - val_loss: 0.1218 - val_accuracy: 0.9511\n",
            "Epoch 166/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9701\n",
            "Epoch 166: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0764 - accuracy: 0.9701 - val_loss: 0.1236 - val_accuracy: 0.9565\n",
            "Epoch 167/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9660\n",
            "Epoch 167: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0776 - accuracy: 0.9660 - val_loss: 0.1196 - val_accuracy: 0.9565\n",
            "Epoch 168/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9715\n",
            "Epoch 168: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0688 - accuracy: 0.9715 - val_loss: 0.1166 - val_accuracy: 0.9620\n",
            "Epoch 169/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9674\n",
            "Epoch 169: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0682 - accuracy: 0.9674 - val_loss: 0.1274 - val_accuracy: 0.9511\n",
            "Epoch 170/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9715\n",
            "Epoch 170: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0765 - accuracy: 0.9715 - val_loss: 0.1114 - val_accuracy: 0.9565\n",
            "Epoch 171/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9674\n",
            "Epoch 171: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0734 - accuracy: 0.9674 - val_loss: 0.1238 - val_accuracy: 0.9511\n",
            "Epoch 172/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9701\n",
            "Epoch 172: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0644 - accuracy: 0.9701 - val_loss: 0.1205 - val_accuracy: 0.9511\n",
            "Epoch 173/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9674\n",
            "Epoch 173: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0711 - accuracy: 0.9674 - val_loss: 0.1167 - val_accuracy: 0.9565\n",
            "Epoch 174/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0777 - accuracy: 0.9620\n",
            "Epoch 174: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0777 - accuracy: 0.9620 - val_loss: 0.1174 - val_accuracy: 0.9565\n",
            "Epoch 175/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0719 - accuracy: 0.9688\n",
            "Epoch 175: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0719 - accuracy: 0.9688 - val_loss: 0.1095 - val_accuracy: 0.9565\n",
            "Epoch 176/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9674\n",
            "Epoch 176: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0766 - accuracy: 0.9674 - val_loss: 0.1169 - val_accuracy: 0.9511\n",
            "Epoch 177/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9688\n",
            "Epoch 177: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0699 - accuracy: 0.9688 - val_loss: 0.1231 - val_accuracy: 0.9511\n",
            "Epoch 178/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9728\n",
            "Epoch 178: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0588 - accuracy: 0.9728 - val_loss: 0.1227 - val_accuracy: 0.9457\n",
            "Epoch 179/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9783\n",
            "Epoch 179: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0590 - accuracy: 0.9783 - val_loss: 0.1181 - val_accuracy: 0.9511\n",
            "Epoch 180/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9647\n",
            "Epoch 180: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0737 - accuracy: 0.9647 - val_loss: 0.1208 - val_accuracy: 0.9511\n",
            "Epoch 181/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9579\n",
            "Epoch 181: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0825 - accuracy: 0.9579 - val_loss: 0.1494 - val_accuracy: 0.9293\n",
            "Epoch 182/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9769\n",
            "Epoch 182: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0642 - accuracy: 0.9769 - val_loss: 0.1152 - val_accuracy: 0.9511\n",
            "Epoch 183/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9755\n",
            "Epoch 183: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0640 - accuracy: 0.9755 - val_loss: 0.1080 - val_accuracy: 0.9565\n",
            "Epoch 184/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0704 - accuracy: 0.9688\n",
            "Epoch 184: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0704 - accuracy: 0.9688 - val_loss: 0.1893 - val_accuracy: 0.8913\n",
            "Epoch 185/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9484\n",
            "Epoch 185: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1184 - accuracy: 0.9484 - val_loss: 0.1173 - val_accuracy: 0.9620\n",
            "Epoch 186/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.9688\n",
            "Epoch 186: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0748 - accuracy: 0.9688 - val_loss: 0.1152 - val_accuracy: 0.9620\n",
            "Epoch 187/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9701\n",
            "Epoch 187: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0718 - accuracy: 0.9701 - val_loss: 0.1135 - val_accuracy: 0.9620\n",
            "Epoch 188/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0659 - accuracy: 0.9688\n",
            "Epoch 188: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0659 - accuracy: 0.9688 - val_loss: 0.1165 - val_accuracy: 0.9565\n",
            "Epoch 189/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0688 - accuracy: 0.9688\n",
            "Epoch 189: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0688 - accuracy: 0.9688 - val_loss: 0.1242 - val_accuracy: 0.9457\n",
            "Epoch 190/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9688\n",
            "Epoch 190: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0621 - accuracy: 0.9688 - val_loss: 0.1379 - val_accuracy: 0.9348\n",
            "Epoch 191/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9715\n",
            "Epoch 191: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0647 - accuracy: 0.9715 - val_loss: 0.1202 - val_accuracy: 0.9565\n",
            "Epoch 192/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9715\n",
            "Epoch 192: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0630 - accuracy: 0.9715 - val_loss: 0.1146 - val_accuracy: 0.9565\n",
            "Epoch 193/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9701\n",
            "Epoch 193: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0706 - accuracy: 0.9701 - val_loss: 0.1346 - val_accuracy: 0.9457\n",
            "Epoch 194/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 0.9769\n",
            "Epoch 194: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0592 - accuracy: 0.9769 - val_loss: 0.1231 - val_accuracy: 0.9511\n",
            "Epoch 195/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0691 - accuracy: 0.9688\n",
            "Epoch 195: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0691 - accuracy: 0.9688 - val_loss: 0.1230 - val_accuracy: 0.9511\n",
            "Epoch 196/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9674\n",
            "Epoch 196: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0663 - accuracy: 0.9674 - val_loss: 0.1290 - val_accuracy: 0.9565\n",
            "Epoch 197/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9769\n",
            "Epoch 197: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0550 - accuracy: 0.9769 - val_loss: 0.1317 - val_accuracy: 0.9457\n",
            "Epoch 198/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9688\n",
            "Epoch 198: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0603 - accuracy: 0.9688 - val_loss: 0.1121 - val_accuracy: 0.9565\n",
            "Epoch 199/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9715\n",
            "Epoch 199: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0596 - accuracy: 0.9715 - val_loss: 0.1317 - val_accuracy: 0.9402\n",
            "Epoch 200/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9688\n",
            "Epoch 200: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0747 - accuracy: 0.9688 - val_loss: 0.1166 - val_accuracy: 0.9565\n",
            "Epoch 201/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0598 - accuracy: 0.9796\n",
            "Epoch 201: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0598 - accuracy: 0.9796 - val_loss: 0.1183 - val_accuracy: 0.9511\n",
            "Epoch 202/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9742\n",
            "Epoch 202: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0593 - accuracy: 0.9742 - val_loss: 0.1202 - val_accuracy: 0.9457\n",
            "Epoch 203/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9755\n",
            "Epoch 203: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0593 - accuracy: 0.9755 - val_loss: 0.1123 - val_accuracy: 0.9565\n",
            "Epoch 204/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9701\n",
            "Epoch 204: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0605 - accuracy: 0.9701 - val_loss: 0.1240 - val_accuracy: 0.9402\n",
            "Epoch 205/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9647\n",
            "Epoch 205: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0766 - accuracy: 0.9647 - val_loss: 0.1013 - val_accuracy: 0.9565\n",
            "Epoch 206/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9701\n",
            "Epoch 206: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0710 - accuracy: 0.9701 - val_loss: 0.1221 - val_accuracy: 0.9511\n",
            "Epoch 207/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9688\n",
            "Epoch 207: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0701 - accuracy: 0.9688 - val_loss: 0.1027 - val_accuracy: 0.9565\n",
            "Epoch 208/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9647\n",
            "Epoch 208: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0741 - accuracy: 0.9647 - val_loss: 0.1606 - val_accuracy: 0.9239\n",
            "Epoch 209/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9633\n",
            "Epoch 209: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0861 - accuracy: 0.9633 - val_loss: 0.1228 - val_accuracy: 0.9565\n",
            "Epoch 210/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9783\n",
            "Epoch 210: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0623 - accuracy: 0.9783 - val_loss: 0.1017 - val_accuracy: 0.9565\n",
            "Epoch 211/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9742\n",
            "Epoch 211: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0560 - accuracy: 0.9742 - val_loss: 0.1135 - val_accuracy: 0.9511\n",
            "Epoch 212/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9810\n",
            "Epoch 212: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0508 - accuracy: 0.9810 - val_loss: 0.1135 - val_accuracy: 0.9402\n",
            "Epoch 213/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0585 - accuracy: 0.9701\n",
            "Epoch 213: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0585 - accuracy: 0.9701 - val_loss: 0.1018 - val_accuracy: 0.9565\n",
            "Epoch 214/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9796\n",
            "Epoch 214: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0493 - accuracy: 0.9796 - val_loss: 0.1161 - val_accuracy: 0.9511\n",
            "Epoch 215/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9837\n",
            "Epoch 215: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0501 - accuracy: 0.9837 - val_loss: 0.1050 - val_accuracy: 0.9565\n",
            "Epoch 216/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9769\n",
            "Epoch 216: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0560 - accuracy: 0.9769 - val_loss: 0.1015 - val_accuracy: 0.9565\n",
            "Epoch 217/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0569 - accuracy: 0.9688\n",
            "Epoch 217: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0569 - accuracy: 0.9688 - val_loss: 0.1119 - val_accuracy: 0.9511\n",
            "Epoch 218/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9769\n",
            "Epoch 218: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0535 - accuracy: 0.9769 - val_loss: 0.0996 - val_accuracy: 0.9620\n",
            "Epoch 219/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9769\n",
            "Epoch 219: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0509 - accuracy: 0.9769 - val_loss: 0.1222 - val_accuracy: 0.9402\n",
            "Epoch 220/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9769\n",
            "Epoch 220: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0560 - accuracy: 0.9769 - val_loss: 0.1086 - val_accuracy: 0.9457\n",
            "Epoch 221/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9810\n",
            "Epoch 221: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0508 - accuracy: 0.9810 - val_loss: 0.0968 - val_accuracy: 0.9565\n",
            "Epoch 222/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9755\n",
            "Epoch 222: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0579 - accuracy: 0.9755 - val_loss: 0.1133 - val_accuracy: 0.9457\n",
            "Epoch 223/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0851 - accuracy: 0.9660\n",
            "Epoch 223: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0851 - accuracy: 0.9660 - val_loss: 0.0875 - val_accuracy: 0.9565\n",
            "Epoch 224/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9755\n",
            "Epoch 224: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0583 - accuracy: 0.9755 - val_loss: 0.1147 - val_accuracy: 0.9565\n",
            "Epoch 225/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9796\n",
            "Epoch 225: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0568 - accuracy: 0.9796 - val_loss: 0.1242 - val_accuracy: 0.9457\n",
            "Epoch 226/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0689 - accuracy: 0.9728\n",
            "Epoch 226: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0689 - accuracy: 0.9728 - val_loss: 0.1141 - val_accuracy: 0.9511\n",
            "Epoch 227/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9715\n",
            "Epoch 227: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0737 - accuracy: 0.9715 - val_loss: 0.0912 - val_accuracy: 0.9565\n",
            "Epoch 228/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0629 - accuracy: 0.9674\n",
            "Epoch 228: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0629 - accuracy: 0.9674 - val_loss: 0.1184 - val_accuracy: 0.9402\n",
            "Epoch 229/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9742\n",
            "Epoch 229: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0720 - accuracy: 0.9742 - val_loss: 0.0984 - val_accuracy: 0.9565\n",
            "Epoch 230/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9783\n",
            "Epoch 230: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0601 - accuracy: 0.9783 - val_loss: 0.1051 - val_accuracy: 0.9620\n",
            "Epoch 231/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9715\n",
            "Epoch 231: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0572 - accuracy: 0.9715 - val_loss: 0.1331 - val_accuracy: 0.9457\n",
            "Epoch 232/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9728\n",
            "Epoch 232: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0636 - accuracy: 0.9728 - val_loss: 0.1464 - val_accuracy: 0.9239\n",
            "Epoch 233/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0566 - accuracy: 0.9728\n",
            "Epoch 233: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0566 - accuracy: 0.9728 - val_loss: 0.1032 - val_accuracy: 0.9511\n",
            "Epoch 234/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0684 - accuracy: 0.9701\n",
            "Epoch 234: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0684 - accuracy: 0.9701 - val_loss: 0.0945 - val_accuracy: 0.9620\n",
            "Epoch 235/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9592\n",
            "Epoch 235: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0832 - accuracy: 0.9592 - val_loss: 0.1550 - val_accuracy: 0.9293\n",
            "Epoch 236/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1069 - accuracy: 0.9524\n",
            "Epoch 236: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.1069 - accuracy: 0.9524 - val_loss: 0.1371 - val_accuracy: 0.9511\n",
            "Epoch 237/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9769\n",
            "Epoch 237: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0542 - accuracy: 0.9769 - val_loss: 0.0990 - val_accuracy: 0.9511\n",
            "Epoch 238/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9755\n",
            "Epoch 238: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0539 - accuracy: 0.9755 - val_loss: 0.0920 - val_accuracy: 0.9620\n",
            "Epoch 239/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9837\n",
            "Epoch 239: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0482 - accuracy: 0.9837 - val_loss: 0.1153 - val_accuracy: 0.9457\n",
            "Epoch 240/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9783\n",
            "Epoch 240: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0475 - accuracy: 0.9783 - val_loss: 0.1141 - val_accuracy: 0.9402\n",
            "Epoch 241/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9823\n",
            "Epoch 241: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0493 - accuracy: 0.9823 - val_loss: 0.0931 - val_accuracy: 0.9565\n",
            "Epoch 242/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9769\n",
            "Epoch 242: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0464 - accuracy: 0.9769 - val_loss: 0.0961 - val_accuracy: 0.9565\n",
            "Epoch 243/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9783\n",
            "Epoch 243: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0489 - accuracy: 0.9783 - val_loss: 0.1014 - val_accuracy: 0.9511\n",
            "Epoch 244/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9783\n",
            "Epoch 244: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0490 - accuracy: 0.9783 - val_loss: 0.0930 - val_accuracy: 0.9620\n",
            "Epoch 245/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9769\n",
            "Epoch 245: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0500 - accuracy: 0.9769 - val_loss: 0.1120 - val_accuracy: 0.9511\n",
            "Epoch 246/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9728\n",
            "Epoch 246: val_accuracy did not improve from 0.96196\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0556 - accuracy: 0.9728 - val_loss: 0.1108 - val_accuracy: 0.9511\n",
            "Epoch 247/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9796\n",
            "Epoch 247: val_accuracy improved from 0.96196 to 0.96739, saving model to mymodel2_247.h5\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0521 - accuracy: 0.9796 - val_loss: 0.0793 - val_accuracy: 0.9674\n",
            "Epoch 248/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9796\n",
            "Epoch 248: val_accuracy did not improve from 0.96739\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0531 - accuracy: 0.9796 - val_loss: 0.0952 - val_accuracy: 0.9457\n",
            "Epoch 249/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9864\n",
            "Epoch 249: val_accuracy did not improve from 0.96739\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0391 - accuracy: 0.9864 - val_loss: 0.1088 - val_accuracy: 0.9511\n",
            "Epoch 250/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9796\n",
            "Epoch 250: val_accuracy did not improve from 0.96739\n",
            "6/6 [==============================] - 21s 4s/step - loss: 0.0483 - accuracy: 0.9796 - val_loss: 0.1011 - val_accuracy: 0.9402\n",
            "Training completed in time:  1:36:26.943534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model\n",
        "\n",
        "Here we will review the accuracy of the model on both the training and test data sets."
      ],
      "metadata": {
        "id": "1dtbO47Cgkvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX2JK0x3ggVq",
        "outputId": "6e6c20c5-6349-4ddf-ba03-05aa84cda751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  0.9660326242446899\n",
            "Testing Accuracy:  0.9402173757553101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(x_test) # label scores\n",
        "\n",
        "classpreds = np.argmax(preds, axis=1) # predicted classes\n",
        "\n",
        "y_testclass = np.argmax(y_test, axis=1) # true classes\n",
        "\n",
        "n_classes=6 # number of classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB27bssMgq02",
        "outputId": "5b4effac-ca65-4643-b420-c4bb1a19ebe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 1s 188ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_testclass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucI3G2Xzha7b",
        "outputId": "c7901796-f1a8-4a51-bc47-2f7447c73790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(2):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], preds[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])"
      ],
      "metadata": {
        "id": "ZRE-vUOMhl48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_names = [\"NO\", 'COPD']\n",
        "\n",
        "# Plot ROC curves\n",
        "fig, ax = plt.subplots(figsize=(16, 10))\n",
        "ax.plot([0, 1], [0, 1], 'k--')\n",
        "ax.set_xlim([0.0, 1.0])\n",
        "ax.set_ylim([0.0, 1.05])\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curve for Each Class')\n",
        "for i in range(2):\n",
        "    ax.plot(fpr[i], tpr[i], linewidth=3, label='ROC curve (area = %0.2f) for %s' % (roc_auc[i], c_names[i]))\n",
        "ax.legend(loc=\"best\", fontsize='x-large')\n",
        "ax.grid(alpha=.4)\n",
        "sns.despine()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "OsyUKaKTh2tc",
        "outputId": "31a476b9-8e13-47d4-c97d-528690c39db9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(classification_report(y_testclass, classpreds, target_names=c_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJONa0j9iLwk",
        "outputId": "0eceea93-58db-43e4-d7c4-8cb1b022f6de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NO       0.97      0.96      0.97       159\n",
            "        COPD       0.77      0.80      0.78        25\n",
            "\n",
            "    accuracy                           0.94       184\n",
            "   macro avg       0.87      0.88      0.87       184\n",
            "weighted avg       0.94      0.94      0.94       184\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "print(confusion_matrix(y_testclass, classpreds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOxKiE_ViOBC",
        "outputId": "42aef207-458c-414b-bf2a-80be0f0cf6bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[153   6]\n",
            " [  5  20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sn"
      ],
      "metadata": {
        "id": "aTG8BY2qiQ6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_testclass, classpreds)\n",
        "matrix_index = [\"COPD\",\"No COPD\"]\n",
        "df_cm = pd.DataFrame(cm, index = matrix_index, columns = matrix_index)\n",
        "df_cm.index.name = 'Actual'\n",
        "df_cm.columns.name = 'Predicted'\n",
        "fig, ax = plt.subplots(figsize=(10,7))\n",
        "\n",
        "cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "cm_perc = cm / cm_sum.astype(float) * 100\n",
        "annot = np.empty_like(cm).astype(str)\n",
        "nrows, ncols = cm.shape\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        c = cm[i, j]\n",
        "        p = cm_perc[i, j]\n",
        "        if i == j:\n",
        "            s = cm_sum[i]\n",
        "            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "        elif c == 0:\n",
        "            annot[i, j] = ''\n",
        "        else:\n",
        "            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "\n",
        "sn.heatmap(df_cm, annot=annot, fmt='')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "n-8a9Y2HiVuU",
        "outputId": "db700671-cf1d-468c-8d8c-8043115616b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}