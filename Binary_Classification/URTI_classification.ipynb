{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfpKHihC-Jyc",
        "outputId": "8ac3b52d-4ae0-4780-850b-080b5a83d7db"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.0)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.11.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.7.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3.6)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.5.0)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.0.5)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (67.7.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (23.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install resampy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k18eohjK-N_V",
        "outputId": "f85928bf-9217-4d8b-ab60-b5ae6c50e311"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting resampy\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from resampy) (1.23.5)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.10/dist-packages (from resampy) (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (67.7.2)\n",
            "Installing collected packages: resampy\n",
            "Successfully installed resampy-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxKzVB4GeTog",
        "outputId": "b4460e0c-8cb2-4acf-d8b3-ec1c7cf3f2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load various imports\n",
        "from datetime import datetime\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mypath = \"/content/gdrive/MyDrive/Major_Project/ICBHI_final_database/audio_and_txt_files/\"\n",
        "filenames = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f.endswith('.wav'))]"
      ],
      "metadata": {
        "id": "stVxcRZIfXIW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_id_in_file = [] # patient IDs corresponding to each file\n",
        "for name in filenames:\n",
        "    p_id_in_file.append(int(name[:3]))\n",
        "\n",
        "p_id_in_file = np.array(p_id_in_file)"
      ],
      "metadata": {
        "id": "-GGoegkGfZpM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_pad_len = 862 # to make the length of all MFCC equal\n",
        "\n",
        "def extract_features(file_name):\n",
        "    \"\"\"\n",
        "    This function takes in the path for an audio file as a string, loads it, and returns the MFCC\n",
        "    of the audio\"\"\"\n",
        "\n",
        "    try:\n",
        "        audio, sample_rate = librosa.load(file_name, duration=20)\n",
        "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
        "        pad_width = max_pad_len - mfccs.shape[1]\n",
        "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error encountered while parsing file: \", file_name)\n",
        "        return None\n",
        "\n",
        "    return mfccs"
      ],
      "metadata": {
        "id": "cHNjdWgDfb_N"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepaths = [join(mypath, f) for f in filenames] # full paths of files\n",
        "p_diag = pd.read_csv(\"/content/gdrive/MyDrive/Major_Project/ICBHI_final_database/patient_diagnosis.csv\") # patient diagnosis file # labels for audio files"
      ],
      "metadata": {
        "id": "YUKMVoXqfq5p"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "for x in p_id_in_file:\n",
        "  labels.append(p_diag[p_diag['patient_Id']==x]['Disease'].values[0])\n",
        "label = []"
      ],
      "metadata": {
        "id": "XZo9lgP94NH_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in  labels:\n",
        "    if i != 'URTI':\n",
        "        i=\"NO\"\n",
        "    label.append(i)"
      ],
      "metadata": {
        "id": "mwA3ABYf4Pgh"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count=0\n",
        "for i in labels:\n",
        "  if(i=='URTI'):\n",
        "    count+=1\n",
        "print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqn1IH6vXV76",
        "outputId": "b7165f3a-ec60-42dd-bb61-1e511f4b6821"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels=np.array(label)"
      ],
      "metadata": {
        "id": "u8AOXSq_fxxz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = []\n",
        "\n",
        "# Iterate through each sound file and extract the features\n",
        "for file_name in filepaths:\n",
        "    data = extract_features(file_name)\n",
        "    features.append(data)\n",
        "\n",
        "print('Finished feature extraction from ', len(features), ' files')\n",
        "features = np.array(features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq78auywfz66",
        "outputId": "b7df59da-aead-4265-9069-4fbd30d442e9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished feature extraction from  920  files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features= np.array(features) # convert to numpy array\n",
        "\n",
        "# delete the very rare diseases\n",
        "features1 = np.delete(features, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)\n",
        "\n",
        "labels1 = np.delete(labels, np.where((labels == 'Asthma') | (labels == 'LRTI'))[0], axis=0)\n",
        "\n",
        "# print class counts\n",
        "unique_elements, counts_elements = np.unique(labels, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAbO0OGVf5Js",
        "outputId": "88e4965d-9319-4d92-e8d9-2055c8b9c629"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['NO' 'URTI']\n",
            " ['897' '23']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot class counts\n",
        "y_pos = np.arange(len(unique_elements))\n",
        "plt.figure(figsize=(12,8))\n",
        "plt.bar(unique_elements, counts_elements, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, unique_elements)\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Disease')\n",
        "plt.title('Disease Count in Sound Files (No Asthma or LRTI)')\n",
        "plt.show()\n",
        "\n",
        "# One-hot encode labels\n",
        "le = LabelEncoder()\n",
        "i_labels = le.fit_transform(labels)\n",
        "oh_labels = to_categorical(i_labels)\n",
        "\n",
        "# add channel dimension for CNN\n",
        "features1 = np.reshape(features, (*features.shape,1))\n",
        "\n",
        "# train test split\n",
        "x_train, x_test, y_train, y_test = train_test_split(features1, oh_labels, stratify=oh_labels,\n",
        "                                                    test_size=0.2, random_state = 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "99ldZp8vgA-I",
        "outputId": "9f5f873e-0e39-44ca-888a-5926c5d4f464"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Network (CNN) model architecture"
      ],
      "metadata": {
        "id": "O_VTStARg9Qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows = 40\n",
        "num_columns = 862\n",
        "num_channels = 1\n",
        "\n",
        "num_labels = oh_labels.shape[1]\n",
        "filter_size = 2\n",
        "\n",
        "# Construct model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=filter_size,\n",
        "                 input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=filter_size, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "model.add(Dense(2, activation='softmax'))"
      ],
      "metadata": {
        "id": "y3lOaVzvgufO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# Display model architecture summary\n",
        "model.summary()\n",
        "\n",
        "# Calculate pre-training accuracy\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "accuracy = 100*score[1]\n",
        "\n",
        "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5hJB6m9YhA9a",
        "outputId": "48d32d45-5b50-4892-f5da-c6a91afcc8cd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 39, 861, 16)       80        \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 19, 430, 16)       0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 19, 430, 16)       0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 18, 429, 32)       2080      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 9, 214, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 9, 214, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 213, 64)        8256      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 106, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 106, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 3, 105, 128)       32896     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 1, 52, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1, 52, 128)        0         \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 128)               0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43570 (170.20 KB)\n",
            "Trainable params: 43570 (170.20 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-05480fc6ce21>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Calculate pre-training accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type NoneType)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n",
        "\n",
        "Here we will train the model. If we have a trained model, we can load it instead from the next cell."
      ],
      "metadata": {
        "id": "fhadGfWzhIO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train model\n",
        "num_epochs = 250\n",
        "num_batch_size = 128\n",
        "\n",
        "callbacks = [\n",
        "    ModelCheckpoint(\n",
        "        filepath='mymodel2_{epoch:02d}.h5',\n",
        "        # Path where to save the model\n",
        "        # The two parameters below mean that we will overwrite\n",
        "        # the current checkpoint if and only if\n",
        "        # the `val_accuracy` score has improved.\n",
        "        save_best_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        verbose=1)\n",
        "]\n",
        "start = datetime.now()\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs,\n",
        "          validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n",
        "\n",
        "\n",
        "duration = datetime.now() - start\n",
        "print(\"Training completed in time: \", duration)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJU49Ct8hEBP",
        "outputId": "4f94003d-1c78-431b-a7d5-315a47451bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 4.9290 - accuracy: 0.7935\n",
            "Epoch 1: val_accuracy improved from -inf to 0.97283, saving model to mymodel2_01.h5\n",
            "6/6 [==============================] - 29s 5s/step - loss: 4.9290 - accuracy: 0.7935 - val_loss: 1.0954 - val_accuracy: 0.9728\n",
            "Epoch 2/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.1962 - accuracy: 0.9755\n",
            "Epoch 2: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 26s 4s/step - loss: 2.1962 - accuracy: 0.9755 - val_loss: 1.2126 - val_accuracy: 0.9728\n",
            "Epoch 3/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 2.0551 - accuracy: 0.9755\n",
            "Epoch 3: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 26s 4s/step - loss: 2.0551 - accuracy: 0.9755 - val_loss: 0.9940 - val_accuracy: 0.9728\n",
            "Epoch 4/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.5123 - accuracy: 0.9755\n",
            "Epoch 4: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 24s 4s/step - loss: 1.5123 - accuracy: 0.9755 - val_loss: 0.7145 - val_accuracy: 0.9728\n",
            "Epoch 5/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 1.0606 - accuracy: 0.9755\n",
            "Epoch 5: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 24s 4s/step - loss: 1.0606 - accuracy: 0.9755 - val_loss: 0.4472 - val_accuracy: 0.9728\n",
            "Epoch 6/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.6198 - accuracy: 0.9755\n",
            "Epoch 6: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.6198 - accuracy: 0.9755 - val_loss: 0.2278 - val_accuracy: 0.9728\n",
            "Epoch 7/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.2864 - accuracy: 0.9755\n",
            "Epoch 7: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.2864 - accuracy: 0.9755 - val_loss: 0.5034 - val_accuracy: 0.8315\n",
            "Epoch 8/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1882 - accuracy: 0.9443\n",
            "Epoch 8: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 27s 4s/step - loss: 0.1882 - accuracy: 0.9443 - val_loss: 0.1921 - val_accuracy: 0.9728\n",
            "Epoch 9/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1964 - accuracy: 0.9755\n",
            "Epoch 9: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1964 - accuracy: 0.9755 - val_loss: 0.1644 - val_accuracy: 0.9728\n",
            "Epoch 10/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1686 - accuracy: 0.9755\n",
            "Epoch 10: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1686 - accuracy: 0.9755 - val_loss: 0.2369 - val_accuracy: 0.9728\n",
            "Epoch 11/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1348 - accuracy: 0.9701\n",
            "Epoch 11: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 32s 5s/step - loss: 0.1348 - accuracy: 0.9701 - val_loss: 0.3686 - val_accuracy: 0.9185\n",
            "Epoch 12/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9755\n",
            "Epoch 12: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.1244 - accuracy: 0.9755 - val_loss: 0.2413 - val_accuracy: 0.9728\n",
            "Epoch 13/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9755\n",
            "Epoch 13: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1284 - accuracy: 0.9755 - val_loss: 0.2449 - val_accuracy: 0.9728\n",
            "Epoch 14/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9742\n",
            "Epoch 14: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.1216 - accuracy: 0.9742 - val_loss: 0.3236 - val_accuracy: 0.9402\n",
            "Epoch 15/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9755\n",
            "Epoch 15: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1147 - accuracy: 0.9755 - val_loss: 0.2570 - val_accuracy: 0.9511\n",
            "Epoch 16/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1132 - accuracy: 0.9755\n",
            "Epoch 16: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.1132 - accuracy: 0.9755 - val_loss: 0.2502 - val_accuracy: 0.9511\n",
            "Epoch 17/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9742\n",
            "Epoch 17: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1081 - accuracy: 0.9742 - val_loss: 0.2519 - val_accuracy: 0.9511\n",
            "Epoch 18/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9728\n",
            "Epoch 18: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.1121 - accuracy: 0.9728 - val_loss: 0.2453 - val_accuracy: 0.9511\n",
            "Epoch 19/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9755\n",
            "Epoch 19: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 26s 5s/step - loss: 0.1059 - accuracy: 0.9755 - val_loss: 0.2222 - val_accuracy: 0.9511\n",
            "Epoch 20/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1079 - accuracy: 0.9715\n",
            "Epoch 20: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.1079 - accuracy: 0.9715 - val_loss: 0.2287 - val_accuracy: 0.9511\n",
            "Epoch 21/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9755\n",
            "Epoch 21: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0981 - accuracy: 0.9755 - val_loss: 0.1865 - val_accuracy: 0.9674\n",
            "Epoch 22/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.1020 - accuracy: 0.9755\n",
            "Epoch 22: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.1020 - accuracy: 0.9755 - val_loss: 0.1930 - val_accuracy: 0.9565\n",
            "Epoch 23/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9755\n",
            "Epoch 23: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0962 - accuracy: 0.9755 - val_loss: 0.1847 - val_accuracy: 0.9620\n",
            "Epoch 24/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0986 - accuracy: 0.9755\n",
            "Epoch 24: val_accuracy did not improve from 0.97283\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0986 - accuracy: 0.9755 - val_loss: 0.1515 - val_accuracy: 0.9728\n",
            "Epoch 25/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0998 - accuracy: 0.9755\n",
            "Epoch 25: val_accuracy improved from 0.97283 to 0.97826, saving model to mymodel2_25.h5\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0998 - accuracy: 0.9755 - val_loss: 0.1445 - val_accuracy: 0.9783\n",
            "Epoch 26/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9755\n",
            "Epoch 26: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0918 - accuracy: 0.9755 - val_loss: 0.1434 - val_accuracy: 0.9783\n",
            "Epoch 27/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0931 - accuracy: 0.9742\n",
            "Epoch 27: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0931 - accuracy: 0.9742 - val_loss: 0.1160 - val_accuracy: 0.9783\n",
            "Epoch 28/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9728\n",
            "Epoch 28: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 29s 5s/step - loss: 0.0957 - accuracy: 0.9728 - val_loss: 0.1234 - val_accuracy: 0.9783\n",
            "Epoch 29/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9728\n",
            "Epoch 29: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0955 - accuracy: 0.9728 - val_loss: 0.1152 - val_accuracy: 0.9783\n",
            "Epoch 30/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9742\n",
            "Epoch 30: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0837 - accuracy: 0.9742 - val_loss: 0.1253 - val_accuracy: 0.9783\n",
            "Epoch 31/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9769\n",
            "Epoch 31: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0915 - accuracy: 0.9769 - val_loss: 0.1090 - val_accuracy: 0.9783\n",
            "Epoch 32/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9701\n",
            "Epoch 32: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.0980 - accuracy: 0.9701 - val_loss: 0.1166 - val_accuracy: 0.9783\n",
            "Epoch 33/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9769\n",
            "Epoch 33: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0890 - accuracy: 0.9769 - val_loss: 0.1041 - val_accuracy: 0.9728\n",
            "Epoch 34/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9769\n",
            "Epoch 34: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0895 - accuracy: 0.9769 - val_loss: 0.1149 - val_accuracy: 0.9728\n",
            "Epoch 35/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9755\n",
            "Epoch 35: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 32s 5s/step - loss: 0.0914 - accuracy: 0.9755 - val_loss: 0.1123 - val_accuracy: 0.9728\n",
            "Epoch 36/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9742\n",
            "Epoch 36: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0831 - accuracy: 0.9742 - val_loss: 0.1147 - val_accuracy: 0.9728\n",
            "Epoch 37/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0811 - accuracy: 0.9755\n",
            "Epoch 37: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0811 - accuracy: 0.9755 - val_loss: 0.1118 - val_accuracy: 0.9728\n",
            "Epoch 38/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0920 - accuracy: 0.9769\n",
            "Epoch 38: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 4s/step - loss: 0.0920 - accuracy: 0.9769 - val_loss: 0.1108 - val_accuracy: 0.9728\n",
            "Epoch 39/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.9742\n",
            "Epoch 39: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0883 - accuracy: 0.9742 - val_loss: 0.1080 - val_accuracy: 0.9728\n",
            "Epoch 40/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9742\n",
            "Epoch 40: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0872 - accuracy: 0.9742 - val_loss: 0.1045 - val_accuracy: 0.9728\n",
            "Epoch 41/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9728\n",
            "Epoch 41: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0882 - accuracy: 0.9728 - val_loss: 0.1033 - val_accuracy: 0.9728\n",
            "Epoch 42/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9755\n",
            "Epoch 42: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0788 - accuracy: 0.9755 - val_loss: 0.1094 - val_accuracy: 0.9728\n",
            "Epoch 43/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9769\n",
            "Epoch 43: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0835 - accuracy: 0.9769 - val_loss: 0.1016 - val_accuracy: 0.9728\n",
            "Epoch 44/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9769\n",
            "Epoch 44: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0739 - accuracy: 0.9769 - val_loss: 0.1054 - val_accuracy: 0.9728\n",
            "Epoch 45/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9755\n",
            "Epoch 45: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0821 - accuracy: 0.9755 - val_loss: 0.0951 - val_accuracy: 0.9728\n",
            "Epoch 46/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.9715\n",
            "Epoch 46: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0886 - accuracy: 0.9715 - val_loss: 0.1019 - val_accuracy: 0.9728\n",
            "Epoch 47/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0837 - accuracy: 0.9769\n",
            "Epoch 47: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0837 - accuracy: 0.9769 - val_loss: 0.0983 - val_accuracy: 0.9728\n",
            "Epoch 48/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9755\n",
            "Epoch 48: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 28s 5s/step - loss: 0.0752 - accuracy: 0.9755 - val_loss: 0.1022 - val_accuracy: 0.9728\n",
            "Epoch 49/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9769\n",
            "Epoch 49: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0819 - accuracy: 0.9769 - val_loss: 0.1004 - val_accuracy: 0.9728\n",
            "Epoch 50/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9715\n",
            "Epoch 50: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0797 - accuracy: 0.9715 - val_loss: 0.0968 - val_accuracy: 0.9728\n",
            "Epoch 51/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9755\n",
            "Epoch 51: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0787 - accuracy: 0.9755 - val_loss: 0.1003 - val_accuracy: 0.9728\n",
            "Epoch 52/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0765 - accuracy: 0.9755\n",
            "Epoch 52: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0765 - accuracy: 0.9755 - val_loss: 0.0936 - val_accuracy: 0.9728\n",
            "Epoch 53/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0907 - accuracy: 0.9701\n",
            "Epoch 53: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0907 - accuracy: 0.9701 - val_loss: 0.0970 - val_accuracy: 0.9728\n",
            "Epoch 54/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9755\n",
            "Epoch 54: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0794 - accuracy: 0.9755 - val_loss: 0.0929 - val_accuracy: 0.9728\n",
            "Epoch 55/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9755\n",
            "Epoch 55: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0779 - accuracy: 0.9755 - val_loss: 0.1119 - val_accuracy: 0.9728\n",
            "Epoch 56/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9701\n",
            "Epoch 56: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0767 - accuracy: 0.9701 - val_loss: 0.0950 - val_accuracy: 0.9728\n",
            "Epoch 57/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9755\n",
            "Epoch 57: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0842 - accuracy: 0.9755 - val_loss: 0.1049 - val_accuracy: 0.9728\n",
            "Epoch 58/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9769\n",
            "Epoch 58: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0774 - accuracy: 0.9769 - val_loss: 0.0905 - val_accuracy: 0.9728\n",
            "Epoch 59/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9783\n",
            "Epoch 59: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0755 - accuracy: 0.9783 - val_loss: 0.0930 - val_accuracy: 0.9728\n",
            "Epoch 60/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9769\n",
            "Epoch 60: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 33s 6s/step - loss: 0.0737 - accuracy: 0.9769 - val_loss: 0.0891 - val_accuracy: 0.9728\n",
            "Epoch 61/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9755\n",
            "Epoch 61: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0769 - accuracy: 0.9755 - val_loss: 0.0892 - val_accuracy: 0.9728\n",
            "Epoch 62/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9755\n",
            "Epoch 62: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0778 - accuracy: 0.9755 - val_loss: 0.0917 - val_accuracy: 0.9728\n",
            "Epoch 63/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9742\n",
            "Epoch 63: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0692 - accuracy: 0.9742 - val_loss: 0.0870 - val_accuracy: 0.9728\n",
            "Epoch 64/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9769\n",
            "Epoch 64: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0825 - accuracy: 0.9769 - val_loss: 0.0911 - val_accuracy: 0.9728\n",
            "Epoch 65/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0727 - accuracy: 0.9728\n",
            "Epoch 65: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0727 - accuracy: 0.9728 - val_loss: 0.0886 - val_accuracy: 0.9728\n",
            "Epoch 66/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9769\n",
            "Epoch 66: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0677 - accuracy: 0.9769 - val_loss: 0.0870 - val_accuracy: 0.9728\n",
            "Epoch 67/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9742\n",
            "Epoch 67: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 29s 5s/step - loss: 0.0747 - accuracy: 0.9742 - val_loss: 0.0847 - val_accuracy: 0.9728\n",
            "Epoch 68/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9796\n",
            "Epoch 68: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0699 - accuracy: 0.9796 - val_loss: 0.0839 - val_accuracy: 0.9728\n",
            "Epoch 69/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9796\n",
            "Epoch 69: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0709 - accuracy: 0.9796 - val_loss: 0.0842 - val_accuracy: 0.9728\n",
            "Epoch 70/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9783\n",
            "Epoch 70: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0737 - accuracy: 0.9783 - val_loss: 0.0859 - val_accuracy: 0.9728\n",
            "Epoch 71/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9796\n",
            "Epoch 71: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0698 - accuracy: 0.9796 - val_loss: 0.0870 - val_accuracy: 0.9728\n",
            "Epoch 72/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0707 - accuracy: 0.9796\n",
            "Epoch 72: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0707 - accuracy: 0.9796 - val_loss: 0.0864 - val_accuracy: 0.9728\n",
            "Epoch 73/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9783\n",
            "Epoch 73: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0636 - accuracy: 0.9783 - val_loss: 0.0868 - val_accuracy: 0.9728\n",
            "Epoch 74/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9769\n",
            "Epoch 74: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 28s 5s/step - loss: 0.0653 - accuracy: 0.9769 - val_loss: 0.0837 - val_accuracy: 0.9728\n",
            "Epoch 75/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9796\n",
            "Epoch 75: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0646 - accuracy: 0.9796 - val_loss: 0.0820 - val_accuracy: 0.9728\n",
            "Epoch 76/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9796\n",
            "Epoch 76: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0604 - accuracy: 0.9796 - val_loss: 0.0811 - val_accuracy: 0.9728\n",
            "Epoch 77/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9742\n",
            "Epoch 77: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.0766 - accuracy: 0.9742 - val_loss: 0.0845 - val_accuracy: 0.9783\n",
            "Epoch 78/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9783\n",
            "Epoch 78: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0615 - accuracy: 0.9783 - val_loss: 0.0881 - val_accuracy: 0.9728\n",
            "Epoch 79/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0816 - accuracy: 0.9796\n",
            "Epoch 79: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0816 - accuracy: 0.9796 - val_loss: 0.0891 - val_accuracy: 0.9728\n",
            "Epoch 80/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0583 - accuracy: 0.9810\n",
            "Epoch 80: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0583 - accuracy: 0.9810 - val_loss: 0.0881 - val_accuracy: 0.9728\n",
            "Epoch 81/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9810\n",
            "Epoch 81: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0575 - accuracy: 0.9810 - val_loss: 0.0894 - val_accuracy: 0.9728\n",
            "Epoch 82/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9823\n",
            "Epoch 82: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0594 - accuracy: 0.9823 - val_loss: 0.0844 - val_accuracy: 0.9728\n",
            "Epoch 83/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0635 - accuracy: 0.9769\n",
            "Epoch 83: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0635 - accuracy: 0.9769 - val_loss: 0.0823 - val_accuracy: 0.9728\n",
            "Epoch 84/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9810\n",
            "Epoch 84: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0648 - accuracy: 0.9810 - val_loss: 0.0836 - val_accuracy: 0.9728\n",
            "Epoch 85/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9769\n",
            "Epoch 85: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0716 - accuracy: 0.9769 - val_loss: 0.0814 - val_accuracy: 0.9728\n",
            "Epoch 86/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9796\n",
            "Epoch 86: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.0556 - accuracy: 0.9796 - val_loss: 0.0822 - val_accuracy: 0.9728\n",
            "Epoch 87/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0639 - accuracy: 0.9783\n",
            "Epoch 87: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0639 - accuracy: 0.9783 - val_loss: 0.0824 - val_accuracy: 0.9728\n",
            "Epoch 88/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0526 - accuracy: 0.9837\n",
            "Epoch 88: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0526 - accuracy: 0.9837 - val_loss: 0.0832 - val_accuracy: 0.9728\n",
            "Epoch 89/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0627 - accuracy: 0.9823\n",
            "Epoch 89: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.0627 - accuracy: 0.9823 - val_loss: 0.0809 - val_accuracy: 0.9783\n",
            "Epoch 90/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9783\n",
            "Epoch 90: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 29s 5s/step - loss: 0.0559 - accuracy: 0.9783 - val_loss: 0.0792 - val_accuracy: 0.9728\n",
            "Epoch 91/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9810\n",
            "Epoch 91: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0588 - accuracy: 0.9810 - val_loss: 0.0804 - val_accuracy: 0.9728\n",
            "Epoch 92/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9796\n",
            "Epoch 92: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0579 - accuracy: 0.9796 - val_loss: 0.0796 - val_accuracy: 0.9728\n",
            "Epoch 93/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9796\n",
            "Epoch 93: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0624 - accuracy: 0.9796 - val_loss: 0.0873 - val_accuracy: 0.9728\n",
            "Epoch 94/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9783\n",
            "Epoch 94: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 31s 6s/step - loss: 0.0658 - accuracy: 0.9783 - val_loss: 0.0820 - val_accuracy: 0.9728\n",
            "Epoch 95/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9810\n",
            "Epoch 95: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0557 - accuracy: 0.9810 - val_loss: 0.0878 - val_accuracy: 0.9728\n",
            "Epoch 96/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9755\n",
            "Epoch 96: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0744 - accuracy: 0.9755 - val_loss: 0.0870 - val_accuracy: 0.9783\n",
            "Epoch 97/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9783\n",
            "Epoch 97: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 5s/step - loss: 0.0638 - accuracy: 0.9783 - val_loss: 0.0897 - val_accuracy: 0.9728\n",
            "Epoch 98/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9769\n",
            "Epoch 98: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0869 - accuracy: 0.9769 - val_loss: 0.0890 - val_accuracy: 0.9783\n",
            "Epoch 99/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9742\n",
            "Epoch 99: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0774 - accuracy: 0.9742 - val_loss: 0.0926 - val_accuracy: 0.9728\n",
            "Epoch 100/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9783\n",
            "Epoch 100: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0579 - accuracy: 0.9783 - val_loss: 0.0843 - val_accuracy: 0.9728\n",
            "Epoch 101/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9783\n",
            "Epoch 101: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 29s 5s/step - loss: 0.0669 - accuracy: 0.9783 - val_loss: 0.0851 - val_accuracy: 0.9728\n",
            "Epoch 102/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9769\n",
            "Epoch 102: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0941 - accuracy: 0.9769 - val_loss: 0.0840 - val_accuracy: 0.9728\n",
            "Epoch 103/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9783\n",
            "Epoch 103: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0687 - accuracy: 0.9783 - val_loss: 0.0833 - val_accuracy: 0.9728\n",
            "Epoch 104/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9810\n",
            "Epoch 104: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0605 - accuracy: 0.9810 - val_loss: 0.0948 - val_accuracy: 0.9728\n",
            "Epoch 105/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9796\n",
            "Epoch 105: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0597 - accuracy: 0.9796 - val_loss: 0.0907 - val_accuracy: 0.9728\n",
            "Epoch 106/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9823\n",
            "Epoch 106: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0650 - accuracy: 0.9823 - val_loss: 0.0899 - val_accuracy: 0.9728\n",
            "Epoch 107/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9769\n",
            "Epoch 107: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.0862 - accuracy: 0.9769 - val_loss: 0.0900 - val_accuracy: 0.9728\n",
            "Epoch 108/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9796\n",
            "Epoch 108: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0715 - accuracy: 0.9796 - val_loss: 0.0878 - val_accuracy: 0.9728\n",
            "Epoch 109/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9823\n",
            "Epoch 109: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0625 - accuracy: 0.9823 - val_loss: 0.0884 - val_accuracy: 0.9728\n",
            "Epoch 110/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9810\n",
            "Epoch 110: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0669 - accuracy: 0.9810 - val_loss: 0.0866 - val_accuracy: 0.9728\n",
            "Epoch 111/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0559 - accuracy: 0.9796\n",
            "Epoch 111: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0559 - accuracy: 0.9796 - val_loss: 0.0844 - val_accuracy: 0.9728\n",
            "Epoch 112/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0530 - accuracy: 0.9851\n",
            "Epoch 112: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0530 - accuracy: 0.9851 - val_loss: 0.0836 - val_accuracy: 0.9728\n",
            "Epoch 113/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9837\n",
            "Epoch 113: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0500 - accuracy: 0.9837 - val_loss: 0.0830 - val_accuracy: 0.9728\n",
            "Epoch 114/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9823\n",
            "Epoch 114: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0537 - accuracy: 0.9823 - val_loss: 0.0843 - val_accuracy: 0.9728\n",
            "Epoch 115/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9837\n",
            "Epoch 115: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0538 - accuracy: 0.9837 - val_loss: 0.0852 - val_accuracy: 0.9728\n",
            "Epoch 116/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0565 - accuracy: 0.9837\n",
            "Epoch 116: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0565 - accuracy: 0.9837 - val_loss: 0.0832 - val_accuracy: 0.9728\n",
            "Epoch 117/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0485 - accuracy: 0.9851\n",
            "Epoch 117: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0485 - accuracy: 0.9851 - val_loss: 0.0820 - val_accuracy: 0.9728\n",
            "Epoch 118/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0505 - accuracy: 0.9864\n",
            "Epoch 118: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0505 - accuracy: 0.9864 - val_loss: 0.0811 - val_accuracy: 0.9728\n",
            "Epoch 119/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9837\n",
            "Epoch 119: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0453 - accuracy: 0.9837 - val_loss: 0.0795 - val_accuracy: 0.9728\n",
            "Epoch 120/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9810\n",
            "Epoch 120: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0490 - accuracy: 0.9810 - val_loss: 0.0796 - val_accuracy: 0.9728\n",
            "Epoch 121/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9810\n",
            "Epoch 121: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0513 - accuracy: 0.9810 - val_loss: 0.0817 - val_accuracy: 0.9728\n",
            "Epoch 122/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9851\n",
            "Epoch 122: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0488 - accuracy: 0.9851 - val_loss: 0.0806 - val_accuracy: 0.9728\n",
            "Epoch 123/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9864\n",
            "Epoch 123: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0459 - accuracy: 0.9864 - val_loss: 0.0818 - val_accuracy: 0.9728\n",
            "Epoch 124/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9851\n",
            "Epoch 124: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0435 - accuracy: 0.9851 - val_loss: 0.0823 - val_accuracy: 0.9783\n",
            "Epoch 125/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9851\n",
            "Epoch 125: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0433 - accuracy: 0.9851 - val_loss: 0.0859 - val_accuracy: 0.9728\n",
            "Epoch 126/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9810\n",
            "Epoch 126: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0548 - accuracy: 0.9810 - val_loss: 0.0760 - val_accuracy: 0.9783\n",
            "Epoch 127/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9837\n",
            "Epoch 127: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0544 - accuracy: 0.9837 - val_loss: 0.0927 - val_accuracy: 0.9728\n",
            "Epoch 128/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.9783\n",
            "Epoch 128: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0961 - accuracy: 0.9783 - val_loss: 0.0791 - val_accuracy: 0.9728\n",
            "Epoch 129/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9688\n",
            "Epoch 129: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0703 - accuracy: 0.9688 - val_loss: 0.0807 - val_accuracy: 0.9728\n",
            "Epoch 130/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9796\n",
            "Epoch 130: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0683 - accuracy: 0.9796 - val_loss: 0.0945 - val_accuracy: 0.9728\n",
            "Epoch 131/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0533 - accuracy: 0.9837\n",
            "Epoch 131: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0533 - accuracy: 0.9837 - val_loss: 0.0913 - val_accuracy: 0.9728\n",
            "Epoch 132/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9864\n",
            "Epoch 132: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0475 - accuracy: 0.9864 - val_loss: 0.0885 - val_accuracy: 0.9728\n",
            "Epoch 133/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0545 - accuracy: 0.9823\n",
            "Epoch 133: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0545 - accuracy: 0.9823 - val_loss: 0.0916 - val_accuracy: 0.9728\n",
            "Epoch 134/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0490 - accuracy: 0.9823\n",
            "Epoch 134: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0490 - accuracy: 0.9823 - val_loss: 0.0884 - val_accuracy: 0.9728\n",
            "Epoch 135/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9851\n",
            "Epoch 135: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0488 - accuracy: 0.9851 - val_loss: 0.0888 - val_accuracy: 0.9728\n",
            "Epoch 136/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0512 - accuracy: 0.9837\n",
            "Epoch 136: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0512 - accuracy: 0.9837 - val_loss: 0.0908 - val_accuracy: 0.9728\n",
            "Epoch 137/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9864\n",
            "Epoch 137: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0440 - accuracy: 0.9864 - val_loss: 0.0794 - val_accuracy: 0.9783\n",
            "Epoch 138/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9851\n",
            "Epoch 138: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 4s/step - loss: 0.0480 - accuracy: 0.9851 - val_loss: 0.0808 - val_accuracy: 0.9783\n",
            "Epoch 139/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9864\n",
            "Epoch 139: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0443 - accuracy: 0.9864 - val_loss: 0.0814 - val_accuracy: 0.9728\n",
            "Epoch 140/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9823\n",
            "Epoch 140: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0456 - accuracy: 0.9823 - val_loss: 0.0834 - val_accuracy: 0.9728\n",
            "Epoch 141/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9864\n",
            "Epoch 141: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0435 - accuracy: 0.9864 - val_loss: 0.0885 - val_accuracy: 0.9728\n",
            "Epoch 142/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9864\n",
            "Epoch 142: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0431 - accuracy: 0.9864 - val_loss: 0.0896 - val_accuracy: 0.9728\n",
            "Epoch 143/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9851\n",
            "Epoch 143: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0431 - accuracy: 0.9851 - val_loss: 0.0907 - val_accuracy: 0.9728\n",
            "Epoch 144/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9823\n",
            "Epoch 144: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0503 - accuracy: 0.9823 - val_loss: 0.0910 - val_accuracy: 0.9728\n",
            "Epoch 145/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9810\n",
            "Epoch 145: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 29s 5s/step - loss: 0.0550 - accuracy: 0.9810 - val_loss: 0.0933 - val_accuracy: 0.9728\n",
            "Epoch 146/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9837\n",
            "Epoch 146: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0465 - accuracy: 0.9837 - val_loss: 0.0897 - val_accuracy: 0.9728\n",
            "Epoch 147/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9823\n",
            "Epoch 147: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0481 - accuracy: 0.9823 - val_loss: 0.0985 - val_accuracy: 0.9728\n",
            "Epoch 148/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9864\n",
            "Epoch 148: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0522 - accuracy: 0.9864 - val_loss: 0.0954 - val_accuracy: 0.9674\n",
            "Epoch 149/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9891\n",
            "Epoch 149: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0368 - accuracy: 0.9891 - val_loss: 0.0927 - val_accuracy: 0.9728\n",
            "Epoch 150/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9878\n",
            "Epoch 150: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0359 - accuracy: 0.9878 - val_loss: 0.0908 - val_accuracy: 0.9674\n",
            "Epoch 151/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9878\n",
            "Epoch 151: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0370 - accuracy: 0.9878 - val_loss: 0.0951 - val_accuracy: 0.9620\n",
            "Epoch 152/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9864\n",
            "Epoch 152: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 5s/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 0.0940 - val_accuracy: 0.9728\n",
            "Epoch 153/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0407 - accuracy: 0.9864\n",
            "Epoch 153: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0407 - accuracy: 0.9864 - val_loss: 0.0939 - val_accuracy: 0.9728\n",
            "Epoch 154/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9878\n",
            "Epoch 154: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0379 - accuracy: 0.9878 - val_loss: 0.0893 - val_accuracy: 0.9674\n",
            "Epoch 155/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9878\n",
            "Epoch 155: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0329 - accuracy: 0.9878 - val_loss: 0.0889 - val_accuracy: 0.9674\n",
            "Epoch 156/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9905\n",
            "Epoch 156: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.0354 - accuracy: 0.9905 - val_loss: 0.0905 - val_accuracy: 0.9674\n",
            "Epoch 157/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9864\n",
            "Epoch 157: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0364 - accuracy: 0.9864 - val_loss: 0.0979 - val_accuracy: 0.9620\n",
            "Epoch 158/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9864\n",
            "Epoch 158: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0409 - accuracy: 0.9864 - val_loss: 0.0974 - val_accuracy: 0.9674\n",
            "Epoch 159/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9891\n",
            "Epoch 159: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0329 - accuracy: 0.9891 - val_loss: 0.0970 - val_accuracy: 0.9674\n",
            "Epoch 160/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9851\n",
            "Epoch 160: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0421 - accuracy: 0.9851 - val_loss: 0.0926 - val_accuracy: 0.9728\n",
            "Epoch 161/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9878\n",
            "Epoch 161: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0362 - accuracy: 0.9878 - val_loss: 0.0916 - val_accuracy: 0.9620\n",
            "Epoch 162/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9823\n",
            "Epoch 162: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0457 - accuracy: 0.9823 - val_loss: 0.1017 - val_accuracy: 0.9674\n",
            "Epoch 163/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0356 - accuracy: 0.9878\n",
            "Epoch 163: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.0356 - accuracy: 0.9878 - val_loss: 0.1019 - val_accuracy: 0.9620\n",
            "Epoch 164/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.9891\n",
            "Epoch 164: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0354 - accuracy: 0.9891 - val_loss: 0.1023 - val_accuracy: 0.9674\n",
            "Epoch 165/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0363 - accuracy: 0.9851\n",
            "Epoch 165: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0363 - accuracy: 0.9851 - val_loss: 0.1305 - val_accuracy: 0.9565\n",
            "Epoch 166/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9851\n",
            "Epoch 166: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0401 - accuracy: 0.9851 - val_loss: 0.1132 - val_accuracy: 0.9728\n",
            "Epoch 167/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9905\n",
            "Epoch 167: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0360 - accuracy: 0.9905 - val_loss: 0.0927 - val_accuracy: 0.9674\n",
            "Epoch 168/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0382 - accuracy: 0.9891\n",
            "Epoch 168: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0382 - accuracy: 0.9891 - val_loss: 0.0914 - val_accuracy: 0.9674\n",
            "Epoch 169/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9851\n",
            "Epoch 169: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0376 - accuracy: 0.9851 - val_loss: 0.0958 - val_accuracy: 0.9674\n",
            "Epoch 170/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9864\n",
            "Epoch 170: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0375 - accuracy: 0.9864 - val_loss: 0.0959 - val_accuracy: 0.9674\n",
            "Epoch 171/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9878\n",
            "Epoch 171: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0307 - accuracy: 0.9878 - val_loss: 0.0969 - val_accuracy: 0.9620\n",
            "Epoch 172/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9864\n",
            "Epoch 172: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0403 - accuracy: 0.9864 - val_loss: 0.1042 - val_accuracy: 0.9728\n",
            "Epoch 173/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9864\n",
            "Epoch 173: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0330 - accuracy: 0.9864 - val_loss: 0.0966 - val_accuracy: 0.9620\n",
            "Epoch 174/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9891\n",
            "Epoch 174: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0303 - accuracy: 0.9891 - val_loss: 0.1006 - val_accuracy: 0.9674\n",
            "Epoch 175/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9891\n",
            "Epoch 175: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0314 - accuracy: 0.9891 - val_loss: 0.0941 - val_accuracy: 0.9674\n",
            "Epoch 176/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9864\n",
            "Epoch 176: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0325 - accuracy: 0.9864 - val_loss: 0.0972 - val_accuracy: 0.9674\n",
            "Epoch 177/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9891\n",
            "Epoch 177: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0283 - accuracy: 0.9891 - val_loss: 0.0983 - val_accuracy: 0.9674\n",
            "Epoch 178/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9905\n",
            "Epoch 178: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0255 - accuracy: 0.9905 - val_loss: 0.1002 - val_accuracy: 0.9674\n",
            "Epoch 179/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0328 - accuracy: 0.9891\n",
            "Epoch 179: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0328 - accuracy: 0.9891 - val_loss: 0.1106 - val_accuracy: 0.9620\n",
            "Epoch 180/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9837\n",
            "Epoch 180: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0417 - accuracy: 0.9837 - val_loss: 0.1078 - val_accuracy: 0.9674\n",
            "Epoch 181/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9918\n",
            "Epoch 181: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0264 - accuracy: 0.9918 - val_loss: 0.0944 - val_accuracy: 0.9674\n",
            "Epoch 182/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9905\n",
            "Epoch 182: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0246 - accuracy: 0.9905 - val_loss: 0.1052 - val_accuracy: 0.9674\n",
            "Epoch 183/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0312 - accuracy: 0.9918\n",
            "Epoch 183: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0312 - accuracy: 0.9918 - val_loss: 0.1047 - val_accuracy: 0.9674\n",
            "Epoch 184/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9905\n",
            "Epoch 184: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0250 - accuracy: 0.9905 - val_loss: 0.1059 - val_accuracy: 0.9674\n",
            "Epoch 185/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9905\n",
            "Epoch 185: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0302 - accuracy: 0.9905 - val_loss: 0.1002 - val_accuracy: 0.9674\n",
            "Epoch 186/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9878\n",
            "Epoch 186: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0341 - accuracy: 0.9878 - val_loss: 0.1119 - val_accuracy: 0.9620\n",
            "Epoch 187/250\n",
            "5/6 [========================>.....] - ETA: 3s - loss: 0.0239 - accuracy: 0.9906\n",
            "Epoch 187: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0312 - accuracy: 0.9878 - val_loss: 0.1075 - val_accuracy: 0.9620\n",
            "Epoch 188/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9891\n",
            "Epoch 188: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0365 - accuracy: 0.9891 - val_loss: 0.1292 - val_accuracy: 0.9728\n",
            "Epoch 189/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9864\n",
            "Epoch 189: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0473 - accuracy: 0.9864 - val_loss: 0.1171 - val_accuracy: 0.9620\n",
            "Epoch 190/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9823\n",
            "Epoch 190: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0504 - accuracy: 0.9823 - val_loss: 0.1005 - val_accuracy: 0.9674\n",
            "Epoch 191/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0375 - accuracy: 0.9864\n",
            "Epoch 191: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0375 - accuracy: 0.9864 - val_loss: 0.1313 - val_accuracy: 0.9728\n",
            "Epoch 192/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9796\n",
            "Epoch 192: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0521 - accuracy: 0.9796 - val_loss: 0.1227 - val_accuracy: 0.9674\n",
            "Epoch 193/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9796\n",
            "Epoch 193: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0532 - accuracy: 0.9796 - val_loss: 0.1330 - val_accuracy: 0.9674\n",
            "Epoch 194/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9823\n",
            "Epoch 194: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0510 - accuracy: 0.9823 - val_loss: 0.1231 - val_accuracy: 0.9674\n",
            "Epoch 195/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9918\n",
            "Epoch 195: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0377 - accuracy: 0.9918 - val_loss: 0.1157 - val_accuracy: 0.9674\n",
            "Epoch 196/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9891\n",
            "Epoch 196: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0306 - accuracy: 0.9891 - val_loss: 0.1220 - val_accuracy: 0.9674\n",
            "Epoch 197/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9891\n",
            "Epoch 197: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0332 - accuracy: 0.9891 - val_loss: 0.1135 - val_accuracy: 0.9620\n",
            "Epoch 198/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0343 - accuracy: 0.9905\n",
            "Epoch 198: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0343 - accuracy: 0.9905 - val_loss: 0.1204 - val_accuracy: 0.9674\n",
            "Epoch 199/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9851\n",
            "Epoch 199: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0308 - accuracy: 0.9851 - val_loss: 0.1185 - val_accuracy: 0.9620\n",
            "Epoch 200/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9932\n",
            "Epoch 200: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0271 - accuracy: 0.9932 - val_loss: 0.1188 - val_accuracy: 0.9620\n",
            "Epoch 201/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0282 - accuracy: 0.9878\n",
            "Epoch 201: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0282 - accuracy: 0.9878 - val_loss: 0.1133 - val_accuracy: 0.9674\n",
            "Epoch 202/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9946\n",
            "Epoch 202: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0236 - accuracy: 0.9946 - val_loss: 0.1119 - val_accuracy: 0.9674\n",
            "Epoch 203/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9905\n",
            "Epoch 203: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0298 - accuracy: 0.9905 - val_loss: 0.1162 - val_accuracy: 0.9674\n",
            "Epoch 204/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9891\n",
            "Epoch 204: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0272 - accuracy: 0.9891 - val_loss: 0.1206 - val_accuracy: 0.9620\n",
            "Epoch 205/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9891\n",
            "Epoch 205: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0288 - accuracy: 0.9891 - val_loss: 0.1325 - val_accuracy: 0.9565\n",
            "Epoch 206/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9918\n",
            "Epoch 206: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0222 - accuracy: 0.9918 - val_loss: 0.1293 - val_accuracy: 0.9620\n",
            "Epoch 207/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9932\n",
            "Epoch 207: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0200 - accuracy: 0.9932 - val_loss: 0.1237 - val_accuracy: 0.9674\n",
            "Epoch 208/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9918\n",
            "Epoch 208: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0229 - accuracy: 0.9918 - val_loss: 0.1193 - val_accuracy: 0.9620\n",
            "Epoch 209/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9932\n",
            "Epoch 209: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0227 - accuracy: 0.9932 - val_loss: 0.1217 - val_accuracy: 0.9674\n",
            "Epoch 210/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9891\n",
            "Epoch 210: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0280 - accuracy: 0.9891 - val_loss: 0.1327 - val_accuracy: 0.9620\n",
            "Epoch 211/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9905\n",
            "Epoch 211: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0250 - accuracy: 0.9905 - val_loss: 0.1322 - val_accuracy: 0.9620\n",
            "Epoch 212/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9878\n",
            "Epoch 212: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.0238 - accuracy: 0.9878 - val_loss: 0.1493 - val_accuracy: 0.9511\n",
            "Epoch 213/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9864\n",
            "Epoch 213: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0336 - accuracy: 0.9864 - val_loss: 0.1431 - val_accuracy: 0.9674\n",
            "Epoch 214/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9864\n",
            "Epoch 214: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0381 - accuracy: 0.9864 - val_loss: 0.1238 - val_accuracy: 0.9620\n",
            "Epoch 215/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9864\n",
            "Epoch 215: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0287 - accuracy: 0.9864 - val_loss: 0.1388 - val_accuracy: 0.9728\n",
            "Epoch 216/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9851\n",
            "Epoch 216: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0470 - accuracy: 0.9851 - val_loss: 0.1049 - val_accuracy: 0.9620\n",
            "Epoch 217/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9878\n",
            "Epoch 217: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0317 - accuracy: 0.9878 - val_loss: 0.1197 - val_accuracy: 0.9620\n",
            "Epoch 218/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9878\n",
            "Epoch 218: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0266 - accuracy: 0.9878 - val_loss: 0.1553 - val_accuracy: 0.9565\n",
            "Epoch 219/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9891\n",
            "Epoch 219: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0307 - accuracy: 0.9891 - val_loss: 0.1453 - val_accuracy: 0.9511\n",
            "Epoch 220/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9851\n",
            "Epoch 220: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0330 - accuracy: 0.9851 - val_loss: 0.1654 - val_accuracy: 0.9674\n",
            "Epoch 221/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9905\n",
            "Epoch 221: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0296 - accuracy: 0.9905 - val_loss: 0.1239 - val_accuracy: 0.9620\n",
            "Epoch 222/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9878\n",
            "Epoch 222: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.0302 - accuracy: 0.9878 - val_loss: 0.1309 - val_accuracy: 0.9674\n",
            "Epoch 223/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9878\n",
            "Epoch 223: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0262 - accuracy: 0.9878 - val_loss: 0.1267 - val_accuracy: 0.9565\n",
            "Epoch 224/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.9905\n",
            "Epoch 224: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0224 - accuracy: 0.9905 - val_loss: 0.1290 - val_accuracy: 0.9674\n",
            "Epoch 225/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9932\n",
            "Epoch 225: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 5s/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 0.1228 - val_accuracy: 0.9620\n",
            "Epoch 226/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9878\n",
            "Epoch 226: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0285 - accuracy: 0.9878 - val_loss: 0.1148 - val_accuracy: 0.9620\n",
            "Epoch 227/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9878\n",
            "Epoch 227: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0299 - accuracy: 0.9878 - val_loss: 0.1152 - val_accuracy: 0.9674\n",
            "Epoch 228/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9823\n",
            "Epoch 228: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0467 - accuracy: 0.9823 - val_loss: 0.1257 - val_accuracy: 0.9620\n",
            "Epoch 229/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9891\n",
            "Epoch 229: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0302 - accuracy: 0.9891 - val_loss: 0.1184 - val_accuracy: 0.9674\n",
            "Epoch 230/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9918\n",
            "Epoch 230: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0226 - accuracy: 0.9918 - val_loss: 0.1553 - val_accuracy: 0.9674\n",
            "Epoch 231/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9918\n",
            "Epoch 231: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0227 - accuracy: 0.9918 - val_loss: 0.1473 - val_accuracy: 0.9620\n",
            "Epoch 232/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9878\n",
            "Epoch 232: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0258 - accuracy: 0.9878 - val_loss: 0.1600 - val_accuracy: 0.9674\n",
            "Epoch 233/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9891\n",
            "Epoch 233: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0257 - accuracy: 0.9891 - val_loss: 0.1544 - val_accuracy: 0.9511\n",
            "Epoch 234/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9932\n",
            "Epoch 234: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0201 - accuracy: 0.9932 - val_loss: 0.1516 - val_accuracy: 0.9620\n",
            "Epoch 235/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9932\n",
            "Epoch 235: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 22s 4s/step - loss: 0.0195 - accuracy: 0.9932 - val_loss: 0.1490 - val_accuracy: 0.9620\n",
            "Epoch 236/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9946\n",
            "Epoch 236: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0174 - accuracy: 0.9946 - val_loss: 0.1531 - val_accuracy: 0.9674\n",
            "Epoch 237/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9959\n",
            "Epoch 237: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0180 - accuracy: 0.9959 - val_loss: 0.1658 - val_accuracy: 0.9511\n",
            "Epoch 238/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9946\n",
            "Epoch 238: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0172 - accuracy: 0.9946 - val_loss: 0.2011 - val_accuracy: 0.9511\n",
            "Epoch 239/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0182 - accuracy: 0.9932\n",
            "Epoch 239: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0182 - accuracy: 0.9932 - val_loss: 0.1646 - val_accuracy: 0.9674\n",
            "Epoch 240/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0238 - accuracy: 0.9905\n",
            "Epoch 240: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 27s 5s/step - loss: 0.0238 - accuracy: 0.9905 - val_loss: 0.1550 - val_accuracy: 0.9620\n",
            "Epoch 241/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9905\n",
            "Epoch 241: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 23s 4s/step - loss: 0.0251 - accuracy: 0.9905 - val_loss: 0.1712 - val_accuracy: 0.9674\n",
            "Epoch 242/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9891\n",
            "Epoch 242: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0253 - accuracy: 0.9891 - val_loss: 0.2184 - val_accuracy: 0.9511\n",
            "Epoch 243/250\n",
            "5/6 [========================>.....] - ETA: 3s - loss: 0.0254 - accuracy: 0.9906\n",
            "Epoch 243: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0247 - accuracy: 0.9905 - val_loss: 0.1903 - val_accuracy: 0.9674\n",
            "Epoch 244/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9905\n",
            "Epoch 244: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0234 - accuracy: 0.9905 - val_loss: 0.1665 - val_accuracy: 0.9620\n",
            "Epoch 245/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9878\n",
            "Epoch 245: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0262 - accuracy: 0.9878 - val_loss: 0.1658 - val_accuracy: 0.9620\n",
            "Epoch 246/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9932\n",
            "Epoch 246: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 25s 4s/step - loss: 0.0192 - accuracy: 0.9932 - val_loss: 0.1646 - val_accuracy: 0.9511\n",
            "Epoch 247/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9946\n",
            "Epoch 247: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0201 - accuracy: 0.9946 - val_loss: 0.1644 - val_accuracy: 0.9620\n",
            "Epoch 248/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9905\n",
            "Epoch 248: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 4s/step - loss: 0.0209 - accuracy: 0.9905 - val_loss: 0.1482 - val_accuracy: 0.9620\n",
            "Epoch 249/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9959\n",
            "Epoch 249: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 26s 5s/step - loss: 0.0179 - accuracy: 0.9959 - val_loss: 0.1424 - val_accuracy: 0.9620\n",
            "Epoch 250/250\n",
            "6/6 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9932\n",
            "Epoch 250: val_accuracy did not improve from 0.97826\n",
            "6/6 [==============================] - 24s 4s/step - loss: 0.0172 - accuracy: 0.9932 - val_loss: 0.1479 - val_accuracy: 0.9620\n",
            "Training completed in time:  1:43:41.188620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model\n",
        "\n",
        "Here we will review the accuracy of the model on both the training and test data sets."
      ],
      "metadata": {
        "id": "paLd3EFnhUtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model on the training and testing set\n",
        "score = model.evaluate(x_train, y_train, verbose=0)\n",
        "print(\"Training Accuracy: \", score[1])\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Testing Accuracy: \", score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB8GT-N1hL41",
        "outputId": "c8dccf6e-34f9-4f37-af02-b45c925b9a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy:  0.991847813129425\n",
            "Testing Accuracy:  0.9619565010070801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model.predict(x_test) # label scores\n",
        "\n",
        "classpreds = np.argmax(preds, axis=1) # predicted classes\n",
        "\n",
        "y_testclass = np.argmax(y_test, axis=1) # true classes\n",
        "\n",
        "n_classes=6 # number of classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTUEW5bfhYSu",
        "outputId": "f3d7dfcd-1925-49bf-fae2-98414c754424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 2s 334ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_testclass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcND6TAMhoc3",
        "outputId": "f126a9d3-e2d5-4035-cdfb-6e035c63634b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "for i in range(2):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], preds[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])"
      ],
      "metadata": {
        "id": "oP1kEyeXij1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c_names = [\"NO\", 'URTI']\n",
        "\n",
        "# Plot ROC curves\n",
        "fig, ax = plt.subplots(figsize=(16, 10))\n",
        "ax.plot([0, 1], [0, 1], 'k--')\n",
        "ax.set_xlim([0.0, 1.0])\n",
        "ax.set_ylim([0.0, 1.05])\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curve for Each Class')\n",
        "for i in range(2):\n",
        "    ax.plot(fpr[i], tpr[i], linewidth=3, label='ROC curve (area = %0.2f) for %s' % (roc_auc[i], c_names[i]))\n",
        "ax.legend(loc=\"best\", fontsize='x-large')\n",
        "ax.grid(alpha=.4)\n",
        "sns.despine()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "55fgUXYLimis",
        "outputId": "b80b3c8d-f38f-4030-ea89-24384cdcb59a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(classification_report(y_testclass, classpreds, target_names=c_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwWuNv6HipA0",
        "outputId": "83932e3f-074e-466a-c630-6344e87a0048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NO       0.97      0.99      0.98       179\n",
            "        URTI       0.00      0.00      0.00         5\n",
            "\n",
            "    accuracy                           0.96       184\n",
            "   macro avg       0.49      0.49      0.49       184\n",
            "weighted avg       0.95      0.96      0.95       184\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "print(confusion_matrix(y_testclass, classpreds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGgz5o16irXo",
        "outputId": "561ca392-071d-4e17-f33f-49d77e8b9587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[177   2]\n",
            " [  5   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sn\n",
        "\n",
        "cm = confusion_matrix(y_testclass, classpreds)\n",
        "matrix_index = [\"URTI\",\"No URTI\"]\n",
        "df_cm = pd.DataFrame(cm, index = matrix_index, columns = matrix_index)\n",
        "df_cm.index.name = 'Actual'\n",
        "df_cm.columns.name = 'Predicted'\n",
        "fig, ax = plt.subplots(figsize=(10,7))\n",
        "\n",
        "cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
        "cm_perc = cm / cm_sum.astype(float) * 100\n",
        "annot = np.empty_like(cm).astype(str)\n",
        "nrows, ncols = cm.shape\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        c = cm[i, j]\n",
        "        p = cm_perc[i, j]\n",
        "        if i == j:\n",
        "            s = cm_sum[i]\n",
        "            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
        "        elif c == 0:\n",
        "            annot[i, j] = ''\n",
        "        else:\n",
        "            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
        "\n",
        "sn.heatmap(df_cm, annot=annot, fmt='')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "SGaFNW-CiuAJ",
        "outputId": "de1ed7fb-02d5-43cc-a40e-98a768a168ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}